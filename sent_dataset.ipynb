{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sent_dataset import SentenceDataset\n",
    "from vocabulary import Vocabulary\n",
    "from utils import words_to_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from utils import sent_to_words, make_dir\n",
    "from vocabulary import BOS_CODE, EOS_CODE, PAD_CODE\n",
    "import pickle\n",
    "\n",
    "class SentenceDataset:\n",
    "    def __init__(self):\n",
    "        self._data = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_text(tokenized_text, path, voc, min_len, max_len):\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.replace('\\xa0', ' ').replace('\\ufeff','')\n",
    "            text = text.lower()\n",
    "\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            words = sent_to_words(sentence)\n",
    "            if min_len <= len(words) <= max_len:\n",
    "                tokens = voc.to_tokens(words)\n",
    "                tokenized_text.append(tokens)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return list(self._data.shape)\n",
    "    \n",
    "    \n",
    "    def build(self, paths, vocab, min_len, max_len):\n",
    "        if type(paths) is str:\n",
    "            paths = [paths]\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for p in paths:\n",
    "            self._read_text(sentences, p, vocab, min_len, max_len)\n",
    "\n",
    "        def to_data(sent):\n",
    "            assert min_len <= len(sent) <= max_len\n",
    "            npads = max_len - len(sent)\n",
    "            sent = [BOS_CODE] + sent + [EOS_CODE] + [PAD_CODE] * npads\n",
    "            assert len(sent) == max_len + 2\n",
    "            return np.array(sent, dtype=np.int32)\n",
    "            \n",
    "        self._data = np.zeros([len(sentences), max_len+2], dtype=np.int32)\n",
    "        for i in range(len(sentences)):\n",
    "            self._data[i] = to_data(sentences[i])\n",
    "            \n",
    "            \n",
    "    def save(self, path):\n",
    "        make_dir(path)\n",
    "        pickle.dump([self._data], open(path, \"wb\"))\n",
    "\n",
    "\n",
    "    def restore(self, path):\n",
    "        [self._data] = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "    \n",
    "    def get_next_batch(self, bs):\n",
    "        num   = len(self._data)\n",
    "        idx   = np.random.choice(np.arange(num), bs, replace=bs>num)\n",
    "        sents = self._data[idx]\n",
    "        return sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n"
     ]
    }
   ],
   "source": [
    "voc_path = \"vocabulary.data\"\n",
    "voc = Vocabulary()\n",
    "voc.restore(voc_path)\n",
    "print(voc.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14117, 22]\n",
      "CPU times: user 4.48 s, sys: 26.7 ms, total: 4.51 s\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = SentenceDataset()\n",
    "dataset.build(\n",
    "    paths   = 'data/anna.txt', # 'data/dostoevsky.txt'\n",
    "    vocab   = voc,\n",
    "    min_len = 2, \n",
    "    max_len = 20\n",
    ")\n",
    "print(dataset.shape)\n",
    "\n",
    "assert dataset.shape[1] == 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceDataset' object has no attribute '_tokens_to_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0a8dfc7a38f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./dataset.data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cf0ae93d08a2>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mmake_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokens_to_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words_to_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentenceDataset' object has no attribute '_tokens_to_words'"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./dataset.data\"\n",
    "dataset.save(dataset_path)\n",
    "old = dataset\n",
    "dataset = SentenceDataset()\n",
    "dataset.restore(dataset_path)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> — велела она мне, оставляя меня с альфонсинкой, — и там умри, если надо, понимаешь? <EOS>\n",
      "<BOS> он двинулся было с места, но четверо, его <UNK>, вдруг разом схватили его за руки. <EOS>_\n",
      "<BOS> зачем мне тогда приснилось дитё в такую минуту? <EOS>___________\n",
      "<BOS> ненависть загорелась в моем сердце. <EOS>______________\n",
      "<BOS> кроме вас, никто другой не поймет, а он во главе всех других. <EOS>_____\n",
      "<BOS> — в статье всего этого нет, там только намеки, — проговорил раскольников. <EOS>_____\n",
      "<BOS> — что ж, мне так и оставаться голым? <EOS>__________\n",
      "<BOS> он здесь живет; бывший крепостной человек, ну, вот пощечину дал. <EOS>______\n",
      "<BOS> — тут хорошо, — ободряла бабенка. <EOS>____________\n",
      "<BOS> — никак невозможно-с. <EOS>________________\n",
      "<BOS> алеша помнил потом, что она показалась ему чрезвычайно хороша собой в ту минуту. <EOS>_____\n",
      "<BOS> адвокат в этом случае правду сказал. <EOS>_____________\n",
      "<BOS> как она вздрогнула! <EOS>________________\n",
      "<BOS> я черных тараканов ночью туфлей <UNK>: так и щелкнет, как наступишь. <EOS>______\n",
      "<BOS> в неделю ко <UNK> сухая рыба да каша. <EOS>___________\n"
     ]
    }
   ],
   "source": [
    "batch = dataset.get_next_batch(15)\n",
    "for sent in batch:\n",
    "    restored = words_to_sent(voc.to_words(sent))\n",
    "    restored = restored.replace(' <PAD>', '_')\n",
    "    print(restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
