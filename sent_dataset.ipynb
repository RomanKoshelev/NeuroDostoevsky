{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ae_dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "UNK = '<UNK>'\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "PAD = '_'\n",
    "\n",
    "IGNORE = \"()[]{}:<>~@#$%^/\\|_+*…–«»\"\n",
    "AS_DOT = \";):»\"\n",
    "\n",
    "class SentenceDataset:\n",
    "    def __init__(self, min_len, max_len):\n",
    "        self.text          = ''\n",
    "        self.sentences     = []\n",
    "        self.word_to_token = {}\n",
    "        self.token_to_word = {}\n",
    "        self.max_len       = max_len\n",
    "        self.min_len       = min_len\n",
    "    \n",
    "    \n",
    "    def load(self, path, size = None):\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            if size is not None:\n",
    "                text = text[:size]\n",
    "            text = text.replace('\\xa0', ' ').replace('\\ufeff','')\n",
    "\n",
    "        for c in AS_DOT:\n",
    "            text=text.replace(c, '. ')\n",
    "            \n",
    "        sents = []\n",
    "        def prepare(s):\n",
    "            s = s.lower()\n",
    "            for c in IGNORE:\n",
    "                s=s.replace(c, ' ')\n",
    "            return s\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            ls = nltk.tokenize.sent_tokenize(line)\n",
    "            sents.extend([prepare(s) for s in ls])\n",
    "        \n",
    "        vocab   = set([UNK, BOS, EOS, PAD])\n",
    "        w_sents = []\n",
    "        for sent in sents:\n",
    "            words = [w for w in nltk.tokenize.word_tokenize(sent)]\n",
    "            vocab.update(words)\n",
    "            if len(words) < self.min_len or len(words) > self.max_len:\n",
    "                continue\n",
    "            w_sents.append(words)\n",
    "\n",
    "        self.num_tokens    = len(vocab)\n",
    "        self.word_to_token = {w: i for i, w in enumerate(vocab)}\n",
    "        self.token_to_word = dict(enumerate(vocab))\n",
    "        \n",
    "        self.sentences = np.zeros([len(w_sents), self.max_len+2], dtype=np.int32)\n",
    "        for i in range(len(w_sents)):\n",
    "            self.sentences[i] = self.encode(w_sents[i])\n",
    "\n",
    "\n",
    "    def encode(self, words):\n",
    "        words = words[:self.max_len]\n",
    "        npads = self.max_len - len(words)\n",
    "        words = [BOS] + words + [EOS] + [PAD] * npads\n",
    "        return np.array([self.word_to_token[w] for w in words], dtype=np.int32)\n",
    "\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.token_to_word[t] for t in tokens])\n",
    "        return text\n",
    "\n",
    "    \n",
    "    def get_next_batch(self, bs):\n",
    "        num   = len(self.sentences)\n",
    "        idx   = np.random.choice(np.arange(num), bs, replace=bs>num)\n",
    "        sents = self.sentences[idx]\n",
    "        return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4706\n",
      "703\n",
      "CPU times: user 532 ms, sys: 0 ns, total: 532 ms\n",
      "Wall time: 530 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = SentenceDataset(min_len=2, max_len=10)\n",
    "dataset.load('data/anna.txt', size=100000)\n",
    "print(dataset.num_tokens)\n",
    "print(len(dataset.sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[<BOS> аяяй ! <EOS> _ _ _ _ _ _ _ _]\n",
      "[<BOS> нечестно . <EOS> _ _ _ _ _ _ _ _]\n",
      "[<BOS> послушай . <EOS> _ _ _ _ _ _ _ _]\n",
      "[<BOS> так и я . <EOS> _ _ _ _ _ _]\n",
      "[<BOS> нынче не советую говорить , сказал степан аркадьич . <EOS> _]\n",
      "[<BOS> всю дорогу приятели молчали . <EOS> _ _ _ _ _]\n",
      "[<BOS> надо не волноваться , надо успокоиться . <EOS> _ _ _]\n",
      "[<BOS> прекрасно , до свидания же . <EOS> _ _ _ _]\n",
      "[<BOS> но левин не мог сидеть . <EOS> _ _ _ _]\n",
      "[<BOS> была пятница , и в столовой часовщик-немец заводил часы . <EOS>]\n"
     ]
    }
   ],
   "source": [
    "x = dataset.get_next_batch(10)\n",
    "#print(x)\n",
    "print()\n",
    "for t in x:\n",
    "    print('[%s]' % dataset.decode(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
