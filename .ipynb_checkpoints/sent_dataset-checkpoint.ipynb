{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sent_dataset import SentenceDataset\n",
    "from vocabulary import Vocabulary\n",
    "from utils import words_to_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from utils import sent_to_words, make_dir\n",
    "from vocabulary import BOS_CODE, EOS_CODE, PAD_CODE\n",
    "import pickle\n",
    "\n",
    "class SentenceDataset:\n",
    "    def __init__(self):\n",
    "        self._data = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_text(tokenized_text, path, voc, min_len, max_len):\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.replace('\\xa0', ' ').replace('\\ufeff','')\n",
    "            text = text.lower()\n",
    "\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            words = sent_to_words(sentence)\n",
    "            if min_len <= len(words) <= max_len:\n",
    "                tokens = voc.to_tokens(words)\n",
    "                tokenized_text.append(tokens)\n",
    "    \n",
    "    \n",
    "    def build(self, paths, vocab, min_len, max_len):\n",
    "        if type(paths) is str:\n",
    "            paths = [paths]\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for p in paths:\n",
    "            self._read_text(sentences, p, vocab, min_len, max_len)\n",
    "\n",
    "        def to_data(sent):\n",
    "            assert min_len <= len(sent) <= max_len\n",
    "            npads = max_len - len(sent)\n",
    "            sent = [BOS_CODE] + sent + [EOS_CODE] + [PAD_CODE] * npads\n",
    "            assert len(sent) == max_len + 2\n",
    "            return np.array(sent, dtype=np.int32)\n",
    "            \n",
    "        self._data = np.zeros([len(sentences), max_len+2], dtype=np.int32)\n",
    "        for i in range(len(sentences)):\n",
    "            self._data[i] = to_data(sentences[i])\n",
    "            \n",
    "            \n",
    "    def save(self, path):\n",
    "        make_dir(path)\n",
    "        pickle.dump([self._tokens_to_words, self._words_to_tokens], open(path, \"wb\"))\n",
    "\n",
    "\n",
    "    def restore(self, path):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def get_next_batch(self, bs):\n",
    "        num   = len(self._data)\n",
    "        idx   = np.random.choice(np.arange(num), bs, replace=bs>num)\n",
    "        sents = self._data[idx]\n",
    "        return sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n"
     ]
    }
   ],
   "source": [
    "voc_path = \"vocabulary.data\"\n",
    "voc = Vocabulary()\n",
    "voc.restore(voc_path)\n",
    "print(voc.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14117, 22)\n",
      "CPU times: user 4.64 s, sys: 15.7 ms, total: 4.66 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = SentenceDataset()\n",
    "dataset.build(\n",
    "    paths   = 'data/anna.txt', # 'data/dostoevsky.txt'\n",
    "    vocab   = voc,\n",
    "    min_len = 2, \n",
    "    max_len = 20\n",
    ")\n",
    "print(dataset._data.shape)\n",
    "\n",
    "assert dataset._data.shape[1] == 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset.data\"\n",
    "dataset.save(dataset_path)\n",
    "old = dataset\n",
    "dataset = SentenceDataset()\n",
    "dataset.restore(dataset_path)\n",
    "print(voc.size)\n",
    "assert voc._tokens_to_words == old._tokens_to_words\n",
    "assert voc._words_to_tokens == old._words_to_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> — велела она мне, оставляя меня с альфонсинкой, — и там умри, если надо, понимаешь? <EOS>\n",
      "<BOS> он двинулся было с места, но четверо, его <UNK>, вдруг разом схватили его за руки. <EOS>_\n",
      "<BOS> зачем мне тогда приснилось дитё в такую минуту? <EOS>___________\n",
      "<BOS> ненависть загорелась в моем сердце. <EOS>______________\n",
      "<BOS> кроме вас, никто другой не поймет, а он во главе всех других. <EOS>_____\n",
      "<BOS> — в статье всего этого нет, там только намеки, — проговорил раскольников. <EOS>_____\n",
      "<BOS> — что ж, мне так и оставаться голым? <EOS>__________\n",
      "<BOS> он здесь живет; бывший крепостной человек, ну, вот пощечину дал. <EOS>______\n",
      "<BOS> — тут хорошо, — ободряла бабенка. <EOS>____________\n",
      "<BOS> — никак невозможно-с. <EOS>________________\n",
      "<BOS> алеша помнил потом, что она показалась ему чрезвычайно хороша собой в ту минуту. <EOS>_____\n",
      "<BOS> адвокат в этом случае правду сказал. <EOS>_____________\n",
      "<BOS> как она вздрогнула! <EOS>________________\n",
      "<BOS> я черных тараканов ночью туфлей <UNK>: так и щелкнет, как наступишь. <EOS>______\n",
      "<BOS> в неделю ко <UNK> сухая рыба да каша. <EOS>___________\n"
     ]
    }
   ],
   "source": [
    "batch = dataset.get_next_batch(15)\n",
    "for sent in batch:\n",
    "    restored = words_to_sent(voc.to_words(sent))\n",
    "    restored = restored.replace(' <PAD>', '_')\n",
    "    print(restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
