{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idiot\n",
    "https://habrahabr.ru/post/342738/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "#dataset.load('data/dostoevsky-all.txt')\n",
    "dataset.load('data/anna.txt')\n",
    "model_path = 'models/002/model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def make_rnn_cell(self, num_units, num_layers, keep_prob):\n",
    "        def make_layer():\n",
    "            l = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "            l = tf.contrib.rnn.DropoutWrapper(l, output_keep_prob=keep_prob)\n",
    "            return l        \n",
    "        layers = [make_layer() for _ in range(num_layers)]\n",
    "        cell   = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "        return cell\n",
    "\n",
    "    \n",
    "    def make_loss(self, logits, targets, lstm_size, num_classes):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, \n",
    "            labels=tf.one_hot(targets, num_classes))\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def make_optimizer(self, loss, lr, grad_clip):\n",
    "        tr_vars   = tf.trainable_variables()\n",
    "        grads, _  = tf.clip_by_global_norm(tf.gradients(loss, tr_vars), grad_clip)\n",
    "        train_op  = tf.train.AdamOptimizer(lr)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tr_vars))\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "    def build(self, num_classes, num_units=128, num_layers=2, grad_clip=5):\n",
    "        tf.reset_default_graph()\n",
    "        self._graph = tf.Graph()\n",
    "        self._scope  = \"char_rnn\"\n",
    "        with self._graph.as_default(), tf.variable_scope(self._scope):\n",
    "            # placeholders\n",
    "            self.inputs_pl     = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "            self.targets_pl    = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "            self.seq_length_pl = tf.placeholder(tf.int32, [None], 'seq_lengths')\n",
    "            self.batch_size_pl = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "            self.keep_prob_pl  = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            self.lr_pl         = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "            # network\n",
    "            embed              = tf.one_hot(self.inputs_pl, num_classes)\n",
    "            cell               = self.make_rnn_cell(num_units, num_layers, self.keep_prob_pl)\n",
    "            initial_state      = cell.zero_state(self.batch_size_pl, tf.float32)\n",
    "            outputs, state     = tf.nn.dynamic_rnn(cell, embed, self.seq_length_pl, initial_state, dtype=tf.float32)\n",
    "            self.initial_state = initial_state\n",
    "            self.final_state   = state\n",
    "\n",
    "            # prediction\n",
    "            logits             = tf.layers.dense(outputs, num_classes)\n",
    "            self.prediction    = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "            # training\n",
    "            self.loss_op       = self.make_loss(logits, self.targets_pl, num_units, num_classes)\n",
    "            self.train_op      = self.make_optimizer(self.loss_op, self.lr_pl, grad_clip)\n",
    "            \n",
    "            # init\n",
    "            self.init_op       = tf.global_variables_initializer()\n",
    "\n",
    "        # session\n",
    "        self._sess = tf.Session(graph=self._graph)\n",
    "        self._sess.run(self.init_op)\n",
    "        \n",
    "        \n",
    "    def train(self, dataset, num_steps, epochs, batch_size, learning_rate, log_every):\n",
    "        try:\n",
    "            counter = 0\n",
    "            for e in range(epochs):\n",
    "                loss = 0\n",
    "                new_state = self._sess.run(self.initial_state, feed_dict={self.batch_size_pl: batch_size})\n",
    "\n",
    "                for x, y in dataset.get_batches(batch_size, num_steps):\n",
    "                    counter += 1\n",
    "                    tr_loss, new_state, _ = self._sess.run(\n",
    "                        [model.loss_op, model.final_state, model.train_op], \n",
    "                        feed_dict = {\n",
    "                            self.inputs_pl    : x,\n",
    "                            self.targets_pl   : y,\n",
    "                            self.seq_length_pl: [num_steps, ]*batch_size,\n",
    "                            self.initial_state: new_state,\n",
    "                            self.keep_prob_pl : keep_prob,\n",
    "                            self.lr_pl        : learning_rate,\n",
    "                    })\n",
    "\n",
    "                    if (counter % log_every == 0):\n",
    "                        print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                              'Training Step: {}... '.format(counter),\n",
    "                              'Training loss: {:.4f}... '.format(tr_loss))\n",
    "        except KeyboardInterrupt:\n",
    "            pass    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_layers    = 2\n",
    "num_units     = 512\n",
    "num_classes   = len(dataset.vocab)\n",
    "grad_clip     = 5.\n",
    "\n",
    "model = CharRNN()\n",
    "model.build(\n",
    "    num_classes = num_classes, \n",
    "    num_units   = num_units, \n",
    "    num_layers  = num_layers, \n",
    "    grad_clip   = grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50...  Training Step: 20...  Training loss: 3.3537... \n",
      "Epoch: 1/50...  Training Step: 40...  Training loss: 3.2902... \n",
      "Epoch: 1/50...  Training Step: 60...  Training loss: 3.3103... \n",
      "Epoch: 1/50...  Training Step: 80...  Training loss: 3.2803... \n",
      "Epoch: 1/50...  Training Step: 100...  Training loss: 3.4192... \n",
      "Epoch: 1/50...  Training Step: 120...  Training loss: 3.2427... \n",
      "Epoch: 1/50...  Training Step: 140...  Training loss: 3.1549... \n",
      "Epoch: 1/50...  Training Step: 160...  Training loss: 3.0743... \n",
      "Epoch: 2/50...  Training Step: 180...  Training loss: 2.9296... \n",
      "Epoch: 2/50...  Training Step: 200...  Training loss: 2.8132... \n",
      "Epoch: 2/50...  Training Step: 220...  Training loss: 2.7252... \n",
      "Epoch: 2/50...  Training Step: 240...  Training loss: 2.6720... \n",
      "Epoch: 2/50...  Training Step: 260...  Training loss: 2.6121... \n",
      "Epoch: 2/50...  Training Step: 280...  Training loss: 2.5939... \n",
      "Epoch: 2/50...  Training Step: 300...  Training loss: 2.5761... \n",
      "CPU times: user 2min 51s, sys: 14.2 s, total: 3min 5s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs        = 50\n",
    "batch_size    = 100\n",
    "num_steps     = 100\n",
    "learning_rate = 0.001\n",
    "keep_prob     = 0.5\n",
    "log_every     = 20\n",
    "\n",
    "model.train(\n",
    "    dataset       = dataset, \n",
    "    num_steps     = num_steps, \n",
    "    epochs        = epochs, \n",
    "    batch_size    = batch_size, \n",
    "    learning_rate = learning_rate, \n",
    "    log_every     = log_every)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, dataset, n_samples, lstm_size, vocab_size, top_n, prime):\n",
    "    samples = [c for c in prime]\n",
    "    new_state = model._sess.run(model.initial_state, feed_dict={model.batch_size_pl: 1})\n",
    "    for c in prime:\n",
    "        x      = np.zeros((1, 1))\n",
    "        x[0,0] = dataset.vocab_to_int[c]\n",
    "        preds, new_state = model._sess.run(\n",
    "            [model.prediction, model.final_state], \n",
    "            feed_dict={\n",
    "                model.inputs_pl    : x,\n",
    "                model.seq_length_pl: [1],\n",
    "                model.initial_state: new_state,\n",
    "                model.keep_prob_pl : 1.\n",
    "            })\n",
    "\n",
    "    c = pick_top_n(preds, len(dataset.vocab), top_n)\n",
    "    samples.append(dataset.int_to_vocab[c])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        x[0,0] = c\n",
    "        preds, new_state = model._sess.run(\n",
    "            [model.prediction, model.final_state], \n",
    "            feed_dict={\n",
    "                model.inputs_pl    : x,\n",
    "                model.seq_length_pl: [1],\n",
    "                model.initial_state: new_state,\n",
    "                model.keep_prob_pl : 1.\n",
    "            })\n",
    "\n",
    "        c = pick_top_n(preds, len(dataset.vocab), top_n)\n",
    "        samples.append(dataset.int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Быть или не быть? И неди педеме, ча са сла порони пелени о пралул прого ом педи ны о бороло падани. Вра ни проволе села,, – кака ста в подол восто оне, чоде брасть в реме ни вони слени но вон ник сте но номо нами вом воде нын вем нене ва сти ны о сло неста пел пилать врога се пратов илини.\n",
      "\n",
      "\n",
      "\n",
      "–, – кок коз омо в постав емо порола пили стол в подне ни веста сто слев воли ни повела, но ного столо сно вал ны сто пол снесил в врино не но но сровитань и сел подито.\n",
      "\n",
      "\n",
      "\n",
      "– – Я козола сальна сельно полень полола, о ста ви нона\n"
     ]
    }
   ],
   "source": [
    "# samp = sample(1000, lstm_size, len(vocab), top_n=5, prime=)\n",
    "primes = ['Быть или не быть?']\n",
    "for prime in primes:\n",
    "    print('-'*50)\n",
    "    samp = sample(model, dataset, 500, num_units, len(dataset.vocab), top_n=5, prime=prime)\n",
    "    samp = samp.replace('\\n', '\\n\\n')\n",
    "    print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
