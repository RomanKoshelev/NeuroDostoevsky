{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Sec autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "examples = 10000\n",
    "symbols = 100\n",
    "length = 10\n",
    "\n",
    "with open('vocab', 'w') as f:\n",
    "    f.write(\"<S>\\n</S>\\n<UNK>\\n\")\n",
    "    for i in range(symbols):\n",
    "        f.write(\"%d\\n\" % i)\n",
    "\n",
    "\n",
    "with open('input', 'w') as fin:\n",
    "    with open('output', 'w') as fout:\n",
    "        for i in range(examples):\n",
    "            inp = [random.randint(0, symbols) + 3 for _ in range(length)]\n",
    "            out = [(x + 5) % 100 + 3 for x in inp]\n",
    "            fin.write(' '.join([str(x) for x in inp]) + '\\n')\n",
    "            fout.write(' '.join([str(x) for x in out]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "import timeline\n",
    "\n",
    "\n",
    "GO_TOKEN = 0\n",
    "END_TOKEN = 1\n",
    "UNK_TOKEN = 2\n",
    "\n",
    "\n",
    "def seq2seq(mode, features, labels, params):\n",
    "    vocab_size = params['vocab_size']\n",
    "    embed_dim = params['embed_dim']\n",
    "    num_units = params['num_units']\n",
    "    input_max_length = params['input_max_length']\n",
    "    output_max_length = params['output_max_length']\n",
    "\n",
    "    inp = features['input']\n",
    "    output = features['output']\n",
    "    batch_size = tf.shape(inp)[0]\n",
    "    start_tokens = tf.zeros([batch_size], dtype=tf.int64)\n",
    "    train_output = tf.concat([tf.expand_dims(start_tokens, 1), output], 1)\n",
    "    input_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(inp, 1)), 1)\n",
    "    output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, 1)), 1)\n",
    "    input_embed = layers.embed_sequence(\n",
    "        inp, vocab_size=vocab_size, embed_dim=embed_dim, scope='embed')\n",
    "    output_embed = layers.embed_sequence(\n",
    "        train_output, vocab_size=vocab_size, embed_dim=embed_dim, scope='embed', reuse=True)\n",
    "    with tf.variable_scope('embed', reuse=True):\n",
    "        embeddings = tf.get_variable('embeddings')\n",
    "\n",
    "    cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell, input_embed, dtype=tf.float32)\n",
    "\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "    # train_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "    #     output_embed, output_lengths, embeddings, 0.3\n",
    "    # )\n",
    "    pred_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embeddings, start_tokens=tf.to_int32(start_tokens), end_token=1)\n",
    "\n",
    "    def decode(helper, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=num_units, memory=encoder_outputs,\n",
    "                memory_sequence_length=input_lengths)\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "            attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "            out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                attn_cell, vocab_size, reuse=reuse\n",
    "            )\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=out_cell, helper=helper,\n",
    "                initial_state=out_cell.zero_state(\n",
    "                    dtype=tf.float32, batch_size=batch_size))\n",
    "                #initial_state=encoder_final_state)\n",
    "            outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder, output_time_major=False,\n",
    "                impute_finished=True, maximum_iterations=output_max_length\n",
    "            )\n",
    "            return outputs[0]\n",
    "    train_outputs = decode(train_helper, 'decode')\n",
    "    pred_outputs = decode(pred_helper, 'decode', reuse=True)\n",
    "\n",
    "    tf.identity(train_outputs.sample_id[0], name='train_pred')\n",
    "    weights = tf.to_float(tf.not_equal(train_output[:, :-1], 1))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_outputs.rnn_output, output, weights=weights)\n",
    "    train_op = layers.optimize_loss(\n",
    "        loss, tf.train.get_global_step(),\n",
    "        optimizer=params.get('optimizer', 'Adam'),\n",
    "        learning_rate=params.get('learning_rate', 0.001),\n",
    "        summaries=['loss', 'learning_rate'])\n",
    "\n",
    "    tf.identity(pred_outputs.sample_id[0], name='predictions')\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_outputs.sample_id,\n",
    "        loss=loss,\n",
    "        train_op=train_op\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_seq2seq(\n",
    "        input_filename, output_filename, vocab_filename,\n",
    "        model_dir):\n",
    "    vocab = load_vocab(vocab_filename)\n",
    "    params = {\n",
    "        'vocab_size': len(vocab),\n",
    "        'batch_size': 32,\n",
    "        'input_max_length': 30,\n",
    "        'output_max_length': 30,\n",
    "        'embed_dim': 100,\n",
    "        'num_units': 256\n",
    "    }\n",
    "    est = tf.estimator.Estimator(\n",
    "        model_fn=seq2seq,\n",
    "        model_dir=model_dir, \n",
    "        params=params)\n",
    "\n",
    "    input_fn, feed_fn = make_input_fn(\n",
    "        params['batch_size'],\n",
    "        input_filename,\n",
    "        output_filename,\n",
    "        vocab, \n",
    "        params['input_max_length'], \n",
    "        params['output_max_length'])\n",
    "\n",
    "    # Make hooks to print examples of inputs/predictions.\n",
    "    print_inputs = tf.train.LoggingTensorHook(\n",
    "        ['input_0', 'output_0'], every_n_iter=100,\n",
    "        formatter=get_formatter(['input_0', 'output_0'], vocab))\n",
    "    print_predictions = tf.train.LoggingTensorHook(\n",
    "        ['predictions', 'train_pred'], every_n_iter=100,\n",
    "        formatter=get_formatter(['predictions', 'train_pred'], vocab))\n",
    "\n",
    "    timeline_hook = timeline.TimelineHook(model_dir, every_n_iter=100)\n",
    "    est.train(\n",
    "        input_fn=input_fn,\n",
    "        hooks=[tf.train.FeedFnHook(feed_fn), print_inputs, print_predictions, timeline_hook],\n",
    "        steps=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_map(line, vocab):\n",
    "    return [vocab.get(token, UNK_TOKEN) for token in line.split(' ')]\n",
    "\n",
    "\n",
    "def make_input_fn(\n",
    "        batch_size, input_filename, output_filename, vocab,\n",
    "        input_max_length, output_max_length,\n",
    "        input_process=tokenize_and_map, output_process=tokenize_and_map):\n",
    "\n",
    "    def input_fn():\n",
    "        inp = tf.placeholder(tf.int64, shape=[None, None], name='input')\n",
    "        output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "        tf.identity(inp[0], 'input_0')\n",
    "        tf.identity(output[0], 'output_0')\n",
    "        return {\n",
    "            'input': inp,\n",
    "            'output': output,\n",
    "        }, None\n",
    "\n",
    "    def sampler():\n",
    "        while True:\n",
    "            with open(input_filename) as finput:\n",
    "                with open(output_filename) as foutput:\n",
    "                    for in_line in finput:\n",
    "                        out_line = foutput.readline()\n",
    "                        yield {\n",
    "                            'input': input_process(in_line, vocab)[:input_max_length - 1] + [END_TOKEN],\n",
    "                            'output': output_process(out_line, vocab)[:output_max_length - 1] + [END_TOKEN]\n",
    "                        }\n",
    "\n",
    "    sample_me = sampler()\n",
    "\n",
    "    def feed_fn():\n",
    "        inputs, outputs = [], []\n",
    "        input_length, output_length = 0, 0\n",
    "        for i in range(batch_size):\n",
    "            rec = next(sample_me)\n",
    "            inputs.append(rec['input'])\n",
    "            outputs.append(rec['output'])\n",
    "            input_length = max(input_length, len(inputs[-1]))\n",
    "            output_length = max(output_length, len(outputs[-1]))\n",
    "        # Pad me right with </S> token.\n",
    "        for i in range(batch_size):\n",
    "            inputs[i] += [END_TOKEN] * (input_length - len(inputs[i]))\n",
    "            outputs[i] += [END_TOKEN] * (output_length - len(outputs[i]))\n",
    "        return {\n",
    "            'input:0': inputs,\n",
    "            'output:0': outputs\n",
    "        }\n",
    "\n",
    "    return input_fn, feed_fn\n",
    "\n",
    "\n",
    "def load_vocab(filename):\n",
    "    vocab = {}\n",
    "    with open(filename) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            vocab[line.strip()] = idx\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_rev_vocab(vocab):\n",
    "    return {idx: key for key, idx in vocab.items()}\n",
    "\n",
    "\n",
    "def get_formatter(keys, vocab):\n",
    "    rev_vocab = get_rev_vocab(vocab)\n",
    "\n",
    "    def to_str(sequence):\n",
    "        tokens = [\n",
    "            rev_vocab.get(x, \"<UNK>\") for x in sequence]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def format(values):\n",
    "        res = []\n",
    "        for key in keys:\n",
    "            res.append(\"%s = %s\" % (key, to_str(values[key])))\n",
    "        return '\\n'.join(res)\n",
    "    return format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.logging._logger.setLevel(logging.INFO)\n",
    "    train_seq2seq('input', 'output', 'vocab', 'model/seq2seq')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
