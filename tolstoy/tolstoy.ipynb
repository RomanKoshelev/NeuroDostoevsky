{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tolstoy\n",
    "https://habrahabr.ru/post/342738/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЧАСТЬ ПЕРВАЯ\\n\\n\\n\\nI\\n\\nВсе счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99,  77,  93,  94, 102,   1,  91,  82,  92,  79,  77, 105,   0,\n",
       "         0,   0,   0,  30,   0,   0,  79, 123, 111,   1, 123, 129, 106,\n",
       "       123, 124, 117, 114, 108, 133, 111,   1, 123, 111, 118, 134, 114,\n",
       "         1, 121, 120, 127, 120, 112, 114,   1, 110, 122, 125, 109,   1,\n",
       "       119, 106,   1, 110, 122, 125, 109, 106,   7,   1, 116, 106, 112,\n",
       "       110, 106, 137,   1, 119, 111, 123, 129, 106, 123, 124, 117, 114,\n",
       "       108, 106, 137,   1, 123, 111, 118, 134, 137,   1, 119, 111, 123,\n",
       "       129, 106, 123, 124, 117, 114, 108, 106,   1, 121, 120,   8, 123,\n",
       "       108, 120, 111, 118, 125,   9], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Создаем генератор, который возвращает пакеты размером\n",
    "       n_seqs x n_steps из массива arr.\n",
    "       \n",
    "       Аргументы\n",
    "       ---------\n",
    "       arr: Массив, из которого получаем пакеты\n",
    "       n_seqs: Batch size, количество последовательностей в пакете\n",
    "       n_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "    '''\n",
    "    # Считаем количество символов на пакет и количество пакетов, которое можем сформировать\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Сохраняем в массиве только символы, которые позволяют сформировать целое число пакетов\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Делаем reshape 1D -> 2D, используя n_seqs как число строк, как на картинке\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # пакет данных, который будет подаваться на вход сети\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # целевой пакет, с которым будем сравнивать предсказание, получаем сдвиганием \"x\" на один символ вперед\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 99  77  93  94 102]\n",
      " [  1 110 108 114 112]\n",
      " [ 79 120 124   1 120]\n",
      " [114 119   1 109 120]\n",
      " [123 121 122 106 108]]\n",
      "\n",
      "y\n",
      " [[ 77  93  94 102   1]\n",
      " [110 108 114 112 111]\n",
      " [120 124   1 120 124]\n",
      " [119   1 109 120 108]\n",
      " [121 122 106 108 111]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:5, :5])\n",
    "print('\\ny\\n', y[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Определяем placeholder'ы для входных, целевых данных, а также вероятности drop out\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        batch_size: Batch size, количество последовательностей в пакете\n",
    "        num_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "        \n",
    "    '''\n",
    "    # Объявляем placeholder'ы\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Placeholder для вероятности drop out\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Строим LSTM ячейку.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        keep_prob: Скаляр (tf.placeholder) для dropout keep probability\n",
    "        lstm_size: Размер скрытых слоев в LSTM ячейках\n",
    "        num_layers: Количество LSTM слоев\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Строим LSTM ячейку\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Начинаем с базовой LSTM ячейки\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Добавляем dropout к ячейке\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    # Инициализируем начальное состояние LTSM ячейки\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Строим softmax слой и возвращаем результат его работы.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        \n",
    "        x: Входящий от LSTM тензор\n",
    "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
    "        out_size: Размер softmax слоя (объем словаря)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Соединяем результат LTSM слоев с softmax слоем\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Считаем logit-функцию\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    # Используем функцию softmax для получения предсказания\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Считаем функцию потери на основании значений logit-функции и целевых значений.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        logits: значение logit-функции\n",
    "        targets: целевые значения, с которыми сравниваем предсказания\n",
    "        lstm_size: Количество юнитов в LSTM слое\n",
    "        num_classes: Количество классов в целевых значениях (размер словаря)\n",
    "        \n",
    "    '''\n",
    "    # Делаем one-hot кодирование целевых значений и решейпим по образу и подобию logits\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Считаем значение функции потери softmax cross entropy loss и возвращаем среднее значение\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Строим оптимизатор для обучения, используя обрезку градиента.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: значение функции потери\n",
    "        learning_rate: параметр скорости обучения\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Оптимизатор для обучения, обрезка градиента для контроля \"взрывающихся\" градиентов\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # Мы будем использовать эту же сеть для сэмплирования (генерации текста),\n",
    "        # при этом будем подавать по одному символу за один раз\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Получаем input placeholder'ы\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Строим LSTM ячейку\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Прогоняем данные через RNN слои\n",
    "        # Делаем one-hot кодирование входящих данных\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Прогоняем данные через RNN и собираем результаты\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Получаем предсказания (softmax) и результат logit-функции\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Считаем потери и оптимизируем (с обрезкой градиента)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        # Размер пакета\n",
    "num_steps = 100         # Шагов в пакете\n",
    "lstm_size = 512         # Количество LSTM юнитов в скрытом слое\n",
    "num_layers = 2          # Количество LSTM слоев\n",
    "learning_rate = 0.001   # Скорость обучения\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30...  Training Step: 1...  Training loss: 4.9429...  0.3129 sec/batch\n",
      "Epoch: 1/30...  Training Step: 2...  Training loss: 4.8686...  0.3276 sec/batch\n",
      "Epoch: 1/30...  Training Step: 3...  Training loss: 4.4622...  0.2294 sec/batch\n",
      "Epoch: 1/30...  Training Step: 4...  Training loss: 5.0477...  0.2836 sec/batch\n",
      "Epoch: 1/30...  Training Step: 5...  Training loss: 4.1799...  0.2826 sec/batch\n",
      "Epoch: 1/30...  Training Step: 6...  Training loss: 3.9392...  0.1968 sec/batch\n",
      "Epoch: 1/30...  Training Step: 7...  Training loss: 3.9166...  0.1785 sec/batch\n",
      "Epoch: 1/30...  Training Step: 8...  Training loss: 3.7942...  0.2283 sec/batch\n",
      "Epoch: 1/30...  Training Step: 9...  Training loss: 3.7031...  0.1859 sec/batch\n",
      "Epoch: 1/30...  Training Step: 10...  Training loss: 3.6427...  0.1794 sec/batch\n",
      "Epoch: 1/30...  Training Step: 11...  Training loss: 3.6076...  0.2558 sec/batch\n",
      "Epoch: 1/30...  Training Step: 12...  Training loss: 3.5756...  0.2708 sec/batch\n",
      "Epoch: 1/30...  Training Step: 13...  Training loss: 3.5357...  0.2758 sec/batch\n",
      "Epoch: 1/30...  Training Step: 14...  Training loss: 3.5248...  0.2818 sec/batch\n",
      "Epoch: 1/30...  Training Step: 15...  Training loss: 3.5207...  0.1991 sec/batch\n",
      "Epoch: 1/30...  Training Step: 16...  Training loss: 3.5343...  0.1881 sec/batch\n",
      "Epoch: 1/30...  Training Step: 17...  Training loss: 3.4849...  0.2055 sec/batch\n",
      "Epoch: 1/30...  Training Step: 18...  Training loss: 3.4498...  0.2820 sec/batch\n",
      "Epoch: 1/30...  Training Step: 19...  Training loss: 3.4461...  0.3582 sec/batch\n",
      "Epoch: 1/30...  Training Step: 20...  Training loss: 3.4385...  0.2164 sec/batch\n",
      "Epoch: 1/30...  Training Step: 21...  Training loss: 3.4448...  0.3032 sec/batch\n",
      "Epoch: 1/30...  Training Step: 22...  Training loss: 3.4457...  0.2184 sec/batch\n",
      "Epoch: 1/30...  Training Step: 23...  Training loss: 3.4116...  0.2032 sec/batch\n",
      "Epoch: 1/30...  Training Step: 24...  Training loss: 3.4083...  0.1965 sec/batch\n",
      "Epoch: 1/30...  Training Step: 25...  Training loss: 3.4652...  0.1745 sec/batch\n",
      "Epoch: 1/30...  Training Step: 26...  Training loss: 3.4226...  0.2478 sec/batch\n",
      "Epoch: 1/30...  Training Step: 27...  Training loss: 3.4222...  0.2574 sec/batch\n",
      "Epoch: 1/30...  Training Step: 28...  Training loss: 3.3912...  0.1729 sec/batch\n",
      "Epoch: 1/30...  Training Step: 29...  Training loss: 3.3752...  0.1921 sec/batch\n",
      "Epoch: 1/30...  Training Step: 30...  Training loss: 3.3937...  0.2685 sec/batch\n",
      "Epoch: 1/30...  Training Step: 31...  Training loss: 3.4079...  0.2232 sec/batch\n",
      "Epoch: 1/30...  Training Step: 32...  Training loss: 3.3949...  0.1618 sec/batch\n",
      "Epoch: 1/30...  Training Step: 33...  Training loss: 3.3891...  0.1939 sec/batch\n",
      "Epoch: 1/30...  Training Step: 34...  Training loss: 3.3547...  0.1730 sec/batch\n",
      "Epoch: 1/30...  Training Step: 35...  Training loss: 3.3968...  0.2004 sec/batch\n",
      "Epoch: 1/30...  Training Step: 36...  Training loss: 3.3961...  0.2690 sec/batch\n",
      "Epoch: 1/30...  Training Step: 37...  Training loss: 3.3916...  0.2397 sec/batch\n",
      "Epoch: 1/30...  Training Step: 38...  Training loss: 3.3850...  0.2376 sec/batch\n",
      "Epoch: 1/30...  Training Step: 39...  Training loss: 3.3735...  0.2977 sec/batch\n",
      "Epoch: 1/30...  Training Step: 40...  Training loss: 3.3397...  0.2716 sec/batch\n",
      "Epoch: 1/30...  Training Step: 41...  Training loss: 3.3408...  0.2658 sec/batch\n",
      "Epoch: 1/30...  Training Step: 42...  Training loss: 3.3752...  0.2773 sec/batch\n",
      "Epoch: 1/30...  Training Step: 43...  Training loss: 3.3516...  0.2746 sec/batch\n",
      "Epoch: 1/30...  Training Step: 44...  Training loss: 3.3670...  0.2186 sec/batch\n",
      "Epoch: 1/30...  Training Step: 45...  Training loss: 3.3602...  0.2044 sec/batch\n",
      "Epoch: 1/30...  Training Step: 46...  Training loss: 3.3584...  0.2394 sec/batch\n",
      "Epoch: 1/30...  Training Step: 47...  Training loss: 3.3314...  0.2898 sec/batch\n",
      "Epoch: 1/30...  Training Step: 48...  Training loss: 3.3323...  0.2440 sec/batch\n",
      "Epoch: 1/30...  Training Step: 49...  Training loss: 3.3509...  0.1571 sec/batch\n",
      "Epoch: 1/30...  Training Step: 50...  Training loss: 3.3179...  0.2390 sec/batch\n",
      "Epoch: 1/30...  Training Step: 51...  Training loss: 3.3154...  0.1903 sec/batch\n",
      "Epoch: 1/30...  Training Step: 52...  Training loss: 3.3232...  0.2344 sec/batch\n",
      "Epoch: 1/30...  Training Step: 53...  Training loss: 3.3081...  0.1925 sec/batch\n",
      "Epoch: 1/30...  Training Step: 54...  Training loss: 3.3022...  0.1798 sec/batch\n",
      "Epoch: 1/30...  Training Step: 55...  Training loss: 3.3164...  0.2713 sec/batch\n",
      "Epoch: 1/30...  Training Step: 56...  Training loss: 3.3125...  0.2021 sec/batch\n",
      "Epoch: 1/30...  Training Step: 57...  Training loss: 3.3118...  0.2261 sec/batch\n",
      "Epoch: 1/30...  Training Step: 58...  Training loss: 3.3276...  0.2181 sec/batch\n",
      "Epoch: 1/30...  Training Step: 59...  Training loss: 3.3166...  0.2635 sec/batch\n",
      "Epoch: 1/30...  Training Step: 60...  Training loss: 3.3329...  0.2599 sec/batch\n",
      "Epoch: 1/30...  Training Step: 61...  Training loss: 3.3074...  0.2012 sec/batch\n",
      "Epoch: 1/30...  Training Step: 62...  Training loss: 3.3115...  0.2012 sec/batch\n",
      "Epoch: 1/30...  Training Step: 63...  Training loss: 3.2902...  0.2111 sec/batch\n",
      "Epoch: 1/30...  Training Step: 64...  Training loss: 3.3217...  0.1689 sec/batch\n",
      "Epoch: 1/30...  Training Step: 65...  Training loss: 3.3322...  0.1975 sec/batch\n",
      "Epoch: 1/30...  Training Step: 66...  Training loss: 3.3192...  0.1672 sec/batch\n",
      "Epoch: 1/30...  Training Step: 67...  Training loss: 3.3020...  0.1765 sec/batch\n",
      "Epoch: 1/30...  Training Step: 68...  Training loss: 3.3085...  0.2002 sec/batch\n",
      "Epoch: 1/30...  Training Step: 69...  Training loss: 3.2822...  0.2834 sec/batch\n",
      "Epoch: 1/30...  Training Step: 70...  Training loss: 3.3000...  0.3013 sec/batch\n",
      "Epoch: 1/30...  Training Step: 71...  Training loss: 3.3045...  0.2626 sec/batch\n",
      "Epoch: 1/30...  Training Step: 72...  Training loss: 3.3089...  0.2106 sec/batch\n",
      "Epoch: 1/30...  Training Step: 73...  Training loss: 3.2943...  0.1983 sec/batch\n",
      "Epoch: 1/30...  Training Step: 74...  Training loss: 3.2960...  0.1711 sec/batch\n",
      "Epoch: 1/30...  Training Step: 75...  Training loss: 3.2780...  0.1957 sec/batch\n",
      "Epoch: 1/30...  Training Step: 76...  Training loss: 3.3042...  0.3045 sec/batch\n",
      "Epoch: 1/30...  Training Step: 77...  Training loss: 3.2931...  0.2978 sec/batch\n",
      "Epoch: 1/30...  Training Step: 78...  Training loss: 3.2693...  0.2369 sec/batch\n",
      "Epoch: 1/30...  Training Step: 79...  Training loss: 3.3109...  0.2127 sec/batch\n",
      "Epoch: 1/30...  Training Step: 80...  Training loss: 3.3028...  0.2232 sec/batch\n",
      "Epoch: 1/30...  Training Step: 81...  Training loss: 3.2746...  0.1935 sec/batch\n",
      "Epoch: 1/30...  Training Step: 82...  Training loss: 3.2679...  0.1671 sec/batch\n",
      "Epoch: 1/30...  Training Step: 83...  Training loss: 3.2747...  0.1628 sec/batch\n",
      "Epoch: 1/30...  Training Step: 84...  Training loss: 3.3275...  0.2425 sec/batch\n",
      "Epoch: 1/30...  Training Step: 85...  Training loss: 3.2988...  0.2274 sec/batch\n",
      "Epoch: 1/30...  Training Step: 86...  Training loss: 3.2919...  0.2369 sec/batch\n",
      "Epoch: 1/30...  Training Step: 87...  Training loss: 3.2746...  0.2908 sec/batch\n",
      "Epoch: 1/30...  Training Step: 88...  Training loss: 3.2743...  0.2449 sec/batch\n",
      "Epoch: 1/30...  Training Step: 89...  Training loss: 3.2746...  0.2530 sec/batch\n",
      "Epoch: 1/30...  Training Step: 90...  Training loss: 3.2442...  0.2088 sec/batch\n",
      "Epoch: 1/30...  Training Step: 91...  Training loss: 3.2393...  0.2774 sec/batch\n",
      "Epoch: 1/30...  Training Step: 92...  Training loss: 3.2504...  0.2313 sec/batch\n",
      "Epoch: 1/30...  Training Step: 93...  Training loss: 3.2554...  0.2018 sec/batch\n",
      "Epoch: 1/30...  Training Step: 94...  Training loss: 3.2492...  0.2326 sec/batch\n",
      "Epoch: 1/30...  Training Step: 95...  Training loss: 3.2584...  0.1792 sec/batch\n",
      "Epoch: 1/30...  Training Step: 96...  Training loss: 3.2205...  0.2468 sec/batch\n",
      "Epoch: 1/30...  Training Step: 97...  Training loss: 3.2195...  0.2207 sec/batch\n",
      "Epoch: 1/30...  Training Step: 98...  Training loss: 3.2326...  0.1899 sec/batch\n",
      "Epoch: 1/30...  Training Step: 99...  Training loss: 3.2235...  0.2490 sec/batch\n",
      "Epoch: 1/30...  Training Step: 100...  Training loss: 3.2277...  0.2567 sec/batch\n",
      "Epoch: 1/30...  Training Step: 101...  Training loss: 3.2394...  0.2215 sec/batch\n",
      "Epoch: 1/30...  Training Step: 102...  Training loss: 3.5148...  0.1766 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30...  Training Step: 103...  Training loss: 3.8288...  0.2437 sec/batch\n",
      "Epoch: 1/30...  Training Step: 104...  Training loss: 3.7700...  0.1580 sec/batch\n",
      "Epoch: 1/30...  Training Step: 105...  Training loss: 3.6463...  0.2126 sec/batch\n",
      "Epoch: 1/30...  Training Step: 106...  Training loss: 3.5880...  0.1955 sec/batch\n",
      "Epoch: 1/30...  Training Step: 107...  Training loss: 3.4745...  0.1558 sec/batch\n",
      "Epoch: 1/30...  Training Step: 108...  Training loss: 3.3973...  0.2311 sec/batch\n",
      "Epoch: 1/30...  Training Step: 109...  Training loss: 3.2636...  0.1685 sec/batch\n",
      "Epoch: 1/30...  Training Step: 110...  Training loss: 3.2536...  0.2097 sec/batch\n",
      "Epoch: 1/30...  Training Step: 111...  Training loss: 3.2231...  0.1792 sec/batch\n",
      "Epoch: 1/30...  Training Step: 112...  Training loss: 3.2347...  0.2429 sec/batch\n",
      "Epoch: 1/30...  Training Step: 113...  Training loss: 3.2414...  0.2803 sec/batch\n",
      "Epoch: 1/30...  Training Step: 114...  Training loss: 3.2465...  0.2693 sec/batch\n",
      "Epoch: 1/30...  Training Step: 115...  Training loss: 3.2435...  0.2496 sec/batch\n",
      "Epoch: 1/30...  Training Step: 116...  Training loss: 3.2291...  0.2409 sec/batch\n",
      "Epoch: 1/30...  Training Step: 117...  Training loss: 3.1935...  0.2367 sec/batch\n",
      "Epoch: 1/30...  Training Step: 118...  Training loss: 3.2121...  0.2207 sec/batch\n",
      "Epoch: 1/30...  Training Step: 119...  Training loss: 3.1833...  0.2569 sec/batch\n",
      "Epoch: 1/30...  Training Step: 120...  Training loss: 3.1931...  0.2502 sec/batch\n",
      "Epoch: 1/30...  Training Step: 121...  Training loss: 3.1693...  0.3740 sec/batch\n",
      "Epoch: 1/30...  Training Step: 122...  Training loss: 3.1702...  0.2990 sec/batch\n",
      "Epoch: 1/30...  Training Step: 123...  Training loss: 3.1792...  0.2049 sec/batch\n",
      "Epoch: 1/30...  Training Step: 124...  Training loss: 3.1667...  0.2496 sec/batch\n",
      "Epoch: 1/30...  Training Step: 125...  Training loss: 3.1974...  0.2835 sec/batch\n",
      "Epoch: 1/30...  Training Step: 126...  Training loss: 3.2760...  0.2107 sec/batch\n",
      "Epoch: 1/30...  Training Step: 127...  Training loss: 3.2312...  0.2196 sec/batch\n",
      "Epoch: 1/30...  Training Step: 128...  Training loss: 3.1912...  0.2301 sec/batch\n",
      "Epoch: 1/30...  Training Step: 129...  Training loss: 3.1697...  0.1579 sec/batch\n",
      "Epoch: 1/30...  Training Step: 130...  Training loss: 3.1811...  0.1704 sec/batch\n",
      "Epoch: 1/30...  Training Step: 131...  Training loss: 3.1821...  0.2128 sec/batch\n",
      "Epoch: 1/30...  Training Step: 132...  Training loss: 3.1430...  0.1969 sec/batch\n",
      "Epoch: 1/30...  Training Step: 133...  Training loss: 3.1360...  0.2122 sec/batch\n",
      "Epoch: 1/30...  Training Step: 134...  Training loss: 3.1548...  0.2501 sec/batch\n",
      "Epoch: 1/30...  Training Step: 135...  Training loss: 3.1735...  0.2613 sec/batch\n",
      "Epoch: 1/30...  Training Step: 136...  Training loss: 3.1408...  0.1940 sec/batch\n",
      "Epoch: 1/30...  Training Step: 137...  Training loss: 3.1364...  0.1894 sec/batch\n",
      "Epoch: 1/30...  Training Step: 138...  Training loss: 3.1131...  0.2352 sec/batch\n",
      "Epoch: 1/30...  Training Step: 139...  Training loss: 3.1474...  0.2531 sec/batch\n",
      "Epoch: 1/30...  Training Step: 140...  Training loss: 3.0976...  0.1619 sec/batch\n",
      "Epoch: 1/30...  Training Step: 141...  Training loss: 3.1120...  0.1912 sec/batch\n",
      "Epoch: 1/30...  Training Step: 142...  Training loss: 3.1072...  0.2528 sec/batch\n",
      "Epoch: 1/30...  Training Step: 143...  Training loss: 3.0979...  0.1669 sec/batch\n",
      "Epoch: 1/30...  Training Step: 144...  Training loss: 3.1111...  0.1916 sec/batch\n",
      "Epoch: 1/30...  Training Step: 145...  Training loss: 3.0881...  0.2071 sec/batch\n",
      "Epoch: 1/30...  Training Step: 146...  Training loss: 3.0812...  0.2192 sec/batch\n",
      "Epoch: 1/30...  Training Step: 147...  Training loss: 3.0901...  0.1599 sec/batch\n",
      "Epoch: 1/30...  Training Step: 148...  Training loss: 3.0864...  0.1835 sec/batch\n",
      "Epoch: 1/30...  Training Step: 149...  Training loss: 3.0920...  0.1704 sec/batch\n",
      "Epoch: 1/30...  Training Step: 150...  Training loss: 3.0617...  0.2144 sec/batch\n",
      "Epoch: 1/30...  Training Step: 151...  Training loss: 3.0993...  0.2359 sec/batch\n",
      "Epoch: 1/30...  Training Step: 152...  Training loss: 3.1492...  0.2804 sec/batch\n",
      "Epoch: 1/30...  Training Step: 153...  Training loss: 3.0734...  0.2424 sec/batch\n",
      "Epoch: 1/30...  Training Step: 154...  Training loss: 3.0454...  0.2345 sec/batch\n",
      "Epoch: 1/30...  Training Step: 155...  Training loss: 3.0315...  0.2305 sec/batch\n",
      "Epoch: 1/30...  Training Step: 156...  Training loss: 3.0211...  0.1687 sec/batch\n",
      "Epoch: 1/30...  Training Step: 157...  Training loss: 3.0358...  0.1862 sec/batch\n",
      "Epoch: 1/30...  Training Step: 158...  Training loss: 3.0371...  0.1915 sec/batch\n",
      "Epoch: 1/30...  Training Step: 159...  Training loss: 3.0249...  0.1689 sec/batch\n",
      "Epoch: 1/30...  Training Step: 160...  Training loss: 3.0246...  0.1851 sec/batch\n",
      "Epoch: 1/30...  Training Step: 161...  Training loss: 3.0064...  0.2267 sec/batch\n",
      "Epoch: 1/30...  Training Step: 162...  Training loss: 2.9823...  0.2212 sec/batch\n",
      "Epoch: 1/30...  Training Step: 163...  Training loss: 2.9863...  0.2015 sec/batch\n",
      "Epoch: 1/30...  Training Step: 164...  Training loss: 2.9905...  0.2735 sec/batch\n",
      "Epoch: 1/30...  Training Step: 165...  Training loss: 2.9597...  0.2535 sec/batch\n",
      "Epoch: 1/30...  Training Step: 166...  Training loss: 2.9624...  0.2552 sec/batch\n",
      "Epoch: 1/30...  Training Step: 167...  Training loss: 2.9777...  0.1562 sec/batch\n",
      "Epoch: 1/30...  Training Step: 168...  Training loss: 2.9482...  0.2053 sec/batch\n",
      "Epoch: 1/30...  Training Step: 169...  Training loss: 2.9655...  0.1988 sec/batch\n",
      "Epoch: 1/30...  Training Step: 170...  Training loss: 2.9409...  0.1860 sec/batch\n",
      "Epoch: 2/30...  Training Step: 171...  Training loss: 3.0372...  0.2024 sec/batch\n",
      "Epoch: 2/30...  Training Step: 172...  Training loss: 2.9517...  0.2173 sec/batch\n",
      "Epoch: 2/30...  Training Step: 173...  Training loss: 2.9303...  0.3053 sec/batch\n",
      "Epoch: 2/30...  Training Step: 174...  Training loss: 2.9638...  0.1956 sec/batch\n",
      "Epoch: 2/30...  Training Step: 175...  Training loss: 2.9391...  0.1871 sec/batch\n",
      "Epoch: 2/30...  Training Step: 176...  Training loss: 2.9477...  0.2698 sec/batch\n",
      "Epoch: 2/30...  Training Step: 177...  Training loss: 2.9355...  0.2701 sec/batch\n",
      "Epoch: 2/30...  Training Step: 178...  Training loss: 2.9165...  0.2791 sec/batch\n",
      "Epoch: 2/30...  Training Step: 179...  Training loss: 2.9015...  0.3577 sec/batch\n",
      "Epoch: 2/30...  Training Step: 180...  Training loss: 2.8771...  0.1907 sec/batch\n",
      "Epoch: 2/30...  Training Step: 181...  Training loss: 2.8951...  0.3028 sec/batch\n",
      "Epoch: 2/30...  Training Step: 182...  Training loss: 2.8912...  0.1596 sec/batch\n",
      "Epoch: 2/30...  Training Step: 183...  Training loss: 2.8815...  0.1629 sec/batch\n",
      "Epoch: 2/30...  Training Step: 184...  Training loss: 2.8617...  0.2060 sec/batch\n",
      "Epoch: 2/30...  Training Step: 185...  Training loss: 2.8640...  0.2457 sec/batch\n",
      "Epoch: 2/30...  Training Step: 186...  Training loss: 2.8773...  0.2236 sec/batch\n",
      "Epoch: 2/30...  Training Step: 187...  Training loss: 2.8394...  0.1884 sec/batch\n",
      "Epoch: 2/30...  Training Step: 188...  Training loss: 2.8378...  0.1617 sec/batch\n",
      "Epoch: 2/30...  Training Step: 189...  Training loss: 2.8531...  0.1917 sec/batch\n",
      "Epoch: 2/30...  Training Step: 190...  Training loss: 2.8385...  0.2500 sec/batch\n",
      "Epoch: 2/30...  Training Step: 191...  Training loss: 2.8199...  0.2160 sec/batch\n",
      "Epoch: 2/30...  Training Step: 192...  Training loss: 2.8264...  0.2391 sec/batch\n",
      "Epoch: 2/30...  Training Step: 193...  Training loss: 2.8047...  0.2546 sec/batch\n",
      "Epoch: 2/30...  Training Step: 194...  Training loss: 2.7924...  0.1589 sec/batch\n",
      "Epoch: 2/30...  Training Step: 195...  Training loss: 2.8206...  0.2070 sec/batch\n",
      "Epoch: 2/30...  Training Step: 196...  Training loss: 2.8081...  0.2945 sec/batch\n",
      "Epoch: 2/30...  Training Step: 197...  Training loss: 2.8015...  0.2357 sec/batch\n",
      "Epoch: 2/30...  Training Step: 198...  Training loss: 2.7738...  0.1970 sec/batch\n",
      "Epoch: 2/30...  Training Step: 199...  Training loss: 2.7652...  0.2172 sec/batch\n",
      "Epoch: 2/30...  Training Step: 200...  Training loss: 2.7374...  0.2132 sec/batch\n",
      "Epoch: 2/30...  Training Step: 201...  Training loss: 2.7599...  0.2362 sec/batch\n",
      "Epoch: 2/30...  Training Step: 202...  Training loss: 2.7497...  0.2254 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30...  Training Step: 203...  Training loss: 2.7434...  0.2690 sec/batch\n",
      "Epoch: 2/30...  Training Step: 204...  Training loss: 2.7220...  0.1998 sec/batch\n",
      "Epoch: 2/30...  Training Step: 205...  Training loss: 2.7286...  0.2150 sec/batch\n",
      "Epoch: 2/30...  Training Step: 206...  Training loss: 2.7461...  0.2697 sec/batch\n",
      "Epoch: 2/30...  Training Step: 207...  Training loss: 2.7331...  0.2774 sec/batch\n",
      "Epoch: 2/30...  Training Step: 208...  Training loss: 2.7159...  0.2515 sec/batch\n",
      "Epoch: 2/30...  Training Step: 209...  Training loss: 2.7216...  0.3241 sec/batch\n",
      "Epoch: 2/30...  Training Step: 210...  Training loss: 2.6989...  0.2039 sec/batch\n",
      "Epoch: 2/30...  Training Step: 211...  Training loss: 2.6953...  0.1740 sec/batch\n",
      "Epoch: 2/30...  Training Step: 212...  Training loss: 2.6995...  0.2502 sec/batch\n",
      "Epoch: 2/30...  Training Step: 213...  Training loss: 2.6720...  0.1954 sec/batch\n",
      "Epoch: 2/30...  Training Step: 214...  Training loss: 2.6732...  0.2288 sec/batch\n",
      "Epoch: 2/30...  Training Step: 215...  Training loss: 2.6613...  0.1558 sec/batch\n",
      "Epoch: 2/30...  Training Step: 216...  Training loss: 2.6883...  0.2474 sec/batch\n",
      "Epoch: 2/30...  Training Step: 217...  Training loss: 2.6518...  0.2242 sec/batch\n",
      "Epoch: 2/30...  Training Step: 218...  Training loss: 2.6536...  0.1766 sec/batch\n",
      "Epoch: 2/30...  Training Step: 219...  Training loss: 2.6651...  0.2074 sec/batch\n",
      "Epoch: 2/30...  Training Step: 220...  Training loss: 2.6468...  0.2506 sec/batch\n",
      "Epoch: 2/30...  Training Step: 221...  Training loss: 2.6574...  0.2161 sec/batch\n",
      "Epoch: 2/30...  Training Step: 222...  Training loss: 2.6471...  0.1690 sec/batch\n",
      "Epoch: 2/30...  Training Step: 223...  Training loss: 2.6237...  0.2330 sec/batch\n",
      "Epoch: 2/30...  Training Step: 224...  Training loss: 2.6542...  0.1750 sec/batch\n",
      "Epoch: 2/30...  Training Step: 225...  Training loss: 2.6311...  0.2653 sec/batch\n",
      "Epoch: 2/30...  Training Step: 226...  Training loss: 2.6430...  0.2780 sec/batch\n",
      "Epoch: 2/30...  Training Step: 227...  Training loss: 2.6404...  0.2691 sec/batch\n",
      "Epoch: 2/30...  Training Step: 228...  Training loss: 2.6312...  0.2657 sec/batch\n",
      "Epoch: 2/30...  Training Step: 229...  Training loss: 2.6361...  0.2232 sec/batch\n",
      "Epoch: 2/30...  Training Step: 230...  Training loss: 2.6303...  0.2614 sec/batch\n",
      "Epoch: 2/30...  Training Step: 231...  Training loss: 2.6234...  0.2078 sec/batch\n",
      "Epoch: 2/30...  Training Step: 232...  Training loss: 2.6102...  0.2183 sec/batch\n",
      "Epoch: 2/30...  Training Step: 233...  Training loss: 2.5989...  0.1705 sec/batch\n",
      "Epoch: 2/30...  Training Step: 234...  Training loss: 2.6029...  0.2754 sec/batch\n",
      "Epoch: 2/30...  Training Step: 235...  Training loss: 2.6070...  0.2543 sec/batch\n",
      "Epoch: 2/30...  Training Step: 236...  Training loss: 2.5958...  0.2122 sec/batch\n",
      "Epoch: 2/30...  Training Step: 237...  Training loss: 2.5911...  0.2302 sec/batch\n",
      "Epoch: 2/30...  Training Step: 238...  Training loss: 2.5872...  0.2263 sec/batch\n",
      "Epoch: 2/30...  Training Step: 239...  Training loss: 2.5991...  0.2576 sec/batch\n",
      "Epoch: 2/30...  Training Step: 240...  Training loss: 2.5706...  0.2615 sec/batch\n",
      "Epoch: 2/30...  Training Step: 241...  Training loss: 2.5876...  0.2444 sec/batch\n",
      "Epoch: 2/30...  Training Step: 242...  Training loss: 2.5847...  0.1776 sec/batch\n",
      "Epoch: 2/30...  Training Step: 243...  Training loss: 2.5863...  0.1735 sec/batch\n",
      "Epoch: 2/30...  Training Step: 244...  Training loss: 2.6086...  0.1981 sec/batch\n",
      "Epoch: 2/30...  Training Step: 245...  Training loss: 2.5840...  0.3063 sec/batch\n",
      "Epoch: 2/30...  Training Step: 246...  Training loss: 2.5824...  0.1978 sec/batch\n",
      "Epoch: 2/30...  Training Step: 247...  Training loss: 2.5789...  0.1840 sec/batch\n",
      "Epoch: 2/30...  Training Step: 248...  Training loss: 2.5673...  0.1562 sec/batch\n",
      "Epoch: 2/30...  Training Step: 249...  Training loss: 2.5751...  0.1901 sec/batch\n",
      "Epoch: 2/30...  Training Step: 250...  Training loss: 2.5832...  0.1944 sec/batch\n",
      "Epoch: 2/30...  Training Step: 251...  Training loss: 2.5482...  0.2753 sec/batch\n",
      "Epoch: 2/30...  Training Step: 252...  Training loss: 2.5749...  0.2469 sec/batch\n",
      "Epoch: 2/30...  Training Step: 253...  Training loss: 2.5642...  0.1584 sec/batch\n",
      "Epoch: 2/30...  Training Step: 254...  Training loss: 2.5827...  0.2833 sec/batch\n",
      "Epoch: 2/30...  Training Step: 255...  Training loss: 2.5830...  0.2135 sec/batch\n",
      "Epoch: 2/30...  Training Step: 256...  Training loss: 2.5602...  0.1740 sec/batch\n",
      "Epoch: 2/30...  Training Step: 257...  Training loss: 2.5549...  0.2690 sec/batch\n",
      "Epoch: 2/30...  Training Step: 258...  Training loss: 2.5475...  0.1856 sec/batch\n",
      "Epoch: 2/30...  Training Step: 259...  Training loss: 2.5460...  0.2154 sec/batch\n",
      "Epoch: 2/30...  Training Step: 260...  Training loss: 2.5096...  0.2678 sec/batch\n",
      "Epoch: 2/30...  Training Step: 261...  Training loss: 2.5340...  0.2153 sec/batch\n",
      "Epoch: 2/30...  Training Step: 262...  Training loss: 2.5324...  0.1562 sec/batch\n",
      "Epoch: 2/30...  Training Step: 263...  Training loss: 2.5610...  0.1573 sec/batch\n",
      "Epoch: 2/30...  Training Step: 264...  Training loss: 2.5639...  0.2537 sec/batch\n",
      "Epoch: 2/30...  Training Step: 265...  Training loss: 2.5524...  0.2106 sec/batch\n",
      "Epoch: 2/30...  Training Step: 266...  Training loss: 2.5102...  0.2647 sec/batch\n",
      "Epoch: 2/30...  Training Step: 267...  Training loss: 2.5192...  0.1956 sec/batch\n",
      "Epoch: 2/30...  Training Step: 268...  Training loss: 2.5455...  0.1933 sec/batch\n",
      "Epoch: 2/30...  Training Step: 269...  Training loss: 2.5203...  0.1544 sec/batch\n",
      "Epoch: 2/30...  Training Step: 270...  Training loss: 2.5377...  0.1879 sec/batch\n",
      "Epoch: 2/30...  Training Step: 271...  Training loss: 2.5282...  0.2349 sec/batch\n",
      "Epoch: 2/30...  Training Step: 272...  Training loss: 2.5028...  0.2196 sec/batch\n",
      "Epoch: 2/30...  Training Step: 273...  Training loss: 2.5068...  0.2041 sec/batch\n",
      "Epoch: 2/30...  Training Step: 274...  Training loss: 2.5186...  0.2119 sec/batch\n",
      "Epoch: 2/30...  Training Step: 275...  Training loss: 2.4970...  0.2508 sec/batch\n",
      "Epoch: 2/30...  Training Step: 276...  Training loss: 2.4972...  0.1996 sec/batch\n",
      "Epoch: 2/30...  Training Step: 277...  Training loss: 2.4900...  0.2580 sec/batch\n",
      "Epoch: 2/30...  Training Step: 278...  Training loss: 2.4813...  0.2603 sec/batch\n",
      "Epoch: 2/30...  Training Step: 279...  Training loss: 2.4844...  0.2567 sec/batch\n",
      "Epoch: 2/30...  Training Step: 280...  Training loss: 2.4914...  0.1915 sec/batch\n",
      "Epoch: 2/30...  Training Step: 281...  Training loss: 2.4790...  0.2487 sec/batch\n",
      "Epoch: 2/30...  Training Step: 282...  Training loss: 2.4829...  0.2397 sec/batch\n",
      "Epoch: 2/30...  Training Step: 283...  Training loss: 2.4780...  0.2530 sec/batch\n",
      "Epoch: 2/30...  Training Step: 284...  Training loss: 2.5165...  0.2433 sec/batch\n",
      "Epoch: 2/30...  Training Step: 285...  Training loss: 2.5085...  0.1657 sec/batch\n",
      "Epoch: 2/30...  Training Step: 286...  Training loss: 2.4839...  0.2208 sec/batch\n",
      "Epoch: 2/30...  Training Step: 287...  Training loss: 2.4854...  0.2097 sec/batch\n",
      "Epoch: 2/30...  Training Step: 288...  Training loss: 2.4721...  0.2466 sec/batch\n",
      "Epoch: 2/30...  Training Step: 289...  Training loss: 2.4725...  0.1834 sec/batch\n",
      "Epoch: 2/30...  Training Step: 290...  Training loss: 2.4840...  0.3155 sec/batch\n",
      "Epoch: 2/30...  Training Step: 291...  Training loss: 2.4612...  0.2748 sec/batch\n",
      "Epoch: 2/30...  Training Step: 292...  Training loss: 2.4706...  0.2051 sec/batch\n",
      "Epoch: 2/30...  Training Step: 293...  Training loss: 2.4801...  0.2513 sec/batch\n",
      "Epoch: 2/30...  Training Step: 294...  Training loss: 2.4829...  0.2539 sec/batch\n",
      "Epoch: 2/30...  Training Step: 295...  Training loss: 2.4741...  0.1961 sec/batch\n",
      "Epoch: 2/30...  Training Step: 296...  Training loss: 2.4771...  0.1798 sec/batch\n",
      "Epoch: 2/30...  Training Step: 297...  Training loss: 2.4871...  0.2825 sec/batch\n",
      "Epoch: 2/30...  Training Step: 298...  Training loss: 2.4661...  0.3152 sec/batch\n",
      "Epoch: 2/30...  Training Step: 299...  Training loss: 2.4778...  0.2526 sec/batch\n",
      "Epoch: 2/30...  Training Step: 300...  Training loss: 2.4786...  0.2581 sec/batch\n",
      "Epoch: 2/30...  Training Step: 301...  Training loss: 2.4875...  0.2102 sec/batch\n",
      "Epoch: 2/30...  Training Step: 302...  Training loss: 2.4326...  0.2461 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30...  Training Step: 303...  Training loss: 2.4387...  0.2277 sec/batch\n",
      "Epoch: 2/30...  Training Step: 304...  Training loss: 2.4836...  0.2720 sec/batch\n",
      "Epoch: 2/30...  Training Step: 305...  Training loss: 2.4754...  0.2317 sec/batch\n",
      "Epoch: 2/30...  Training Step: 306...  Training loss: 2.4751...  0.2181 sec/batch\n",
      "Epoch: 2/30...  Training Step: 307...  Training loss: 2.4387...  0.1569 sec/batch\n",
      "Epoch: 2/30...  Training Step: 308...  Training loss: 2.4396...  0.2585 sec/batch\n",
      "Epoch: 2/30...  Training Step: 309...  Training loss: 2.4463...  0.2909 sec/batch\n",
      "Epoch: 2/30...  Training Step: 310...  Training loss: 2.4377...  0.1860 sec/batch\n",
      "Epoch: 2/30...  Training Step: 311...  Training loss: 2.4330...  0.2384 sec/batch\n",
      "Epoch: 2/30...  Training Step: 312...  Training loss: 2.4429...  0.2065 sec/batch\n",
      "Epoch: 2/30...  Training Step: 313...  Training loss: 2.4358...  0.1637 sec/batch\n",
      "Epoch: 2/30...  Training Step: 314...  Training loss: 2.4515...  0.1892 sec/batch\n",
      "Epoch: 2/30...  Training Step: 315...  Training loss: 2.4318...  0.1618 sec/batch\n",
      "Epoch: 2/30...  Training Step: 316...  Training loss: 2.4146...  0.2029 sec/batch\n",
      "Epoch: 2/30...  Training Step: 317...  Training loss: 2.4645...  0.1682 sec/batch\n",
      "Epoch: 2/30...  Training Step: 318...  Training loss: 2.4204...  0.2479 sec/batch\n",
      "Epoch: 2/30...  Training Step: 319...  Training loss: 2.4321...  0.2381 sec/batch\n",
      "Epoch: 2/30...  Training Step: 320...  Training loss: 2.4355...  0.2065 sec/batch\n",
      "Epoch: 2/30...  Training Step: 321...  Training loss: 2.4594...  0.1687 sec/batch\n",
      "Epoch: 2/30...  Training Step: 322...  Training loss: 2.4538...  0.2656 sec/batch\n",
      "Epoch: 2/30...  Training Step: 323...  Training loss: 2.4313...  0.1746 sec/batch\n",
      "Epoch: 2/30...  Training Step: 324...  Training loss: 2.4247...  0.2369 sec/batch\n",
      "Epoch: 2/30...  Training Step: 325...  Training loss: 2.4085...  0.2750 sec/batch\n",
      "Epoch: 2/30...  Training Step: 326...  Training loss: 2.4016...  0.2336 sec/batch\n",
      "Epoch: 2/30...  Training Step: 327...  Training loss: 2.4224...  0.2420 sec/batch\n",
      "Epoch: 2/30...  Training Step: 328...  Training loss: 2.4403...  0.2494 sec/batch\n",
      "Epoch: 2/30...  Training Step: 329...  Training loss: 2.4082...  0.2519 sec/batch\n",
      "Epoch: 2/30...  Training Step: 330...  Training loss: 2.4212...  0.1700 sec/batch\n",
      "Epoch: 2/30...  Training Step: 331...  Training loss: 2.4097...  0.2444 sec/batch\n",
      "Epoch: 2/30...  Training Step: 332...  Training loss: 2.4041...  0.1720 sec/batch\n",
      "Epoch: 2/30...  Training Step: 333...  Training loss: 2.4032...  0.2484 sec/batch\n",
      "Epoch: 2/30...  Training Step: 334...  Training loss: 2.3899...  0.2036 sec/batch\n",
      "Epoch: 2/30...  Training Step: 335...  Training loss: 2.3926...  0.2578 sec/batch\n",
      "Epoch: 2/30...  Training Step: 336...  Training loss: 2.3823...  0.2488 sec/batch\n",
      "Epoch: 2/30...  Training Step: 337...  Training loss: 2.3909...  0.1543 sec/batch\n",
      "Epoch: 2/30...  Training Step: 338...  Training loss: 2.3817...  0.2773 sec/batch\n",
      "Epoch: 2/30...  Training Step: 339...  Training loss: 2.3968...  0.2549 sec/batch\n",
      "Epoch: 2/30...  Training Step: 340...  Training loss: 2.3641...  0.1828 sec/batch\n",
      "Epoch: 3/30...  Training Step: 341...  Training loss: 2.4893...  0.2315 sec/batch\n",
      "Epoch: 3/30...  Training Step: 342...  Training loss: 2.3937...  0.2334 sec/batch\n",
      "Epoch: 3/30...  Training Step: 343...  Training loss: 2.3781...  0.1892 sec/batch\n",
      "Epoch: 3/30...  Training Step: 344...  Training loss: 2.4187...  0.2841 sec/batch\n",
      "Epoch: 3/30...  Training Step: 345...  Training loss: 2.3848...  0.2084 sec/batch\n",
      "Epoch: 3/30...  Training Step: 346...  Training loss: 2.4226...  0.1889 sec/batch\n",
      "Epoch: 3/30...  Training Step: 347...  Training loss: 2.4029...  0.2807 sec/batch\n",
      "Epoch: 3/30...  Training Step: 348...  Training loss: 2.3825...  0.2826 sec/batch\n",
      "Epoch: 3/30...  Training Step: 349...  Training loss: 2.3639...  0.2464 sec/batch\n",
      "Epoch: 3/30...  Training Step: 350...  Training loss: 2.3701...  0.1693 sec/batch\n",
      "Epoch: 3/30...  Training Step: 351...  Training loss: 2.3656...  0.2076 sec/batch\n",
      "Epoch: 3/30...  Training Step: 352...  Training loss: 2.3833...  0.1569 sec/batch\n",
      "Epoch: 3/30...  Training Step: 353...  Training loss: 2.3762...  0.2367 sec/batch\n",
      "Epoch: 3/30...  Training Step: 354...  Training loss: 2.3586...  0.2740 sec/batch\n",
      "Epoch: 3/30...  Training Step: 355...  Training loss: 2.3880...  0.2853 sec/batch\n",
      "Epoch: 3/30...  Training Step: 356...  Training loss: 2.3958...  0.2493 sec/batch\n",
      "Epoch: 3/30...  Training Step: 357...  Training loss: 2.3311...  0.2134 sec/batch\n",
      "Epoch: 3/30...  Training Step: 358...  Training loss: 2.3753...  0.2619 sec/batch\n",
      "Epoch: 3/30...  Training Step: 359...  Training loss: 2.3841...  0.2085 sec/batch\n",
      "Epoch: 3/30...  Training Step: 360...  Training loss: 2.3585...  0.2446 sec/batch\n",
      "Epoch: 3/30...  Training Step: 361...  Training loss: 2.3504...  0.2746 sec/batch\n",
      "Epoch: 3/30...  Training Step: 362...  Training loss: 2.3281...  0.2540 sec/batch\n",
      "Epoch: 3/30...  Training Step: 363...  Training loss: 2.3535...  0.2689 sec/batch\n",
      "Epoch: 3/30...  Training Step: 364...  Training loss: 2.3271...  0.2384 sec/batch\n",
      "Epoch: 3/30...  Training Step: 365...  Training loss: 2.3743...  0.2166 sec/batch\n",
      "Epoch: 3/30...  Training Step: 366...  Training loss: 2.3544...  0.2588 sec/batch\n",
      "Epoch: 3/30...  Training Step: 367...  Training loss: 2.3799...  0.1592 sec/batch\n",
      "Epoch: 3/30...  Training Step: 368...  Training loss: 2.3400...  0.2428 sec/batch\n",
      "Epoch: 3/30...  Training Step: 369...  Training loss: 2.3568...  0.1601 sec/batch\n",
      "Epoch: 3/30...  Training Step: 370...  Training loss: 2.3196...  0.2633 sec/batch\n",
      "Epoch: 3/30...  Training Step: 371...  Training loss: 2.3454...  0.2507 sec/batch\n",
      "Epoch: 3/30...  Training Step: 372...  Training loss: 2.3561...  0.2865 sec/batch\n",
      "Epoch: 3/30...  Training Step: 373...  Training loss: 2.3393...  0.2707 sec/batch\n",
      "Epoch: 3/30...  Training Step: 374...  Training loss: 2.3235...  0.2388 sec/batch\n",
      "Epoch: 3/30...  Training Step: 375...  Training loss: 2.3341...  0.2716 sec/batch\n",
      "Epoch: 3/30...  Training Step: 376...  Training loss: 2.3544...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 377...  Training loss: 2.3431...  0.1912 sec/batch\n",
      "Epoch: 3/30...  Training Step: 378...  Training loss: 2.3345...  0.2616 sec/batch\n",
      "Epoch: 3/30...  Training Step: 379...  Training loss: 2.3477...  0.2364 sec/batch\n",
      "Epoch: 3/30...  Training Step: 380...  Training loss: 2.3370...  0.1527 sec/batch\n",
      "Epoch: 3/30...  Training Step: 381...  Training loss: 2.3477...  0.2279 sec/batch\n",
      "Epoch: 3/30...  Training Step: 382...  Training loss: 2.3164...  0.2214 sec/batch\n",
      "Epoch: 3/30...  Training Step: 383...  Training loss: 2.3105...  0.2664 sec/batch\n",
      "Epoch: 3/30...  Training Step: 384...  Training loss: 2.3420...  0.3004 sec/batch\n",
      "Epoch: 3/30...  Training Step: 385...  Training loss: 2.3150...  0.2511 sec/batch\n",
      "Epoch: 3/30...  Training Step: 386...  Training loss: 2.3279...  0.1954 sec/batch\n",
      "Epoch: 3/30...  Training Step: 387...  Training loss: 2.3164...  0.1614 sec/batch\n",
      "Epoch: 3/30...  Training Step: 388...  Training loss: 2.3133...  0.1812 sec/batch\n",
      "Epoch: 3/30...  Training Step: 389...  Training loss: 2.3213...  0.2515 sec/batch\n",
      "Epoch: 3/30...  Training Step: 390...  Training loss: 2.3243...  0.1777 sec/batch\n",
      "Epoch: 3/30...  Training Step: 391...  Training loss: 2.3436...  0.1583 sec/batch\n",
      "Epoch: 3/30...  Training Step: 392...  Training loss: 2.3293...  0.1782 sec/batch\n",
      "Epoch: 3/30...  Training Step: 393...  Training loss: 2.2944...  0.2926 sec/batch\n",
      "Epoch: 3/30...  Training Step: 394...  Training loss: 2.3340...  0.2478 sec/batch\n",
      "Epoch: 3/30...  Training Step: 395...  Training loss: 2.3044...  0.2378 sec/batch\n",
      "Epoch: 3/30...  Training Step: 396...  Training loss: 2.3269...  0.2344 sec/batch\n",
      "Epoch: 3/30...  Training Step: 397...  Training loss: 2.3174...  0.1977 sec/batch\n",
      "Epoch: 3/30...  Training Step: 398...  Training loss: 2.3053...  0.2171 sec/batch\n",
      "Epoch: 3/30...  Training Step: 399...  Training loss: 2.3310...  0.2724 sec/batch\n",
      "Epoch: 3/30...  Training Step: 400...  Training loss: 2.3098...  0.2759 sec/batch\n",
      "Epoch: 3/30...  Training Step: 401...  Training loss: 2.3155...  0.1696 sec/batch\n",
      "Epoch: 3/30...  Training Step: 402...  Training loss: 2.3024...  0.1788 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30...  Training Step: 403...  Training loss: 2.2942...  0.1646 sec/batch\n",
      "Epoch: 3/30...  Training Step: 404...  Training loss: 2.2840...  0.2340 sec/batch\n",
      "Epoch: 3/30...  Training Step: 405...  Training loss: 2.3145...  0.2608 sec/batch\n",
      "Epoch: 3/30...  Training Step: 406...  Training loss: 2.2991...  0.2679 sec/batch\n",
      "Epoch: 3/30...  Training Step: 407...  Training loss: 2.3105...  0.1857 sec/batch\n",
      "Epoch: 3/30...  Training Step: 408...  Training loss: 2.2730...  0.1899 sec/batch\n",
      "Epoch: 3/30...  Training Step: 409...  Training loss: 2.3078...  0.2445 sec/batch\n",
      "Epoch: 3/30...  Training Step: 410...  Training loss: 2.2686...  0.2447 sec/batch\n",
      "Epoch: 3/30...  Training Step: 411...  Training loss: 2.2840...  0.1581 sec/batch\n",
      "Epoch: 3/30...  Training Step: 412...  Training loss: 2.2755...  0.1713 sec/batch\n",
      "Epoch: 3/30...  Training Step: 413...  Training loss: 2.2984...  0.1938 sec/batch\n",
      "Epoch: 3/30...  Training Step: 414...  Training loss: 2.3180...  0.2541 sec/batch\n",
      "Epoch: 3/30...  Training Step: 415...  Training loss: 2.2920...  0.2384 sec/batch\n",
      "Epoch: 3/30...  Training Step: 416...  Training loss: 2.2749...  0.1686 sec/batch\n",
      "Epoch: 3/30...  Training Step: 417...  Training loss: 2.2797...  0.2373 sec/batch\n",
      "Epoch: 3/30...  Training Step: 418...  Training loss: 2.2895...  0.2392 sec/batch\n",
      "Epoch: 3/30...  Training Step: 419...  Training loss: 2.2832...  0.1847 sec/batch\n",
      "Epoch: 3/30...  Training Step: 420...  Training loss: 2.2971...  0.2395 sec/batch\n",
      "Epoch: 3/30...  Training Step: 421...  Training loss: 2.2789...  0.1589 sec/batch\n",
      "Epoch: 3/30...  Training Step: 422...  Training loss: 2.2983...  0.1566 sec/batch\n",
      "Epoch: 3/30...  Training Step: 423...  Training loss: 2.2866...  0.1876 sec/batch\n",
      "Epoch: 3/30...  Training Step: 424...  Training loss: 2.2611...  0.1610 sec/batch\n",
      "Epoch: 3/30...  Training Step: 425...  Training loss: 2.3051...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 426...  Training loss: 2.2585...  0.2691 sec/batch\n",
      "Epoch: 3/30...  Training Step: 427...  Training loss: 2.2736...  0.2809 sec/batch\n",
      "Epoch: 3/30...  Training Step: 428...  Training loss: 2.2808...  0.2586 sec/batch\n",
      "Epoch: 3/30...  Training Step: 429...  Training loss: 2.2639...  0.2350 sec/batch\n",
      "Epoch: 3/30...  Training Step: 430...  Training loss: 2.2399...  0.2406 sec/batch\n",
      "Epoch: 3/30...  Training Step: 431...  Training loss: 2.2647...  0.1609 sec/batch\n",
      "Epoch: 3/30...  Training Step: 432...  Training loss: 2.2605...  0.1630 sec/batch\n",
      "Epoch: 3/30...  Training Step: 433...  Training loss: 2.2747...  0.2497 sec/batch\n",
      "Epoch: 3/30...  Training Step: 434...  Training loss: 2.2964...  0.2871 sec/batch\n",
      "Epoch: 3/30...  Training Step: 435...  Training loss: 2.2672...  0.1874 sec/batch\n",
      "Epoch: 3/30...  Training Step: 436...  Training loss: 2.2485...  0.2864 sec/batch\n",
      "Epoch: 3/30...  Training Step: 437...  Training loss: 2.2511...  0.2694 sec/batch\n",
      "Epoch: 3/30...  Training Step: 438...  Training loss: 2.2758...  0.2040 sec/batch\n",
      "Epoch: 3/30...  Training Step: 439...  Training loss: 2.2501...  0.1628 sec/batch\n",
      "Epoch: 3/30...  Training Step: 440...  Training loss: 2.2841...  0.1726 sec/batch\n",
      "Epoch: 3/30...  Training Step: 441...  Training loss: 2.2768...  0.1538 sec/batch\n",
      "Epoch: 3/30...  Training Step: 442...  Training loss: 2.2375...  0.2733 sec/batch\n",
      "Epoch: 3/30...  Training Step: 443...  Training loss: 2.2438...  0.3005 sec/batch\n",
      "Epoch: 3/30...  Training Step: 444...  Training loss: 2.2444...  0.2762 sec/batch\n",
      "Epoch: 3/30...  Training Step: 445...  Training loss: 2.2473...  0.3263 sec/batch\n",
      "Epoch: 3/30...  Training Step: 446...  Training loss: 2.2298...  0.1730 sec/batch\n",
      "Epoch: 3/30...  Training Step: 447...  Training loss: 2.2406...  0.2561 sec/batch\n",
      "Epoch: 3/30...  Training Step: 448...  Training loss: 2.2165...  0.1813 sec/batch\n",
      "Epoch: 3/30...  Training Step: 449...  Training loss: 2.2271...  0.1831 sec/batch\n",
      "Epoch: 3/30...  Training Step: 450...  Training loss: 2.2280...  0.1985 sec/batch\n",
      "Epoch: 3/30...  Training Step: 451...  Training loss: 2.2199...  0.2319 sec/batch\n",
      "Epoch: 3/30...  Training Step: 452...  Training loss: 2.1972...  0.2754 sec/batch\n",
      "Epoch: 3/30...  Training Step: 453...  Training loss: 2.2235...  0.1562 sec/batch\n",
      "Epoch: 3/30...  Training Step: 454...  Training loss: 2.2627...  0.2746 sec/batch\n",
      "Epoch: 3/30...  Training Step: 455...  Training loss: 2.2545...  0.1657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 456...  Training loss: 2.2215...  0.1626 sec/batch\n",
      "Epoch: 3/30...  Training Step: 457...  Training loss: 2.2332...  0.2274 sec/batch\n",
      "Epoch: 3/30...  Training Step: 458...  Training loss: 2.2338...  0.2685 sec/batch\n",
      "Epoch: 3/30...  Training Step: 459...  Training loss: 2.2012...  0.2520 sec/batch\n",
      "Epoch: 3/30...  Training Step: 460...  Training loss: 2.2390...  0.2146 sec/batch\n",
      "Epoch: 3/30...  Training Step: 461...  Training loss: 2.2098...  0.2272 sec/batch\n",
      "Epoch: 3/30...  Training Step: 462...  Training loss: 2.2190...  0.2344 sec/batch\n",
      "Epoch: 3/30...  Training Step: 463...  Training loss: 2.2110...  0.1577 sec/batch\n",
      "Epoch: 3/30...  Training Step: 464...  Training loss: 2.2452...  0.2638 sec/batch\n",
      "Epoch: 3/30...  Training Step: 465...  Training loss: 2.2085...  0.1967 sec/batch\n",
      "Epoch: 3/30...  Training Step: 466...  Training loss: 2.2213...  0.1684 sec/batch\n",
      "Epoch: 3/30...  Training Step: 467...  Training loss: 2.2368...  0.2264 sec/batch\n",
      "Epoch: 3/30...  Training Step: 468...  Training loss: 2.2141...  0.2955 sec/batch\n",
      "Epoch: 3/30...  Training Step: 469...  Training loss: 2.2250...  0.2525 sec/batch\n",
      "Epoch: 3/30...  Training Step: 470...  Training loss: 2.2416...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 471...  Training loss: 2.2653...  0.3602 sec/batch\n",
      "Epoch: 3/30...  Training Step: 472...  Training loss: 2.1934...  0.2569 sec/batch\n",
      "Epoch: 3/30...  Training Step: 473...  Training loss: 2.2035...  0.2606 sec/batch\n",
      "Epoch: 3/30...  Training Step: 474...  Training loss: 2.2395...  0.1554 sec/batch\n",
      "Epoch: 3/30...  Training Step: 475...  Training loss: 2.2287...  0.2455 sec/batch\n",
      "Epoch: 3/30...  Training Step: 476...  Training loss: 2.2503...  0.2851 sec/batch\n",
      "Epoch: 3/30...  Training Step: 477...  Training loss: 2.1876...  0.2505 sec/batch\n",
      "Epoch: 3/30...  Training Step: 478...  Training loss: 2.1963...  0.2506 sec/batch\n",
      "Epoch: 3/30...  Training Step: 479...  Training loss: 2.1901...  0.2574 sec/batch\n",
      "Epoch: 3/30...  Training Step: 480...  Training loss: 2.1980...  0.2635 sec/batch\n",
      "Epoch: 3/30...  Training Step: 481...  Training loss: 2.1959...  0.2900 sec/batch\n",
      "Epoch: 3/30...  Training Step: 482...  Training loss: 2.1974...  0.2827 sec/batch\n",
      "Epoch: 3/30...  Training Step: 483...  Training loss: 2.1863...  0.2728 sec/batch\n",
      "Epoch: 3/30...  Training Step: 484...  Training loss: 2.2030...  0.2014 sec/batch\n",
      "Epoch: 3/30...  Training Step: 485...  Training loss: 2.1828...  0.1647 sec/batch\n",
      "Epoch: 3/30...  Training Step: 486...  Training loss: 2.1730...  0.2285 sec/batch\n",
      "Epoch: 3/30...  Training Step: 487...  Training loss: 2.2243...  0.1863 sec/batch\n",
      "Epoch: 3/30...  Training Step: 488...  Training loss: 2.1891...  0.2198 sec/batch\n",
      "Epoch: 3/30...  Training Step: 489...  Training loss: 2.1971...  0.1994 sec/batch\n",
      "Epoch: 3/30...  Training Step: 490...  Training loss: 2.1982...  0.2606 sec/batch\n",
      "Epoch: 3/30...  Training Step: 491...  Training loss: 2.2113...  0.2837 sec/batch\n",
      "Epoch: 3/30...  Training Step: 492...  Training loss: 2.1929...  0.2878 sec/batch\n",
      "Epoch: 3/30...  Training Step: 493...  Training loss: 2.2022...  0.2530 sec/batch\n",
      "Epoch: 3/30...  Training Step: 494...  Training loss: 2.1739...  0.2251 sec/batch\n",
      "Epoch: 3/30...  Training Step: 495...  Training loss: 2.1776...  0.2029 sec/batch\n",
      "Epoch: 3/30...  Training Step: 496...  Training loss: 2.1447...  0.2038 sec/batch\n",
      "Epoch: 3/30...  Training Step: 497...  Training loss: 2.1758...  0.2332 sec/batch\n",
      "Epoch: 3/30...  Training Step: 498...  Training loss: 2.2066...  0.2194 sec/batch\n",
      "Epoch: 3/30...  Training Step: 499...  Training loss: 2.1706...  0.2792 sec/batch\n",
      "Epoch: 3/30...  Training Step: 500...  Training loss: 2.2014...  0.3133 sec/batch\n",
      "Epoch: 3/30...  Training Step: 501...  Training loss: 2.1797...  0.2789 sec/batch\n",
      "Epoch: 3/30...  Training Step: 502...  Training loss: 2.1774...  0.2763 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30...  Training Step: 503...  Training loss: 2.1761...  0.2585 sec/batch\n",
      "Epoch: 3/30...  Training Step: 504...  Training loss: 2.1593...  0.2349 sec/batch\n",
      "Epoch: 3/30...  Training Step: 505...  Training loss: 2.1649...  0.1739 sec/batch\n",
      "Epoch: 3/30...  Training Step: 506...  Training loss: 2.1488...  0.1903 sec/batch\n",
      "Epoch: 3/30...  Training Step: 507...  Training loss: 2.1626...  0.2363 sec/batch\n",
      "Epoch: 3/30...  Training Step: 508...  Training loss: 2.1603...  0.2912 sec/batch\n",
      "Epoch: 3/30...  Training Step: 509...  Training loss: 2.1852...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 510...  Training loss: 2.1337...  0.2784 sec/batch\n",
      "Epoch: 4/30...  Training Step: 511...  Training loss: 2.2658...  0.2355 sec/batch\n",
      "Epoch: 4/30...  Training Step: 512...  Training loss: 2.1531...  0.2786 sec/batch\n",
      "Epoch: 4/30...  Training Step: 513...  Training loss: 2.1655...  0.1884 sec/batch\n",
      "Epoch: 4/30...  Training Step: 514...  Training loss: 2.1926...  0.1727 sec/batch\n",
      "Epoch: 4/30...  Training Step: 515...  Training loss: 2.1773...  0.1598 sec/batch\n",
      "Epoch: 4/30...  Training Step: 516...  Training loss: 2.1942...  0.2351 sec/batch\n",
      "Epoch: 4/30...  Training Step: 517...  Training loss: 2.1634...  0.2134 sec/batch\n",
      "Epoch: 4/30...  Training Step: 518...  Training loss: 2.1405...  0.2165 sec/batch\n",
      "Epoch: 4/30...  Training Step: 519...  Training loss: 2.1379...  0.2462 sec/batch\n",
      "Epoch: 4/30...  Training Step: 520...  Training loss: 2.1418...  0.1986 sec/batch\n",
      "Epoch: 4/30...  Training Step: 521...  Training loss: 2.1306...  0.2488 sec/batch\n",
      "Epoch: 4/30...  Training Step: 522...  Training loss: 2.1684...  0.2365 sec/batch\n",
      "Epoch: 4/30...  Training Step: 523...  Training loss: 2.1452...  0.1757 sec/batch\n",
      "Epoch: 4/30...  Training Step: 524...  Training loss: 2.1402...  0.2043 sec/batch\n",
      "Epoch: 4/30...  Training Step: 525...  Training loss: 2.1750...  0.1577 sec/batch\n",
      "Epoch: 4/30...  Training Step: 526...  Training loss: 2.1836...  0.1855 sec/batch\n",
      "Epoch: 4/30...  Training Step: 527...  Training loss: 2.1144...  0.1782 sec/batch\n",
      "Epoch: 4/30...  Training Step: 528...  Training loss: 2.1591...  0.2479 sec/batch\n",
      "Epoch: 4/30...  Training Step: 529...  Training loss: 2.1583...  0.2024 sec/batch\n",
      "Epoch: 4/30...  Training Step: 530...  Training loss: 2.1492...  0.2116 sec/batch\n",
      "Epoch: 4/30...  Training Step: 531...  Training loss: 2.1341...  0.2051 sec/batch\n",
      "Epoch: 4/30...  Training Step: 532...  Training loss: 2.1081...  0.2426 sec/batch\n",
      "Epoch: 4/30...  Training Step: 533...  Training loss: 2.1235...  0.2821 sec/batch\n",
      "Epoch: 4/30...  Training Step: 534...  Training loss: 2.0996...  0.2330 sec/batch\n",
      "Epoch: 4/30...  Training Step: 535...  Training loss: 2.1634...  0.1558 sec/batch\n",
      "Epoch: 4/30...  Training Step: 536...  Training loss: 2.1334...  0.2000 sec/batch\n",
      "Epoch: 4/30...  Training Step: 537...  Training loss: 2.1749...  0.2400 sec/batch\n",
      "Epoch: 4/30...  Training Step: 538...  Training loss: 2.1489...  0.1863 sec/batch\n",
      "Epoch: 4/30...  Training Step: 539...  Training loss: 2.1431...  0.2294 sec/batch\n",
      "Epoch: 4/30...  Training Step: 540...  Training loss: 2.0983...  0.1869 sec/batch\n",
      "Epoch: 4/30...  Training Step: 541...  Training loss: 2.1461...  0.2430 sec/batch\n",
      "Epoch: 4/30...  Training Step: 542...  Training loss: 2.1396...  0.2072 sec/batch\n",
      "Epoch: 4/30...  Training Step: 543...  Training loss: 2.1298...  0.1773 sec/batch\n",
      "Epoch: 4/30...  Training Step: 544...  Training loss: 2.0998...  0.1551 sec/batch\n",
      "Epoch: 4/30...  Training Step: 545...  Training loss: 2.1261...  0.1593 sec/batch\n",
      "Epoch: 4/30...  Training Step: 546...  Training loss: 2.1344...  0.1648 sec/batch\n",
      "Epoch: 4/30...  Training Step: 547...  Training loss: 2.1543...  0.1526 sec/batch\n",
      "Epoch: 4/30...  Training Step: 548...  Training loss: 2.1352...  0.2016 sec/batch\n",
      "Epoch: 4/30...  Training Step: 549...  Training loss: 2.1320...  0.2580 sec/batch\n",
      "Epoch: 4/30...  Training Step: 550...  Training loss: 2.1333...  0.2765 sec/batch\n",
      "Epoch: 4/30...  Training Step: 551...  Training loss: 2.1593...  0.1825 sec/batch\n",
      "Epoch: 4/30...  Training Step: 552...  Training loss: 2.1168...  0.2028 sec/batch\n",
      "Epoch: 4/30...  Training Step: 553...  Training loss: 2.1104...  0.2496 sec/batch\n",
      "Epoch: 4/30...  Training Step: 554...  Training loss: 2.1557...  0.2017 sec/batch\n",
      "Epoch: 4/30...  Training Step: 555...  Training loss: 2.1130...  0.1993 sec/batch\n",
      "Epoch: 4/30...  Training Step: 556...  Training loss: 2.1201...  0.1740 sec/batch\n",
      "Epoch: 4/30...  Training Step: 557...  Training loss: 2.1108...  0.1920 sec/batch\n",
      "Epoch: 4/30...  Training Step: 558...  Training loss: 2.1344...  0.2060 sec/batch\n",
      "Epoch: 4/30...  Training Step: 559...  Training loss: 2.1143...  0.1767 sec/batch\n",
      "Epoch: 4/30...  Training Step: 560...  Training loss: 2.1358...  0.3452 sec/batch\n",
      "Epoch: 4/30...  Training Step: 561...  Training loss: 2.1662...  0.2431 sec/batch\n",
      "Epoch: 4/30...  Training Step: 562...  Training loss: 2.1317...  0.2099 sec/batch\n",
      "Epoch: 4/30...  Training Step: 563...  Training loss: 2.0917...  0.2310 sec/batch\n",
      "Epoch: 4/30...  Training Step: 564...  Training loss: 2.1332...  0.2741 sec/batch\n",
      "Epoch: 4/30...  Training Step: 565...  Training loss: 2.0986...  0.2606 sec/batch\n",
      "Epoch: 4/30...  Training Step: 566...  Training loss: 2.1261...  0.1711 sec/batch\n",
      "Epoch: 4/30...  Training Step: 567...  Training loss: 2.1302...  0.2438 sec/batch\n",
      "Epoch: 4/30...  Training Step: 568...  Training loss: 2.0943...  0.2750 sec/batch\n",
      "Epoch: 4/30...  Training Step: 569...  Training loss: 2.1325...  0.2855 sec/batch\n",
      "Epoch: 4/30...  Training Step: 570...  Training loss: 2.1208...  0.2585 sec/batch\n",
      "Epoch: 4/30...  Training Step: 571...  Training loss: 2.1209...  0.2053 sec/batch\n",
      "Epoch: 4/30...  Training Step: 572...  Training loss: 2.1167...  0.2350 sec/batch\n",
      "Epoch: 4/30...  Training Step: 573...  Training loss: 2.0954...  0.2147 sec/batch\n",
      "Epoch: 4/30...  Training Step: 574...  Training loss: 2.0924...  0.1903 sec/batch\n",
      "Epoch: 4/30...  Training Step: 575...  Training loss: 2.1336...  0.1699 sec/batch\n",
      "Epoch: 4/30...  Training Step: 576...  Training loss: 2.1102...  0.2312 sec/batch\n",
      "Epoch: 4/30...  Training Step: 577...  Training loss: 2.1311...  0.2544 sec/batch\n",
      "Epoch: 4/30...  Training Step: 578...  Training loss: 2.0738...  0.2901 sec/batch\n",
      "Epoch: 4/30...  Training Step: 579...  Training loss: 2.1217...  0.2124 sec/batch\n",
      "Epoch: 4/30...  Training Step: 580...  Training loss: 2.0829...  0.2570 sec/batch\n",
      "Epoch: 4/30...  Training Step: 581...  Training loss: 2.0871...  0.2887 sec/batch\n",
      "Epoch: 4/30...  Training Step: 582...  Training loss: 2.0845...  0.2406 sec/batch\n",
      "Epoch: 4/30...  Training Step: 583...  Training loss: 2.1023...  0.2165 sec/batch\n",
      "Epoch: 4/30...  Training Step: 584...  Training loss: 2.1283...  0.2536 sec/batch\n",
      "Epoch: 4/30...  Training Step: 585...  Training loss: 2.0940...  0.2879 sec/batch\n",
      "Epoch: 4/30...  Training Step: 586...  Training loss: 2.0653...  0.2410 sec/batch\n",
      "Epoch: 4/30...  Training Step: 587...  Training loss: 2.0944...  0.2337 sec/batch\n",
      "Epoch: 4/30...  Training Step: 588...  Training loss: 2.0994...  0.2554 sec/batch\n",
      "Epoch: 4/30...  Training Step: 589...  Training loss: 2.0962...  0.2469 sec/batch\n",
      "Epoch: 4/30...  Training Step: 590...  Training loss: 2.1199...  0.2152 sec/batch\n",
      "Epoch: 4/30...  Training Step: 591...  Training loss: 2.1035...  0.2560 sec/batch\n",
      "Epoch: 4/30...  Training Step: 592...  Training loss: 2.1047...  0.2706 sec/batch\n",
      "Epoch: 4/30...  Training Step: 593...  Training loss: 2.0935...  0.2105 sec/batch\n",
      "Epoch: 4/30...  Training Step: 594...  Training loss: 2.0702...  0.2263 sec/batch\n",
      "Epoch: 4/30...  Training Step: 595...  Training loss: 2.1142...  0.1945 sec/batch\n",
      "Epoch: 4/30...  Training Step: 596...  Training loss: 2.0847...  0.2497 sec/batch\n",
      "Epoch: 4/30...  Training Step: 597...  Training loss: 2.0972...  0.2468 sec/batch\n",
      "Epoch: 4/30...  Training Step: 598...  Training loss: 2.1113...  0.2486 sec/batch\n",
      "Epoch: 4/30...  Training Step: 599...  Training loss: 2.0946...  0.2675 sec/batch\n",
      "Epoch: 4/30...  Training Step: 600...  Training loss: 2.0599...  0.1848 sec/batch\n",
      "Epoch: 4/30...  Training Step: 601...  Training loss: 2.0737...  0.1552 sec/batch\n",
      "Epoch: 4/30...  Training Step: 602...  Training loss: 2.0675...  0.2040 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/30...  Training Step: 603...  Training loss: 2.0972...  0.2297 sec/batch\n",
      "Epoch: 4/30...  Training Step: 604...  Training loss: 2.1233...  0.2472 sec/batch\n",
      "Epoch: 4/30...  Training Step: 605...  Training loss: 2.0740...  0.1830 sec/batch\n",
      "Epoch: 4/30...  Training Step: 606...  Training loss: 2.0579...  0.1722 sec/batch\n",
      "Epoch: 4/30...  Training Step: 607...  Training loss: 2.0699...  0.2218 sec/batch\n",
      "Epoch: 4/30...  Training Step: 608...  Training loss: 2.0788...  0.1553 sec/batch\n",
      "Epoch: 4/30...  Training Step: 609...  Training loss: 2.0810...  0.1989 sec/batch\n",
      "Epoch: 4/30...  Training Step: 610...  Training loss: 2.1181...  0.2434 sec/batch\n",
      "Epoch: 4/30...  Training Step: 611...  Training loss: 2.1126...  0.3072 sec/batch\n",
      "Epoch: 4/30...  Training Step: 612...  Training loss: 2.0646...  0.2683 sec/batch\n",
      "Epoch: 4/30...  Training Step: 613...  Training loss: 2.0645...  0.2703 sec/batch\n",
      "Epoch: 4/30...  Training Step: 614...  Training loss: 2.0584...  0.2516 sec/batch\n",
      "Epoch: 4/30...  Training Step: 615...  Training loss: 2.0676...  0.2628 sec/batch\n",
      "Epoch: 4/30...  Training Step: 616...  Training loss: 2.0575...  0.1978 sec/batch\n",
      "Epoch: 4/30...  Training Step: 617...  Training loss: 2.0670...  0.1576 sec/batch\n",
      "Epoch: 4/30...  Training Step: 618...  Training loss: 2.0451...  0.2435 sec/batch\n",
      "Epoch: 4/30...  Training Step: 619...  Training loss: 2.0587...  0.2489 sec/batch\n",
      "Epoch: 4/30...  Training Step: 620...  Training loss: 2.0603...  0.1557 sec/batch\n",
      "Epoch: 4/30...  Training Step: 621...  Training loss: 2.0459...  0.2312 sec/batch\n",
      "Epoch: 4/30...  Training Step: 622...  Training loss: 2.0318...  0.2690 sec/batch\n",
      "Epoch: 4/30...  Training Step: 623...  Training loss: 2.0533...  0.2589 sec/batch\n",
      "Epoch: 4/30...  Training Step: 624...  Training loss: 2.0798...  0.2025 sec/batch\n",
      "Epoch: 4/30...  Training Step: 625...  Training loss: 2.0773...  0.2708 sec/batch\n",
      "Epoch: 4/30...  Training Step: 626...  Training loss: 2.0639...  0.2388 sec/batch\n",
      "Epoch: 4/30...  Training Step: 627...  Training loss: 2.0664...  0.1743 sec/batch\n",
      "Epoch: 4/30...  Training Step: 628...  Training loss: 2.0626...  0.2546 sec/batch\n",
      "Epoch: 4/30...  Training Step: 629...  Training loss: 2.0392...  0.2398 sec/batch\n",
      "Epoch: 4/30...  Training Step: 630...  Training loss: 2.0740...  0.2454 sec/batch\n",
      "Epoch: 4/30...  Training Step: 631...  Training loss: 2.0219...  0.2666 sec/batch\n",
      "Epoch: 4/30...  Training Step: 632...  Training loss: 2.0480...  0.2717 sec/batch\n",
      "Epoch: 4/30...  Training Step: 633...  Training loss: 2.0567...  0.2457 sec/batch\n",
      "Epoch: 4/30...  Training Step: 634...  Training loss: 2.0808...  0.1548 sec/batch\n",
      "Epoch: 4/30...  Training Step: 635...  Training loss: 2.0439...  0.1840 sec/batch\n",
      "Epoch: 4/30...  Training Step: 636...  Training loss: 2.0513...  0.2105 sec/batch\n",
      "Epoch: 4/30...  Training Step: 637...  Training loss: 2.0859...  0.2324 sec/batch\n",
      "Epoch: 4/30...  Training Step: 638...  Training loss: 2.0523...  0.2575 sec/batch\n",
      "Epoch: 4/30...  Training Step: 639...  Training loss: 2.0735...  0.2614 sec/batch\n",
      "Epoch: 4/30...  Training Step: 640...  Training loss: 2.0832...  0.2880 sec/batch\n",
      "Epoch: 4/30...  Training Step: 641...  Training loss: 2.1111...  0.2254 sec/batch\n",
      "Epoch: 4/30...  Training Step: 642...  Training loss: 2.0282...  0.2831 sec/batch\n",
      "Epoch: 4/30...  Training Step: 643...  Training loss: 2.0357...  0.2690 sec/batch\n",
      "Epoch: 4/30...  Training Step: 644...  Training loss: 2.0679...  0.2723 sec/batch\n",
      "Epoch: 4/30...  Training Step: 645...  Training loss: 2.0489...  0.2575 sec/batch\n",
      "Epoch: 4/30...  Training Step: 646...  Training loss: 2.0838...  0.2686 sec/batch\n",
      "Epoch: 4/30...  Training Step: 647...  Training loss: 2.0154...  0.2280 sec/batch\n",
      "Epoch: 4/30...  Training Step: 648...  Training loss: 2.0375...  0.2637 sec/batch\n",
      "Epoch: 4/30...  Training Step: 649...  Training loss: 2.0228...  0.2460 sec/batch\n",
      "Epoch: 4/30...  Training Step: 650...  Training loss: 2.0293...  0.2071 sec/batch\n",
      "Epoch: 4/30...  Training Step: 651...  Training loss: 2.0384...  0.2253 sec/batch\n",
      "Epoch: 4/30...  Training Step: 652...  Training loss: 2.0375...  0.1593 sec/batch\n",
      "Epoch: 4/30...  Training Step: 653...  Training loss: 2.0136...  0.2662 sec/batch\n",
      "Epoch: 4/30...  Training Step: 654...  Training loss: 2.0303...  0.1790 sec/batch\n",
      "Epoch: 4/30...  Training Step: 655...  Training loss: 2.0253...  0.2173 sec/batch\n",
      "Epoch: 4/30...  Training Step: 656...  Training loss: 1.9966...  0.2717 sec/batch\n",
      "Epoch: 4/30...  Training Step: 657...  Training loss: 2.0563...  0.2802 sec/batch\n",
      "Epoch: 4/30...  Training Step: 658...  Training loss: 2.0332...  0.2418 sec/batch\n",
      "Epoch: 4/30...  Training Step: 659...  Training loss: 2.0363...  0.2355 sec/batch\n",
      "Epoch: 4/30...  Training Step: 660...  Training loss: 2.0362...  0.2723 sec/batch\n",
      "Epoch: 4/30...  Training Step: 661...  Training loss: 2.0643...  0.2498 sec/batch\n",
      "Epoch: 4/30...  Training Step: 662...  Training loss: 2.0491...  0.2486 sec/batch\n",
      "Epoch: 4/30...  Training Step: 663...  Training loss: 2.0456...  0.1682 sec/batch\n",
      "Epoch: 4/30...  Training Step: 664...  Training loss: 2.0062...  0.1978 sec/batch\n",
      "Epoch: 4/30...  Training Step: 665...  Training loss: 2.0219...  0.2384 sec/batch\n",
      "Epoch: 4/30...  Training Step: 666...  Training loss: 1.9809...  0.2504 sec/batch\n",
      "Epoch: 4/30...  Training Step: 667...  Training loss: 2.0217...  0.2773 sec/batch\n",
      "Epoch: 4/30...  Training Step: 668...  Training loss: 2.0444...  0.2707 sec/batch\n",
      "Epoch: 4/30...  Training Step: 669...  Training loss: 2.0030...  0.1977 sec/batch\n",
      "Epoch: 4/30...  Training Step: 670...  Training loss: 2.0381...  0.2095 sec/batch\n",
      "Epoch: 4/30...  Training Step: 671...  Training loss: 2.0289...  0.2799 sec/batch\n",
      "Epoch: 4/30...  Training Step: 672...  Training loss: 2.0375...  0.2030 sec/batch\n",
      "Epoch: 4/30...  Training Step: 673...  Training loss: 2.0277...  0.2338 sec/batch\n",
      "Epoch: 4/30...  Training Step: 674...  Training loss: 1.9955...  0.2173 sec/batch\n",
      "Epoch: 4/30...  Training Step: 675...  Training loss: 1.9939...  0.1820 sec/batch\n",
      "Epoch: 4/30...  Training Step: 676...  Training loss: 1.9839...  0.2179 sec/batch\n",
      "Epoch: 4/30...  Training Step: 677...  Training loss: 2.0057...  0.2616 sec/batch\n",
      "Epoch: 4/30...  Training Step: 678...  Training loss: 2.0060...  0.3050 sec/batch\n",
      "Epoch: 4/30...  Training Step: 679...  Training loss: 2.0364...  0.2082 sec/batch\n",
      "Epoch: 4/30...  Training Step: 680...  Training loss: 2.0014...  0.2602 sec/batch\n",
      "Epoch: 5/30...  Training Step: 681...  Training loss: 2.1102...  0.2621 sec/batch\n",
      "Epoch: 5/30...  Training Step: 682...  Training loss: 1.9904...  0.1646 sec/batch\n",
      "Epoch: 5/30...  Training Step: 683...  Training loss: 1.9999...  0.1810 sec/batch\n",
      "Epoch: 5/30...  Training Step: 684...  Training loss: 2.0446...  0.2446 sec/batch\n",
      "Epoch: 5/30...  Training Step: 685...  Training loss: 2.0231...  0.2375 sec/batch\n",
      "Epoch: 5/30...  Training Step: 686...  Training loss: 2.0352...  0.2208 sec/batch\n",
      "Epoch: 5/30...  Training Step: 687...  Training loss: 2.0079...  0.2329 sec/batch\n",
      "Epoch: 5/30...  Training Step: 688...  Training loss: 1.9873...  0.2252 sec/batch\n",
      "Epoch: 5/30...  Training Step: 689...  Training loss: 1.9762...  0.2969 sec/batch\n",
      "Epoch: 5/30...  Training Step: 690...  Training loss: 1.9853...  0.2219 sec/batch\n",
      "Epoch: 5/30...  Training Step: 691...  Training loss: 1.9718...  0.1736 sec/batch\n",
      "Epoch: 5/30...  Training Step: 692...  Training loss: 2.0075...  0.1544 sec/batch\n",
      "Epoch: 5/30...  Training Step: 693...  Training loss: 1.9897...  0.1849 sec/batch\n",
      "Epoch: 5/30...  Training Step: 694...  Training loss: 1.9809...  0.2944 sec/batch\n",
      "Epoch: 5/30...  Training Step: 695...  Training loss: 2.0323...  0.2526 sec/batch\n",
      "Epoch: 5/30...  Training Step: 696...  Training loss: 2.0228...  0.1976 sec/batch\n",
      "Epoch: 5/30...  Training Step: 697...  Training loss: 1.9697...  0.1551 sec/batch\n",
      "Epoch: 5/30...  Training Step: 698...  Training loss: 2.0105...  0.2162 sec/batch\n",
      "Epoch: 5/30...  Training Step: 699...  Training loss: 1.9961...  0.2093 sec/batch\n",
      "Epoch: 5/30...  Training Step: 700...  Training loss: 1.9872...  0.2280 sec/batch\n",
      "Epoch: 5/30...  Training Step: 701...  Training loss: 2.0002...  0.2180 sec/batch\n",
      "Epoch: 5/30...  Training Step: 702...  Training loss: 1.9547...  0.1773 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30...  Training Step: 703...  Training loss: 1.9769...  0.1838 sec/batch\n",
      "Epoch: 5/30...  Training Step: 704...  Training loss: 1.9390...  0.1582 sec/batch\n",
      "Epoch: 5/30...  Training Step: 705...  Training loss: 2.0007...  0.1948 sec/batch\n",
      "Epoch: 5/30...  Training Step: 706...  Training loss: 1.9868...  0.2859 sec/batch\n",
      "Epoch: 5/30...  Training Step: 707...  Training loss: 2.0248...  0.2783 sec/batch\n",
      "Epoch: 5/30...  Training Step: 708...  Training loss: 2.0001...  0.2503 sec/batch\n",
      "Epoch: 5/30...  Training Step: 709...  Training loss: 2.0001...  0.2172 sec/batch\n",
      "Epoch: 5/30...  Training Step: 710...  Training loss: 1.9527...  0.2699 sec/batch\n",
      "Epoch: 5/30...  Training Step: 711...  Training loss: 2.0038...  0.1817 sec/batch\n",
      "Epoch: 5/30...  Training Step: 712...  Training loss: 1.9890...  0.1626 sec/batch\n",
      "Epoch: 5/30...  Training Step: 713...  Training loss: 1.9686...  0.2474 sec/batch\n",
      "Epoch: 5/30...  Training Step: 714...  Training loss: 1.9490...  0.2766 sec/batch\n",
      "Epoch: 5/30...  Training Step: 715...  Training loss: 1.9825...  0.2415 sec/batch\n",
      "Epoch: 5/30...  Training Step: 716...  Training loss: 1.9862...  0.2060 sec/batch\n",
      "Epoch: 5/30...  Training Step: 717...  Training loss: 2.0194...  0.2573 sec/batch\n",
      "Epoch: 5/30...  Training Step: 718...  Training loss: 1.9868...  0.2682 sec/batch\n",
      "Epoch: 5/30...  Training Step: 719...  Training loss: 1.9907...  0.2677 sec/batch\n",
      "Epoch: 5/30...  Training Step: 720...  Training loss: 1.9952...  0.2685 sec/batch\n",
      "Epoch: 5/30...  Training Step: 721...  Training loss: 2.0127...  0.1671 sec/batch\n",
      "Epoch: 5/30...  Training Step: 722...  Training loss: 1.9793...  0.2221 sec/batch\n",
      "Epoch: 5/30...  Training Step: 723...  Training loss: 1.9530...  0.2063 sec/batch\n",
      "Epoch: 5/30...  Training Step: 724...  Training loss: 2.0186...  0.2555 sec/batch\n",
      "Epoch: 5/30...  Training Step: 725...  Training loss: 1.9796...  0.2941 sec/batch\n",
      "Epoch: 5/30...  Training Step: 726...  Training loss: 1.9691...  0.2414 sec/batch\n",
      "Epoch: 5/30...  Training Step: 727...  Training loss: 1.9868...  0.2118 sec/batch\n",
      "Epoch: 5/30...  Training Step: 728...  Training loss: 1.9970...  0.2328 sec/batch\n",
      "Epoch: 5/30...  Training Step: 729...  Training loss: 1.9872...  0.3191 sec/batch\n",
      "Epoch: 5/30...  Training Step: 730...  Training loss: 1.9852...  0.3759 sec/batch\n",
      "Epoch: 5/30...  Training Step: 731...  Training loss: 2.0229...  0.1714 sec/batch\n",
      "Epoch: 5/30...  Training Step: 732...  Training loss: 1.9762...  0.2347 sec/batch\n",
      "Epoch: 5/30...  Training Step: 733...  Training loss: 1.9556...  0.1578 sec/batch\n",
      "Epoch: 5/30...  Training Step: 734...  Training loss: 1.9847...  0.1711 sec/batch\n",
      "Epoch: 5/30...  Training Step: 735...  Training loss: 1.9573...  0.1917 sec/batch\n",
      "Epoch: 5/30...  Training Step: 736...  Training loss: 1.9970...  0.2740 sec/batch\n",
      "Epoch: 5/30...  Training Step: 737...  Training loss: 1.9937...  0.2568 sec/batch\n",
      "Epoch: 5/30...  Training Step: 738...  Training loss: 1.9559...  0.2707 sec/batch\n",
      "Epoch: 5/30...  Training Step: 739...  Training loss: 1.9882...  0.1887 sec/batch\n",
      "Epoch: 5/30...  Training Step: 740...  Training loss: 1.9783...  0.2178 sec/batch\n",
      "Epoch: 5/30...  Training Step: 741...  Training loss: 1.9887...  0.2335 sec/batch\n",
      "Epoch: 5/30...  Training Step: 742...  Training loss: 1.9802...  0.1965 sec/batch\n",
      "Epoch: 5/30...  Training Step: 743...  Training loss: 1.9608...  0.2695 sec/batch\n",
      "Epoch: 5/30...  Training Step: 744...  Training loss: 1.9483...  0.2652 sec/batch\n",
      "Epoch: 5/30...  Training Step: 745...  Training loss: 2.0103...  0.1784 sec/batch\n",
      "Epoch: 5/30...  Training Step: 746...  Training loss: 1.9721...  0.2396 sec/batch\n",
      "Epoch: 5/30...  Training Step: 747...  Training loss: 1.9912...  0.1839 sec/batch\n",
      "Epoch: 5/30...  Training Step: 748...  Training loss: 1.9375...  0.2249 sec/batch\n",
      "Epoch: 5/30...  Training Step: 749...  Training loss: 1.9822...  0.1683 sec/batch\n",
      "Epoch: 5/30...  Training Step: 750...  Training loss: 1.9693...  0.2400 sec/batch\n",
      "Epoch: 5/30...  Training Step: 751...  Training loss: 1.9349...  0.1672 sec/batch\n",
      "Epoch: 5/30...  Training Step: 752...  Training loss: 1.9560...  0.2755 sec/batch\n",
      "Epoch: 5/30...  Training Step: 753...  Training loss: 1.9603...  0.2713 sec/batch\n",
      "Epoch: 5/30...  Training Step: 754...  Training loss: 1.9983...  0.1785 sec/batch\n",
      "Epoch: 5/30...  Training Step: 755...  Training loss: 1.9615...  0.2742 sec/batch\n",
      "Epoch: 5/30...  Training Step: 756...  Training loss: 1.9396...  0.2738 sec/batch\n",
      "Epoch: 5/30...  Training Step: 757...  Training loss: 1.9523...  0.2762 sec/batch\n",
      "Epoch: 5/30...  Training Step: 758...  Training loss: 1.9799...  0.2239 sec/batch\n",
      "Epoch: 5/30...  Training Step: 759...  Training loss: 1.9734...  0.1916 sec/batch\n",
      "Epoch: 5/30...  Training Step: 760...  Training loss: 1.9941...  0.1901 sec/batch\n",
      "Epoch: 5/30...  Training Step: 761...  Training loss: 1.9715...  0.2599 sec/batch\n",
      "Epoch: 5/30...  Training Step: 762...  Training loss: 1.9766...  0.1771 sec/batch\n",
      "Epoch: 5/30...  Training Step: 763...  Training loss: 1.9658...  0.1587 sec/batch\n",
      "Epoch: 5/30...  Training Step: 764...  Training loss: 1.9421...  0.2047 sec/batch\n",
      "Epoch: 5/30...  Training Step: 765...  Training loss: 1.9764...  0.2347 sec/batch\n",
      "Epoch: 5/30...  Training Step: 766...  Training loss: 1.9399...  0.1627 sec/batch\n",
      "Epoch: 5/30...  Training Step: 767...  Training loss: 1.9707...  0.1940 sec/batch\n",
      "Epoch: 5/30...  Training Step: 768...  Training loss: 1.9799...  0.1824 sec/batch\n",
      "Epoch: 5/30...  Training Step: 769...  Training loss: 1.9705...  0.1719 sec/batch\n",
      "Epoch: 5/30...  Training Step: 770...  Training loss: 1.9262...  0.1657 sec/batch\n",
      "Epoch: 5/30...  Training Step: 771...  Training loss: 1.9370...  0.1571 sec/batch\n",
      "Epoch: 5/30...  Training Step: 772...  Training loss: 1.9264...  0.2020 sec/batch\n",
      "Epoch: 5/30...  Training Step: 773...  Training loss: 1.9688...  0.2589 sec/batch\n",
      "Epoch: 5/30...  Training Step: 774...  Training loss: 1.9842...  0.2836 sec/batch\n",
      "Epoch: 5/30...  Training Step: 775...  Training loss: 1.9495...  0.1676 sec/batch\n",
      "Epoch: 5/30...  Training Step: 776...  Training loss: 1.9129...  0.1886 sec/batch\n",
      "Epoch: 5/30...  Training Step: 777...  Training loss: 1.9144...  0.2477 sec/batch\n",
      "Epoch: 5/30...  Training Step: 778...  Training loss: 1.9435...  0.2733 sec/batch\n",
      "Epoch: 5/30...  Training Step: 779...  Training loss: 1.9378...  0.1903 sec/batch\n",
      "Epoch: 5/30...  Training Step: 780...  Training loss: 1.9900...  0.2308 sec/batch\n",
      "Epoch: 5/30...  Training Step: 781...  Training loss: 1.9732...  0.2507 sec/batch\n",
      "Epoch: 5/30...  Training Step: 782...  Training loss: 1.9236...  0.1824 sec/batch\n",
      "Epoch: 5/30...  Training Step: 783...  Training loss: 1.9400...  0.2648 sec/batch\n",
      "Epoch: 5/30...  Training Step: 784...  Training loss: 1.9395...  0.2609 sec/batch\n",
      "Epoch: 5/30...  Training Step: 785...  Training loss: 1.9407...  0.2335 sec/batch\n",
      "Epoch: 5/30...  Training Step: 786...  Training loss: 1.9323...  0.2544 sec/batch\n",
      "Epoch: 5/30...  Training Step: 787...  Training loss: 1.9403...  0.2525 sec/batch\n",
      "Epoch: 5/30...  Training Step: 788...  Training loss: 1.9132...  0.2570 sec/batch\n",
      "Epoch: 5/30...  Training Step: 789...  Training loss: 1.9493...  0.1755 sec/batch\n",
      "Epoch: 5/30...  Training Step: 790...  Training loss: 1.9389...  0.2349 sec/batch\n",
      "Epoch: 5/30...  Training Step: 791...  Training loss: 1.9235...  0.2198 sec/batch\n",
      "Epoch: 5/30...  Training Step: 792...  Training loss: 1.8920...  0.1721 sec/batch\n",
      "Epoch: 5/30...  Training Step: 793...  Training loss: 1.9293...  0.1667 sec/batch\n",
      "Epoch: 5/30...  Training Step: 794...  Training loss: 1.9498...  0.2529 sec/batch\n",
      "Epoch: 5/30...  Training Step: 795...  Training loss: 1.9541...  0.2616 sec/batch\n",
      "Epoch: 5/30...  Training Step: 796...  Training loss: 1.9311...  0.2743 sec/batch\n",
      "Epoch: 5/30...  Training Step: 797...  Training loss: 1.9301...  0.2529 sec/batch\n",
      "Epoch: 5/30...  Training Step: 798...  Training loss: 1.9427...  0.1879 sec/batch\n",
      "Epoch: 5/30...  Training Step: 799...  Training loss: 1.9133...  0.2401 sec/batch\n",
      "Epoch: 5/30...  Training Step: 800...  Training loss: 1.9388...  0.2445 sec/batch\n",
      "Epoch: 5/30...  Training Step: 801...  Training loss: 1.8915...  0.1888 sec/batch\n",
      "Epoch: 5/30...  Training Step: 802...  Training loss: 1.9165...  0.1615 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30...  Training Step: 803...  Training loss: 1.9364...  0.2510 sec/batch\n",
      "Epoch: 5/30...  Training Step: 804...  Training loss: 1.9634...  0.2235 sec/batch\n",
      "Epoch: 5/30...  Training Step: 805...  Training loss: 1.9092...  0.2458 sec/batch\n",
      "Epoch: 5/30...  Training Step: 806...  Training loss: 1.9225...  0.2868 sec/batch\n",
      "Epoch: 5/30...  Training Step: 807...  Training loss: 1.9431...  0.2166 sec/batch\n",
      "Epoch: 5/30...  Training Step: 808...  Training loss: 1.9307...  0.1553 sec/batch\n",
      "Epoch: 5/30...  Training Step: 809...  Training loss: 1.9446...  0.1850 sec/batch\n",
      "Epoch: 5/30...  Training Step: 810...  Training loss: 1.9599...  0.1587 sec/batch\n",
      "Epoch: 5/30...  Training Step: 811...  Training loss: 1.9857...  0.2592 sec/batch\n",
      "Epoch: 5/30...  Training Step: 812...  Training loss: 1.9138...  0.2367 sec/batch\n",
      "Epoch: 5/30...  Training Step: 813...  Training loss: 1.9231...  0.2945 sec/batch\n",
      "Epoch: 5/30...  Training Step: 814...  Training loss: 1.9533...  0.2410 sec/batch\n",
      "Epoch: 5/30...  Training Step: 815...  Training loss: 1.9380...  0.2559 sec/batch\n",
      "Epoch: 5/30...  Training Step: 816...  Training loss: 1.9548...  0.2497 sec/batch\n",
      "Epoch: 5/30...  Training Step: 817...  Training loss: 1.8993...  0.2517 sec/batch\n",
      "Epoch: 5/30...  Training Step: 818...  Training loss: 1.9266...  0.2097 sec/batch\n",
      "Epoch: 5/30...  Training Step: 819...  Training loss: 1.9066...  0.2900 sec/batch\n",
      "Epoch: 5/30...  Training Step: 820...  Training loss: 1.9058...  0.3022 sec/batch\n",
      "Epoch: 5/30...  Training Step: 821...  Training loss: 1.9181...  0.1722 sec/batch\n",
      "Epoch: 5/30...  Training Step: 822...  Training loss: 1.9172...  0.2017 sec/batch\n",
      "Epoch: 5/30...  Training Step: 823...  Training loss: 1.8895...  0.1595 sec/batch\n",
      "Epoch: 5/30...  Training Step: 824...  Training loss: 1.9128...  0.1938 sec/batch\n",
      "Epoch: 5/30...  Training Step: 825...  Training loss: 1.8990...  0.2619 sec/batch\n",
      "Epoch: 5/30...  Training Step: 826...  Training loss: 1.8709...  0.2696 sec/batch\n",
      "Epoch: 5/30...  Training Step: 827...  Training loss: 1.9400...  0.1990 sec/batch\n",
      "Epoch: 5/30...  Training Step: 828...  Training loss: 1.9109...  0.1607 sec/batch\n",
      "Epoch: 5/30...  Training Step: 829...  Training loss: 1.9103...  0.2574 sec/batch\n",
      "Epoch: 5/30...  Training Step: 830...  Training loss: 1.9137...  0.1950 sec/batch\n",
      "Epoch: 5/30...  Training Step: 831...  Training loss: 1.9280...  0.2698 sec/batch\n",
      "Epoch: 5/30...  Training Step: 832...  Training loss: 1.9252...  0.2395 sec/batch\n",
      "Epoch: 5/30...  Training Step: 833...  Training loss: 1.9178...  0.1925 sec/batch\n",
      "Epoch: 5/30...  Training Step: 834...  Training loss: 1.8826...  0.2768 sec/batch\n",
      "Epoch: 5/30...  Training Step: 835...  Training loss: 1.8890...  0.3275 sec/batch\n",
      "Epoch: 5/30...  Training Step: 836...  Training loss: 1.8545...  0.1865 sec/batch\n",
      "Epoch: 5/30...  Training Step: 837...  Training loss: 1.8888...  0.2538 sec/batch\n",
      "Epoch: 5/30...  Training Step: 838...  Training loss: 1.9240...  0.2292 sec/batch\n",
      "Epoch: 5/30...  Training Step: 839...  Training loss: 1.8971...  0.1569 sec/batch\n",
      "Epoch: 5/30...  Training Step: 840...  Training loss: 1.9246...  0.1727 sec/batch\n",
      "Epoch: 5/30...  Training Step: 841...  Training loss: 1.9007...  0.1700 sec/batch\n",
      "Epoch: 5/30...  Training Step: 842...  Training loss: 1.9199...  0.2107 sec/batch\n",
      "Epoch: 5/30...  Training Step: 843...  Training loss: 1.9027...  0.1896 sec/batch\n",
      "Epoch: 5/30...  Training Step: 844...  Training loss: 1.8912...  0.2833 sec/batch\n",
      "Epoch: 5/30...  Training Step: 845...  Training loss: 1.8799...  0.2855 sec/batch\n",
      "Epoch: 5/30...  Training Step: 846...  Training loss: 1.8783...  0.2847 sec/batch\n",
      "Epoch: 5/30...  Training Step: 847...  Training loss: 1.8808...  0.2718 sec/batch\n",
      "Epoch: 5/30...  Training Step: 848...  Training loss: 1.8964...  0.1996 sec/batch\n",
      "Epoch: 5/30...  Training Step: 849...  Training loss: 1.9144...  0.1575 sec/batch\n",
      "Epoch: 5/30...  Training Step: 850...  Training loss: 1.8776...  0.2337 sec/batch\n",
      "Epoch: 6/30...  Training Step: 851...  Training loss: 1.9808...  0.2920 sec/batch\n",
      "Epoch: 6/30...  Training Step: 852...  Training loss: 1.8682...  0.2208 sec/batch\n",
      "Epoch: 6/30...  Training Step: 853...  Training loss: 1.8851...  0.1540 sec/batch\n",
      "Epoch: 6/30...  Training Step: 854...  Training loss: 1.9198...  0.1817 sec/batch\n",
      "Epoch: 6/30...  Training Step: 855...  Training loss: 1.9135...  0.2211 sec/batch\n",
      "Epoch: 6/30...  Training Step: 856...  Training loss: 1.9196...  0.1974 sec/batch\n",
      "Epoch: 6/30...  Training Step: 857...  Training loss: 1.8992...  0.1784 sec/batch\n",
      "Epoch: 6/30...  Training Step: 858...  Training loss: 1.8761...  0.2215 sec/batch\n",
      "Epoch: 6/30...  Training Step: 859...  Training loss: 1.8576...  0.1659 sec/batch\n",
      "Epoch: 6/30...  Training Step: 860...  Training loss: 1.8635...  0.2051 sec/batch\n",
      "Epoch: 6/30...  Training Step: 861...  Training loss: 1.8667...  0.2336 sec/batch\n",
      "Epoch: 6/30...  Training Step: 862...  Training loss: 1.8879...  0.2217 sec/batch\n",
      "Epoch: 6/30...  Training Step: 863...  Training loss: 1.8727...  0.2010 sec/batch\n",
      "Epoch: 6/30...  Training Step: 864...  Training loss: 1.8754...  0.2673 sec/batch\n",
      "Epoch: 6/30...  Training Step: 865...  Training loss: 1.9323...  0.2156 sec/batch\n",
      "Epoch: 6/30...  Training Step: 866...  Training loss: 1.9269...  0.2402 sec/batch\n",
      "Epoch: 6/30...  Training Step: 867...  Training loss: 1.8565...  0.2275 sec/batch\n",
      "Epoch: 6/30...  Training Step: 868...  Training loss: 1.8975...  0.1729 sec/batch\n",
      "Epoch: 6/30...  Training Step: 869...  Training loss: 1.8921...  0.2175 sec/batch\n",
      "Epoch: 6/30...  Training Step: 870...  Training loss: 1.8747...  0.1563 sec/batch\n",
      "Epoch: 6/30...  Training Step: 871...  Training loss: 1.8855...  0.2394 sec/batch\n",
      "Epoch: 6/30...  Training Step: 872...  Training loss: 1.8422...  0.1935 sec/batch\n",
      "Epoch: 6/30...  Training Step: 873...  Training loss: 1.8472...  0.1584 sec/batch\n",
      "Epoch: 6/30...  Training Step: 874...  Training loss: 1.8314...  0.1701 sec/batch\n",
      "Epoch: 6/30...  Training Step: 875...  Training loss: 1.8990...  0.1686 sec/batch\n",
      "Epoch: 6/30...  Training Step: 876...  Training loss: 1.8739...  0.2409 sec/batch\n",
      "Epoch: 6/30...  Training Step: 877...  Training loss: 1.9088...  0.1863 sec/batch\n",
      "Epoch: 6/30...  Training Step: 878...  Training loss: 1.8988...  0.2547 sec/batch\n",
      "Epoch: 6/30...  Training Step: 879...  Training loss: 1.8809...  0.2762 sec/batch\n",
      "Epoch: 6/30...  Training Step: 880...  Training loss: 1.8259...  0.1832 sec/batch\n",
      "Epoch: 6/30...  Training Step: 881...  Training loss: 1.8727...  0.2404 sec/batch\n",
      "Epoch: 6/30...  Training Step: 882...  Training loss: 1.8686...  0.1528 sec/batch\n",
      "Epoch: 6/30...  Training Step: 883...  Training loss: 1.8699...  0.1569 sec/batch\n",
      "Epoch: 6/30...  Training Step: 884...  Training loss: 1.8423...  0.1956 sec/batch\n",
      "Epoch: 6/30...  Training Step: 885...  Training loss: 1.8584...  0.2333 sec/batch\n",
      "Epoch: 6/30...  Training Step: 886...  Training loss: 1.8825...  0.1733 sec/batch\n",
      "Epoch: 6/30...  Training Step: 887...  Training loss: 1.9112...  0.1682 sec/batch\n",
      "Epoch: 6/30...  Training Step: 888...  Training loss: 1.8730...  0.1752 sec/batch\n",
      "Epoch: 6/30...  Training Step: 889...  Training loss: 1.8806...  0.1748 sec/batch\n",
      "Epoch: 6/30...  Training Step: 890...  Training loss: 1.8850...  0.1691 sec/batch\n",
      "Epoch: 6/30...  Training Step: 891...  Training loss: 1.8983...  0.2216 sec/batch\n",
      "Epoch: 6/30...  Training Step: 892...  Training loss: 1.8534...  0.1772 sec/batch\n",
      "Epoch: 6/30...  Training Step: 893...  Training loss: 1.8444...  0.1768 sec/batch\n",
      "Epoch: 6/30...  Training Step: 894...  Training loss: 1.9170...  0.2733 sec/batch\n",
      "Epoch: 6/30...  Training Step: 895...  Training loss: 1.8756...  0.2143 sec/batch\n",
      "Epoch: 6/30...  Training Step: 896...  Training loss: 1.8591...  0.2202 sec/batch\n",
      "Epoch: 6/30...  Training Step: 897...  Training loss: 1.8835...  0.1693 sec/batch\n",
      "Epoch: 6/30...  Training Step: 898...  Training loss: 1.8843...  0.2245 sec/batch\n",
      "Epoch: 6/30...  Training Step: 899...  Training loss: 1.8706...  0.1856 sec/batch\n",
      "Epoch: 6/30...  Training Step: 900...  Training loss: 1.8857...  0.2236 sec/batch\n",
      "Epoch: 6/30...  Training Step: 901...  Training loss: 1.9281...  0.2521 sec/batch\n",
      "Epoch: 6/30...  Training Step: 902...  Training loss: 1.8609...  0.1838 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30...  Training Step: 903...  Training loss: 1.8460...  0.2285 sec/batch\n",
      "Epoch: 6/30...  Training Step: 904...  Training loss: 1.8767...  0.3295 sec/batch\n",
      "Epoch: 6/30...  Training Step: 905...  Training loss: 1.8567...  0.1976 sec/batch\n",
      "Epoch: 6/30...  Training Step: 906...  Training loss: 1.8901...  0.2687 sec/batch\n",
      "Epoch: 6/30...  Training Step: 907...  Training loss: 1.8780...  0.2934 sec/batch\n",
      "Epoch: 6/30...  Training Step: 908...  Training loss: 1.8365...  0.2598 sec/batch\n",
      "Epoch: 6/30...  Training Step: 909...  Training loss: 1.8759...  0.2275 sec/batch\n",
      "Epoch: 6/30...  Training Step: 910...  Training loss: 1.8647...  0.2554 sec/batch\n",
      "Epoch: 6/30...  Training Step: 911...  Training loss: 1.8715...  0.2484 sec/batch\n",
      "Epoch: 6/30...  Training Step: 912...  Training loss: 1.8606...  0.2955 sec/batch\n",
      "Epoch: 6/30...  Training Step: 913...  Training loss: 1.8457...  0.2638 sec/batch\n",
      "Epoch: 6/30...  Training Step: 914...  Training loss: 1.8530...  0.2652 sec/batch\n",
      "Epoch: 6/30...  Training Step: 915...  Training loss: 1.9009...  0.2574 sec/batch\n",
      "Epoch: 6/30...  Training Step: 916...  Training loss: 1.8824...  0.2175 sec/batch\n",
      "Epoch: 6/30...  Training Step: 917...  Training loss: 1.8941...  0.2315 sec/batch\n",
      "Epoch: 6/30...  Training Step: 918...  Training loss: 1.8370...  0.2840 sec/batch\n",
      "Epoch: 6/30...  Training Step: 919...  Training loss: 1.8619...  0.2519 sec/batch\n",
      "Epoch: 6/30...  Training Step: 920...  Training loss: 1.8567...  0.2568 sec/batch\n",
      "Epoch: 6/30...  Training Step: 921...  Training loss: 1.8350...  0.1852 sec/batch\n",
      "Epoch: 6/30...  Training Step: 922...  Training loss: 1.8560...  0.2630 sec/batch\n",
      "Epoch: 6/30...  Training Step: 923...  Training loss: 1.8621...  0.2496 sec/batch\n",
      "Epoch: 6/30...  Training Step: 924...  Training loss: 1.8874...  0.2146 sec/batch\n",
      "Epoch: 6/30...  Training Step: 925...  Training loss: 1.8653...  0.1996 sec/batch\n",
      "Epoch: 6/30...  Training Step: 926...  Training loss: 1.8219...  0.2148 sec/batch\n",
      "Epoch: 6/30...  Training Step: 927...  Training loss: 1.8408...  0.2806 sec/batch\n",
      "Epoch: 6/30...  Training Step: 928...  Training loss: 1.8629...  0.1837 sec/batch\n",
      "Epoch: 6/30...  Training Step: 929...  Training loss: 1.8736...  0.1611 sec/batch\n",
      "Epoch: 6/30...  Training Step: 930...  Training loss: 1.8789...  0.2050 sec/batch\n",
      "Epoch: 6/30...  Training Step: 931...  Training loss: 1.8699...  0.1990 sec/batch\n",
      "Epoch: 6/30...  Training Step: 932...  Training loss: 1.8768...  0.2414 sec/batch\n",
      "Epoch: 6/30...  Training Step: 933...  Training loss: 1.8675...  0.2342 sec/batch\n",
      "Epoch: 6/30...  Training Step: 934...  Training loss: 1.8421...  0.2716 sec/batch\n",
      "Epoch: 6/30...  Training Step: 935...  Training loss: 1.8685...  0.3057 sec/batch\n",
      "Epoch: 6/30...  Training Step: 936...  Training loss: 1.8364...  0.2436 sec/batch\n",
      "Epoch: 6/30...  Training Step: 937...  Training loss: 1.8713...  0.2430 sec/batch\n",
      "Epoch: 6/30...  Training Step: 938...  Training loss: 1.8702...  0.1677 sec/batch\n",
      "Epoch: 6/30...  Training Step: 939...  Training loss: 1.8718...  0.2733 sec/batch\n",
      "Epoch: 6/30...  Training Step: 940...  Training loss: 1.8158...  0.2863 sec/batch\n",
      "Epoch: 6/30...  Training Step: 941...  Training loss: 1.8469...  0.2767 sec/batch\n",
      "Epoch: 6/30...  Training Step: 942...  Training loss: 1.8247...  0.2752 sec/batch\n",
      "Epoch: 6/30...  Training Step: 943...  Training loss: 1.8740...  0.2280 sec/batch\n",
      "Epoch: 6/30...  Training Step: 944...  Training loss: 1.8918...  0.2399 sec/batch\n",
      "Epoch: 6/30...  Training Step: 945...  Training loss: 1.8431...  0.1593 sec/batch\n",
      "Epoch: 6/30...  Training Step: 946...  Training loss: 1.8136...  0.3045 sec/batch\n",
      "Epoch: 6/30...  Training Step: 947...  Training loss: 1.8171...  0.2197 sec/batch\n",
      "Epoch: 6/30...  Training Step: 948...  Training loss: 1.8350...  0.3147 sec/batch\n",
      "Epoch: 6/30...  Training Step: 949...  Training loss: 1.8330...  0.1989 sec/batch\n",
      "Epoch: 6/30...  Training Step: 950...  Training loss: 1.8844...  0.1763 sec/batch\n",
      "Epoch: 6/30...  Training Step: 951...  Training loss: 1.8747...  0.2586 sec/batch\n",
      "Epoch: 6/30...  Training Step: 952...  Training loss: 1.8372...  0.2719 sec/batch\n",
      "Epoch: 6/30...  Training Step: 953...  Training loss: 1.8450...  0.2308 sec/batch\n",
      "Epoch: 6/30...  Training Step: 954...  Training loss: 1.8316...  0.3217 sec/batch\n",
      "Epoch: 6/30...  Training Step: 955...  Training loss: 1.8377...  0.2483 sec/batch\n",
      "Epoch: 6/30...  Training Step: 956...  Training loss: 1.8285...  0.2854 sec/batch\n",
      "Epoch: 6/30...  Training Step: 957...  Training loss: 1.8339...  0.1911 sec/batch\n",
      "Epoch: 6/30...  Training Step: 958...  Training loss: 1.8169...  0.2603 sec/batch\n",
      "Epoch: 6/30...  Training Step: 959...  Training loss: 1.8521...  0.2542 sec/batch\n",
      "Epoch: 6/30...  Training Step: 960...  Training loss: 1.8286...  0.2306 sec/batch\n",
      "Epoch: 6/30...  Training Step: 961...  Training loss: 1.8296...  0.1716 sec/batch\n",
      "Epoch: 6/30...  Training Step: 962...  Training loss: 1.8059...  0.2004 sec/batch\n",
      "Epoch: 6/30...  Training Step: 963...  Training loss: 1.8286...  0.1727 sec/batch\n",
      "Epoch: 6/30...  Training Step: 964...  Training loss: 1.8595...  0.2740 sec/batch\n",
      "Epoch: 6/30...  Training Step: 965...  Training loss: 1.8591...  0.1620 sec/batch\n",
      "Epoch: 6/30...  Training Step: 966...  Training loss: 1.8421...  0.1930 sec/batch\n",
      "Epoch: 6/30...  Training Step: 967...  Training loss: 1.8301...  0.1704 sec/batch\n",
      "Epoch: 6/30...  Training Step: 968...  Training loss: 1.8421...  0.1623 sec/batch\n",
      "Epoch: 6/30...  Training Step: 969...  Training loss: 1.8081...  0.1896 sec/batch\n",
      "Epoch: 6/30...  Training Step: 970...  Training loss: 1.8575...  0.1727 sec/batch\n",
      "Epoch: 6/30...  Training Step: 971...  Training loss: 1.8046...  0.1752 sec/batch\n",
      "Epoch: 6/30...  Training Step: 972...  Training loss: 1.8164...  0.2310 sec/batch\n",
      "Epoch: 6/30...  Training Step: 973...  Training loss: 1.8370...  0.1980 sec/batch\n",
      "Epoch: 6/30...  Training Step: 974...  Training loss: 1.8642...  0.2349 sec/batch\n",
      "Epoch: 6/30...  Training Step: 975...  Training loss: 1.8132...  0.1964 sec/batch\n",
      "Epoch: 6/30...  Training Step: 976...  Training loss: 1.8251...  0.2467 sec/batch\n",
      "Epoch: 6/30...  Training Step: 977...  Training loss: 1.8484...  0.1575 sec/batch\n",
      "Epoch: 6/30...  Training Step: 978...  Training loss: 1.8291...  0.1731 sec/batch\n",
      "Epoch: 6/30...  Training Step: 979...  Training loss: 1.8363...  0.1857 sec/batch\n",
      "Epoch: 6/30...  Training Step: 980...  Training loss: 1.8624...  0.2994 sec/batch\n",
      "Epoch: 6/30...  Training Step: 981...  Training loss: 1.8822...  0.2626 sec/batch\n",
      "Epoch: 6/30...  Training Step: 982...  Training loss: 1.8030...  0.2874 sec/batch\n",
      "Epoch: 6/30...  Training Step: 983...  Training loss: 1.8164...  0.2627 sec/batch\n",
      "Epoch: 6/30...  Training Step: 984...  Training loss: 1.8556...  0.2445 sec/batch\n",
      "Epoch: 6/30...  Training Step: 985...  Training loss: 1.8396...  0.2094 sec/batch\n",
      "Epoch: 6/30...  Training Step: 986...  Training loss: 1.8695...  0.1624 sec/batch\n",
      "Epoch: 6/30...  Training Step: 987...  Training loss: 1.7914...  0.2139 sec/batch\n",
      "Epoch: 6/30...  Training Step: 988...  Training loss: 1.8203...  0.2477 sec/batch\n",
      "Epoch: 6/30...  Training Step: 989...  Training loss: 1.8087...  0.2392 sec/batch\n",
      "Epoch: 6/30...  Training Step: 990...  Training loss: 1.8168...  0.2118 sec/batch\n",
      "Epoch: 6/30...  Training Step: 991...  Training loss: 1.8135...  0.1874 sec/batch\n",
      "Epoch: 6/30...  Training Step: 992...  Training loss: 1.8287...  0.1674 sec/batch\n",
      "Epoch: 6/30...  Training Step: 993...  Training loss: 1.7969...  0.2681 sec/batch\n",
      "Epoch: 6/30...  Training Step: 994...  Training loss: 1.8268...  0.2136 sec/batch\n",
      "Epoch: 6/30...  Training Step: 995...  Training loss: 1.8096...  0.2437 sec/batch\n",
      "Epoch: 6/30...  Training Step: 996...  Training loss: 1.7731...  0.1860 sec/batch\n",
      "Epoch: 6/30...  Training Step: 997...  Training loss: 1.8395...  0.2594 sec/batch\n",
      "Epoch: 6/30...  Training Step: 998...  Training loss: 1.8212...  0.2222 sec/batch\n",
      "Epoch: 6/30...  Training Step: 999...  Training loss: 1.8157...  0.1516 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1000...  Training loss: 1.8203...  0.2818 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1001...  Training loss: 1.8320...  0.2609 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1002...  Training loss: 1.8373...  0.2059 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1003...  Training loss: 1.8340...  0.1585 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30...  Training Step: 1004...  Training loss: 1.7936...  0.1634 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1005...  Training loss: 1.8112...  0.3037 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1006...  Training loss: 1.7750...  0.1747 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1007...  Training loss: 1.8102...  0.1588 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1008...  Training loss: 1.8396...  0.1782 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1009...  Training loss: 1.8021...  0.2616 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1010...  Training loss: 1.8427...  0.2002 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1011...  Training loss: 1.8153...  0.2303 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1012...  Training loss: 1.8340...  0.2393 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1013...  Training loss: 1.8255...  0.2741 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1014...  Training loss: 1.7919...  0.2983 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1015...  Training loss: 1.7877...  0.2583 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1016...  Training loss: 1.7984...  0.2662 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1017...  Training loss: 1.7970...  0.2555 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1018...  Training loss: 1.7984...  0.2781 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1019...  Training loss: 1.8365...  0.2544 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1020...  Training loss: 1.7936...  0.2564 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1021...  Training loss: 1.8975...  0.2974 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1022...  Training loss: 1.7879...  0.2605 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1023...  Training loss: 1.7891...  0.2586 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1024...  Training loss: 1.8278...  0.2606 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1025...  Training loss: 1.8086...  0.2432 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1026...  Training loss: 1.8305...  0.2660 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1027...  Training loss: 1.8146...  0.2261 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1028...  Training loss: 1.7756...  0.1588 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1029...  Training loss: 1.7652...  0.1798 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1030...  Training loss: 1.7725...  0.2428 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1031...  Training loss: 1.7663...  0.2373 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1032...  Training loss: 1.8060...  0.1974 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1033...  Training loss: 1.7882...  0.2603 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1034...  Training loss: 1.7866...  0.2850 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1035...  Training loss: 1.8389...  0.2162 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1036...  Training loss: 1.8431...  0.2847 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1037...  Training loss: 1.7800...  0.2506 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1038...  Training loss: 1.8009...  0.2572 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1039...  Training loss: 1.8002...  0.2331 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1040...  Training loss: 1.7882...  0.2676 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1041...  Training loss: 1.7929...  0.2454 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1042...  Training loss: 1.7606...  0.2947 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1043...  Training loss: 1.7636...  0.2864 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1044...  Training loss: 1.7468...  0.2958 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1045...  Training loss: 1.8215...  0.2519 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1046...  Training loss: 1.7838...  0.2591 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1047...  Training loss: 1.8238...  0.2607 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1048...  Training loss: 1.8068...  0.2800 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1049...  Training loss: 1.7916...  0.2672 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1050...  Training loss: 1.7475...  0.1952 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1051...  Training loss: 1.7946...  0.2749 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1052...  Training loss: 1.7806...  0.1728 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1053...  Training loss: 1.7677...  0.2171 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1054...  Training loss: 1.7678...  0.2056 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1055...  Training loss: 1.7698...  0.1633 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1056...  Training loss: 1.7963...  0.2300 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1057...  Training loss: 1.8288...  0.2881 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1058...  Training loss: 1.7957...  0.2004 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1059...  Training loss: 1.7975...  0.1617 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1060...  Training loss: 1.7983...  0.1896 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1061...  Training loss: 1.8138...  0.2023 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1062...  Training loss: 1.7710...  0.2700 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1063...  Training loss: 1.7643...  0.2620 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1064...  Training loss: 1.8210...  0.2186 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1065...  Training loss: 1.8003...  0.1604 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1066...  Training loss: 1.7809...  0.2273 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1067...  Training loss: 1.8016...  0.1576 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1068...  Training loss: 1.7988...  0.2924 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1069...  Training loss: 1.7876...  0.3089 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1070...  Training loss: 1.7914...  0.2134 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1071...  Training loss: 1.8483...  0.1909 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1072...  Training loss: 1.7747...  0.2834 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1073...  Training loss: 1.7647...  0.3108 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1074...  Training loss: 1.7742...  0.1886 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1075...  Training loss: 1.7820...  0.1753 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1076...  Training loss: 1.8113...  0.2210 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1077...  Training loss: 1.7983...  0.2271 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1078...  Training loss: 1.7608...  0.2265 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1079...  Training loss: 1.8085...  0.2050 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1080...  Training loss: 1.7883...  0.1564 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1081...  Training loss: 1.8092...  0.1872 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1082...  Training loss: 1.7840...  0.1787 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1083...  Training loss: 1.7720...  0.2635 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1084...  Training loss: 1.7828...  0.1833 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1085...  Training loss: 1.8146...  0.2260 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1086...  Training loss: 1.7972...  0.2679 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1087...  Training loss: 1.8143...  0.2515 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1088...  Training loss: 1.7601...  0.2878 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1089...  Training loss: 1.7920...  0.2588 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1090...  Training loss: 1.7718...  0.2064 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1091...  Training loss: 1.7515...  0.1911 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1092...  Training loss: 1.7792...  0.2021 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1093...  Training loss: 1.7879...  0.1772 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1094...  Training loss: 1.8121...  0.1598 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1095...  Training loss: 1.7856...  0.2066 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1096...  Training loss: 1.7517...  0.1825 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1097...  Training loss: 1.7746...  0.2512 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1098...  Training loss: 1.7853...  0.1675 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1099...  Training loss: 1.7949...  0.2724 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1100...  Training loss: 1.7964...  0.2571 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1101...  Training loss: 1.7955...  0.2574 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1102...  Training loss: 1.7889...  0.1577 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/30...  Training Step: 1103...  Training loss: 1.7879...  0.2512 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1104...  Training loss: 1.7659...  0.2025 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1105...  Training loss: 1.7872...  0.2601 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1106...  Training loss: 1.7565...  0.2553 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1107...  Training loss: 1.7911...  0.2905 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1108...  Training loss: 1.7878...  0.2540 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1109...  Training loss: 1.7903...  0.2511 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1110...  Training loss: 1.7330...  0.2232 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1111...  Training loss: 1.7602...  0.2227 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1112...  Training loss: 1.7500...  0.1785 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1113...  Training loss: 1.7902...  0.2911 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1114...  Training loss: 1.8060...  0.2552 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1115...  Training loss: 1.7742...  0.3331 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1116...  Training loss: 1.7383...  0.2340 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1117...  Training loss: 1.7366...  0.3017 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1118...  Training loss: 1.7674...  0.2587 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1119...  Training loss: 1.7618...  0.2710 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1120...  Training loss: 1.8020...  0.3199 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1121...  Training loss: 1.8107...  0.2820 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1122...  Training loss: 1.7666...  0.1932 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1123...  Training loss: 1.7636...  0.1897 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1124...  Training loss: 1.7712...  0.2947 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1125...  Training loss: 1.7591...  0.1685 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1126...  Training loss: 1.7528...  0.2028 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1127...  Training loss: 1.7745...  0.2658 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1128...  Training loss: 1.7390...  0.3351 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1129...  Training loss: 1.7846...  0.2327 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1130...  Training loss: 1.7600...  0.1631 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1131...  Training loss: 1.7359...  0.3498 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1132...  Training loss: 1.7275...  0.2041 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1133...  Training loss: 1.7452...  0.2506 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1134...  Training loss: 1.7763...  0.2640 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1135...  Training loss: 1.7720...  0.2122 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1136...  Training loss: 1.7589...  0.2716 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1137...  Training loss: 1.7461...  0.2662 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1138...  Training loss: 1.7801...  0.1621 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1139...  Training loss: 1.7326...  0.1750 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1140...  Training loss: 1.7734...  0.1859 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1141...  Training loss: 1.7319...  0.3224 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1142...  Training loss: 1.7500...  0.1575 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1143...  Training loss: 1.7612...  0.2314 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1144...  Training loss: 1.7923...  0.2793 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1145...  Training loss: 1.7306...  0.2639 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1146...  Training loss: 1.7408...  0.1768 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1147...  Training loss: 1.7721...  0.1711 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1148...  Training loss: 1.7511...  0.2543 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1149...  Training loss: 1.7679...  0.1794 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1150...  Training loss: 1.7933...  0.1937 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1151...  Training loss: 1.8090...  0.2557 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1152...  Training loss: 1.7288...  0.1847 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1153...  Training loss: 1.7429...  0.1965 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1154...  Training loss: 1.7828...  0.2606 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1155...  Training loss: 1.7707...  0.2714 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1156...  Training loss: 1.8011...  0.2547 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1157...  Training loss: 1.7221...  0.1719 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1158...  Training loss: 1.7486...  0.1812 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1159...  Training loss: 1.7375...  0.1819 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1160...  Training loss: 1.7435...  0.2916 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1161...  Training loss: 1.7411...  0.1835 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1162...  Training loss: 1.7562...  0.1774 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1163...  Training loss: 1.7296...  0.1786 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1164...  Training loss: 1.7453...  0.2008 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1165...  Training loss: 1.7289...  0.2732 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1166...  Training loss: 1.6981...  0.2698 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1167...  Training loss: 1.7720...  0.2894 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1168...  Training loss: 1.7497...  0.1950 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1169...  Training loss: 1.7439...  0.2958 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1170...  Training loss: 1.7436...  0.2007 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1171...  Training loss: 1.7560...  0.2039 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1172...  Training loss: 1.7680...  0.2115 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1173...  Training loss: 1.7422...  0.2387 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1174...  Training loss: 1.7229...  0.2699 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1175...  Training loss: 1.7221...  0.2585 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1176...  Training loss: 1.6977...  0.2506 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1177...  Training loss: 1.7222...  0.1743 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1178...  Training loss: 1.7606...  0.2058 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1179...  Training loss: 1.7236...  0.2500 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1180...  Training loss: 1.7692...  0.1642 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1181...  Training loss: 1.7303...  0.1938 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1182...  Training loss: 1.7716...  0.1790 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1183...  Training loss: 1.7554...  0.1792 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1184...  Training loss: 1.7264...  0.2823 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1185...  Training loss: 1.7130...  0.2863 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1186...  Training loss: 1.7107...  0.2107 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1187...  Training loss: 1.7244...  0.1810 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1188...  Training loss: 1.7359...  0.1671 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1189...  Training loss: 1.7556...  0.1709 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1190...  Training loss: 1.7051...  0.1842 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1191...  Training loss: 1.8083...  0.2641 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1192...  Training loss: 1.7123...  0.1835 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1193...  Training loss: 1.7175...  0.1713 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1194...  Training loss: 1.7608...  0.2482 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1195...  Training loss: 1.7468...  0.3304 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1196...  Training loss: 1.7695...  0.2684 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1197...  Training loss: 1.7360...  0.1653 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1198...  Training loss: 1.7153...  0.1783 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1199...  Training loss: 1.7028...  0.2894 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1200...  Training loss: 1.6992...  0.2004 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1201...  Training loss: 1.6953...  0.1907 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30...  Training Step: 1202...  Training loss: 1.7208...  0.1998 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1203...  Training loss: 1.7180...  0.1656 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1204...  Training loss: 1.7084...  0.2093 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1205...  Training loss: 1.7774...  0.2004 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1206...  Training loss: 1.7729...  0.3042 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1207...  Training loss: 1.6980...  0.2991 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1208...  Training loss: 1.7352...  0.2195 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1209...  Training loss: 1.7267...  0.1726 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1210...  Training loss: 1.7193...  0.2203 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1211...  Training loss: 1.7331...  0.2078 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1212...  Training loss: 1.6863...  0.1702 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1213...  Training loss: 1.6971...  0.1579 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1214...  Training loss: 1.6755...  0.2031 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1215...  Training loss: 1.7410...  0.2030 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1216...  Training loss: 1.7244...  0.2450 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1217...  Training loss: 1.7529...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1218...  Training loss: 1.7273...  0.1921 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1219...  Training loss: 1.7177...  0.1921 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1220...  Training loss: 1.6887...  0.1964 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1221...  Training loss: 1.7192...  0.2096 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1222...  Training loss: 1.7158...  0.3081 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1223...  Training loss: 1.7108...  0.2348 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1224...  Training loss: 1.7044...  0.2122 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1225...  Training loss: 1.7003...  0.1694 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1226...  Training loss: 1.7276...  0.2512 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1227...  Training loss: 1.7676...  0.2674 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1228...  Training loss: 1.7432...  0.1706 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1229...  Training loss: 1.7311...  0.2264 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1230...  Training loss: 1.7356...  0.2161 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1231...  Training loss: 1.7384...  0.1681 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1232...  Training loss: 1.7157...  0.2137 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1233...  Training loss: 1.6928...  0.2886 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1234...  Training loss: 1.7559...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1235...  Training loss: 1.7313...  0.1576 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1236...  Training loss: 1.7099...  0.1904 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1237...  Training loss: 1.7412...  0.2063 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1238...  Training loss: 1.7403...  0.2665 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1239...  Training loss: 1.7303...  0.2727 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1240...  Training loss: 1.7380...  0.1814 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1241...  Training loss: 1.7744...  0.1741 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1242...  Training loss: 1.7168...  0.1929 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1243...  Training loss: 1.7068...  0.1675 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1244...  Training loss: 1.7209...  0.1554 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1245...  Training loss: 1.7184...  0.2886 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1246...  Training loss: 1.7469...  0.2621 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1247...  Training loss: 1.7382...  0.2914 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1248...  Training loss: 1.6931...  0.2550 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1249...  Training loss: 1.7324...  0.2462 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1250...  Training loss: 1.7237...  0.2143 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1251...  Training loss: 1.7290...  0.3138 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1252...  Training loss: 1.7211...  0.1756 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1253...  Training loss: 1.7137...  0.2568 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1254...  Training loss: 1.7135...  0.2769 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1255...  Training loss: 1.7457...  0.2024 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1256...  Training loss: 1.7347...  0.1831 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1257...  Training loss: 1.7336...  0.1655 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1258...  Training loss: 1.6990...  0.2646 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1259...  Training loss: 1.7096...  0.2258 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1260...  Training loss: 1.7105...  0.3452 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1261...  Training loss: 1.6879...  0.1559 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1262...  Training loss: 1.7085...  0.2549 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1263...  Training loss: 1.7123...  0.2174 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1264...  Training loss: 1.7315...  0.2371 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1265...  Training loss: 1.7184...  0.2590 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1266...  Training loss: 1.6707...  0.2458 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1267...  Training loss: 1.7005...  0.2615 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1268...  Training loss: 1.7260...  0.2477 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1269...  Training loss: 1.7347...  0.2599 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1270...  Training loss: 1.7362...  0.1879 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1271...  Training loss: 1.7291...  0.2379 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1272...  Training loss: 1.7105...  0.1967 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1273...  Training loss: 1.7252...  0.1725 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1274...  Training loss: 1.7006...  0.2360 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1275...  Training loss: 1.7227...  0.1629 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1276...  Training loss: 1.6975...  0.1702 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1277...  Training loss: 1.7399...  0.2449 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1278...  Training loss: 1.7312...  0.2215 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1279...  Training loss: 1.7245...  0.2944 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1280...  Training loss: 1.6807...  0.2307 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1281...  Training loss: 1.6995...  0.1701 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1282...  Training loss: 1.6733...  0.2160 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1283...  Training loss: 1.7219...  0.2455 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1284...  Training loss: 1.7423...  0.2537 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1285...  Training loss: 1.7079...  0.1747 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1286...  Training loss: 1.6699...  0.2398 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1287...  Training loss: 1.6691...  0.2291 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1288...  Training loss: 1.6924...  0.1947 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1289...  Training loss: 1.6941...  0.2755 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1290...  Training loss: 1.7388...  0.1735 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1291...  Training loss: 1.7489...  0.1615 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1292...  Training loss: 1.7035...  0.2122 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1293...  Training loss: 1.6915...  0.2734 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1294...  Training loss: 1.7093...  0.2793 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1295...  Training loss: 1.7019...  0.2697 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1296...  Training loss: 1.6961...  0.2876 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1297...  Training loss: 1.7140...  0.2863 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1298...  Training loss: 1.6780...  0.2285 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1299...  Training loss: 1.7183...  0.1664 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1300...  Training loss: 1.6907...  0.1590 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30...  Training Step: 1301...  Training loss: 1.6882...  0.2484 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1302...  Training loss: 1.6665...  0.2628 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1303...  Training loss: 1.6981...  0.2665 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1304...  Training loss: 1.7146...  0.2766 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1305...  Training loss: 1.7181...  0.2289 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1306...  Training loss: 1.7075...  0.2451 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1307...  Training loss: 1.6759...  0.2501 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1308...  Training loss: 1.7156...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1309...  Training loss: 1.6784...  0.2392 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1310...  Training loss: 1.7087...  0.2423 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1311...  Training loss: 1.6708...  0.2057 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1312...  Training loss: 1.6844...  0.2959 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1313...  Training loss: 1.7012...  0.2356 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1314...  Training loss: 1.7316...  0.1645 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1315...  Training loss: 1.6801...  0.2244 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1316...  Training loss: 1.6759...  0.1904 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1317...  Training loss: 1.7127...  0.2482 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1318...  Training loss: 1.7041...  0.2003 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1319...  Training loss: 1.7111...  0.2593 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1320...  Training loss: 1.7298...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1321...  Training loss: 1.7506...  0.2832 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1322...  Training loss: 1.6697...  0.2101 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1323...  Training loss: 1.6785...  0.1674 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1324...  Training loss: 1.7274...  0.2562 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1325...  Training loss: 1.7095...  0.2086 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1326...  Training loss: 1.7356...  0.2745 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1327...  Training loss: 1.6707...  0.2132 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1328...  Training loss: 1.6804...  0.2415 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1329...  Training loss: 1.6759...  0.2669 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1330...  Training loss: 1.6753...  0.2757 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1331...  Training loss: 1.6837...  0.2653 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1332...  Training loss: 1.6905...  0.2931 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1333...  Training loss: 1.6672...  0.2666 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1334...  Training loss: 1.6880...  0.2661 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1335...  Training loss: 1.6712...  0.3489 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1336...  Training loss: 1.6438...  0.2069 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1337...  Training loss: 1.7121...  0.2878 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1338...  Training loss: 1.6849...  0.2116 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1339...  Training loss: 1.6766...  0.1968 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1340...  Training loss: 1.6871...  0.2556 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1341...  Training loss: 1.6864...  0.3094 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1342...  Training loss: 1.7085...  0.1973 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1343...  Training loss: 1.6889...  0.2576 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1344...  Training loss: 1.6753...  0.1660 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1345...  Training loss: 1.6747...  0.1580 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1346...  Training loss: 1.6299...  0.1705 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1347...  Training loss: 1.6724...  0.2850 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1348...  Training loss: 1.6982...  0.2237 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1349...  Training loss: 1.6676...  0.1544 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1350...  Training loss: 1.7088...  0.1682 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1351...  Training loss: 1.6748...  0.2616 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1352...  Training loss: 1.7055...  0.1941 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1353...  Training loss: 1.6973...  0.1556 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1354...  Training loss: 1.6699...  0.1931 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1355...  Training loss: 1.6610...  0.1643 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1356...  Training loss: 1.6581...  0.1537 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1357...  Training loss: 1.6679...  0.2676 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1358...  Training loss: 1.6796...  0.2528 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1359...  Training loss: 1.6867...  0.2835 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1360...  Training loss: 1.6567...  0.2706 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1361...  Training loss: 1.7546...  0.2001 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1362...  Training loss: 1.6447...  0.2875 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1363...  Training loss: 1.6644...  0.2334 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1364...  Training loss: 1.7097...  0.2610 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1365...  Training loss: 1.6881...  0.1820 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1366...  Training loss: 1.7124...  0.2238 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1367...  Training loss: 1.6849...  0.2519 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1368...  Training loss: 1.6621...  0.2774 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1369...  Training loss: 1.6372...  0.2608 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1370...  Training loss: 1.6460...  0.2016 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1371...  Training loss: 1.6314...  0.1611 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1372...  Training loss: 1.6573...  0.2814 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1373...  Training loss: 1.6699...  0.1711 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1374...  Training loss: 1.6609...  0.1703 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1375...  Training loss: 1.7265...  0.1640 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1376...  Training loss: 1.7235...  0.2278 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1377...  Training loss: 1.6370...  0.1843 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1378...  Training loss: 1.6724...  0.2565 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1379...  Training loss: 1.6679...  0.2087 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1380...  Training loss: 1.6656...  0.2166 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1381...  Training loss: 1.6828...  0.2450 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1382...  Training loss: 1.6403...  0.2312 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1383...  Training loss: 1.6462...  0.2852 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1384...  Training loss: 1.6275...  0.1946 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1385...  Training loss: 1.6960...  0.1803 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1386...  Training loss: 1.6648...  0.1941 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1387...  Training loss: 1.6999...  0.3052 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1388...  Training loss: 1.6867...  0.1643 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1389...  Training loss: 1.6523...  0.2306 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1390...  Training loss: 1.6313...  0.3071 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1391...  Training loss: 1.6595...  0.2057 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1392...  Training loss: 1.6623...  0.2012 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1393...  Training loss: 1.6628...  0.3036 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1394...  Training loss: 1.6305...  0.3084 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1395...  Training loss: 1.6506...  0.2308 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1396...  Training loss: 1.6653...  0.2886 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1397...  Training loss: 1.7081...  0.1945 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1398...  Training loss: 1.6776...  0.1639 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1399...  Training loss: 1.6801...  0.1881 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30...  Training Step: 1400...  Training loss: 1.6848...  0.2042 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1401...  Training loss: 1.6859...  0.1825 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1402...  Training loss: 1.6544...  0.2551 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1403...  Training loss: 1.6489...  0.1808 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1404...  Training loss: 1.7095...  0.2233 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1405...  Training loss: 1.6814...  0.2278 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1406...  Training loss: 1.6533...  0.2506 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1407...  Training loss: 1.6876...  0.1853 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1408...  Training loss: 1.6815...  0.1660 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1409...  Training loss: 1.6776...  0.1964 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1410...  Training loss: 1.6838...  0.2802 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1411...  Training loss: 1.7223...  0.2847 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1412...  Training loss: 1.6665...  0.2599 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1413...  Training loss: 1.6461...  0.2310 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1414...  Training loss: 1.6579...  0.2533 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1415...  Training loss: 1.6746...  0.2833 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1416...  Training loss: 1.6840...  0.2819 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1417...  Training loss: 1.6795...  0.1840 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1418...  Training loss: 1.6350...  0.2603 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1419...  Training loss: 1.6895...  0.1806 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1420...  Training loss: 1.6714...  0.1856 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1421...  Training loss: 1.6799...  0.1785 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1422...  Training loss: 1.6684...  0.1577 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1423...  Training loss: 1.6509...  0.2299 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1424...  Training loss: 1.6658...  0.1982 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1425...  Training loss: 1.6931...  0.3246 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1426...  Training loss: 1.6821...  0.2695 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1427...  Training loss: 1.7016...  0.3667 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1428...  Training loss: 1.6526...  0.1702 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1429...  Training loss: 1.6664...  0.2980 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1430...  Training loss: 1.6584...  0.2651 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1431...  Training loss: 1.6234...  0.2036 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1432...  Training loss: 1.6611...  0.2085 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1433...  Training loss: 1.6637...  0.2325 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1434...  Training loss: 1.6902...  0.2365 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1435...  Training loss: 1.6777...  0.2785 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1436...  Training loss: 1.6201...  0.2092 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1437...  Training loss: 1.6527...  0.2533 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1438...  Training loss: 1.6719...  0.1697 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1439...  Training loss: 1.6767...  0.2648 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1440...  Training loss: 1.6878...  0.2148 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1441...  Training loss: 1.6855...  0.2598 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1442...  Training loss: 1.6666...  0.1911 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1443...  Training loss: 1.6769...  0.2625 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1444...  Training loss: 1.6421...  0.2176 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1445...  Training loss: 1.6639...  0.2377 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1446...  Training loss: 1.6545...  0.2342 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1447...  Training loss: 1.6796...  0.2360 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1448...  Training loss: 1.6812...  0.2082 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1449...  Training loss: 1.6779...  0.2588 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1450...  Training loss: 1.6339...  0.2653 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1451...  Training loss: 1.6503...  0.2748 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1452...  Training loss: 1.6314...  0.2734 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1453...  Training loss: 1.6758...  0.2379 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1454...  Training loss: 1.7005...  0.2495 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1455...  Training loss: 1.6463...  0.2038 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1456...  Training loss: 1.6155...  0.2512 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1457...  Training loss: 1.6204...  0.2400 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1458...  Training loss: 1.6435...  0.1986 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1459...  Training loss: 1.6522...  0.2489 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1460...  Training loss: 1.7013...  0.2611 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1461...  Training loss: 1.6886...  0.1925 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1462...  Training loss: 1.6436...  0.1654 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1463...  Training loss: 1.6457...  0.2566 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1464...  Training loss: 1.6644...  0.1603 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1465...  Training loss: 1.6522...  0.2549 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1466...  Training loss: 1.6399...  0.2452 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1467...  Training loss: 1.6535...  0.2438 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1468...  Training loss: 1.6346...  0.2645 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1469...  Training loss: 1.6631...  0.2781 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1470...  Training loss: 1.6486...  0.2012 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1471...  Training loss: 1.6348...  0.2660 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1472...  Training loss: 1.6215...  0.2546 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1473...  Training loss: 1.6451...  0.2420 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1474...  Training loss: 1.6713...  0.2604 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1475...  Training loss: 1.6682...  0.1639 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1476...  Training loss: 1.6486...  0.1709 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1477...  Training loss: 1.6238...  0.1808 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1478...  Training loss: 1.6579...  0.1628 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1479...  Training loss: 1.6260...  0.2446 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1480...  Training loss: 1.6573...  0.2228 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1481...  Training loss: 1.6223...  0.1817 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1482...  Training loss: 1.6388...  0.2685 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1483...  Training loss: 1.6566...  0.2677 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1484...  Training loss: 1.6777...  0.2612 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1485...  Training loss: 1.6339...  0.2590 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1486...  Training loss: 1.6415...  0.1628 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1487...  Training loss: 1.6547...  0.1944 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1488...  Training loss: 1.6548...  0.2025 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1489...  Training loss: 1.6582...  0.1588 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1490...  Training loss: 1.6800...  0.2628 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1491...  Training loss: 1.7002...  0.2910 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1492...  Training loss: 1.6159...  0.2326 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1493...  Training loss: 1.6274...  0.2000 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1494...  Training loss: 1.6780...  0.2143 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1495...  Training loss: 1.6688...  0.2294 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1496...  Training loss: 1.6795...  0.2737 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1497...  Training loss: 1.6231...  0.3579 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1498...  Training loss: 1.6300...  0.2177 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30...  Training Step: 1499...  Training loss: 1.6259...  0.2452 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1500...  Training loss: 1.6439...  0.2967 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1501...  Training loss: 1.6405...  0.2061 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1502...  Training loss: 1.6422...  0.1885 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1503...  Training loss: 1.6270...  0.2647 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1504...  Training loss: 1.6420...  0.1843 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1505...  Training loss: 1.6321...  0.2723 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1506...  Training loss: 1.5968...  0.2165 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1507...  Training loss: 1.6623...  0.2759 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1508...  Training loss: 1.6353...  0.1991 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1509...  Training loss: 1.6356...  0.2861 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1510...  Training loss: 1.6433...  0.1967 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1511...  Training loss: 1.6487...  0.1610 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1512...  Training loss: 1.6602...  0.2194 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1513...  Training loss: 1.6424...  0.2021 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1514...  Training loss: 1.6287...  0.1601 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1515...  Training loss: 1.6280...  0.2180 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1516...  Training loss: 1.6019...  0.1991 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1517...  Training loss: 1.6209...  0.2937 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1518...  Training loss: 1.6468...  0.1721 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1519...  Training loss: 1.6306...  0.1555 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1520...  Training loss: 1.6650...  0.1933 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1521...  Training loss: 1.6331...  0.2232 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1522...  Training loss: 1.6552...  0.1933 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1523...  Training loss: 1.6544...  0.2167 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1524...  Training loss: 1.6204...  0.2338 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1525...  Training loss: 1.6182...  0.1594 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1526...  Training loss: 1.6191...  0.2747 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1527...  Training loss: 1.6251...  0.2121 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1528...  Training loss: 1.6309...  0.2380 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1529...  Training loss: 1.6351...  0.2717 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1530...  Training loss: 1.6087...  0.2424 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1531...  Training loss: 1.7148...  0.2152 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1532...  Training loss: 1.6023...  0.2030 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1533...  Training loss: 1.6177...  0.2688 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1534...  Training loss: 1.6644...  0.2028 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1535...  Training loss: 1.6425...  0.1774 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1536...  Training loss: 1.6661...  0.2279 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1537...  Training loss: 1.6389...  0.2740 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1538...  Training loss: 1.6177...  0.2418 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1539...  Training loss: 1.6014...  0.2708 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1540...  Training loss: 1.6027...  0.2170 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1541...  Training loss: 1.5914...  0.1678 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1542...  Training loss: 1.6158...  0.2775 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1543...  Training loss: 1.6114...  0.2197 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1544...  Training loss: 1.6024...  0.2489 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1545...  Training loss: 1.6781...  0.2481 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1546...  Training loss: 1.6735...  0.1934 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1547...  Training loss: 1.6067...  0.1991 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1548...  Training loss: 1.6221...  0.1702 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1549...  Training loss: 1.6307...  0.1867 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1550...  Training loss: 1.6260...  0.1980 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1551...  Training loss: 1.6311...  0.1601 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1552...  Training loss: 1.5968...  0.2129 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1553...  Training loss: 1.5912...  0.3046 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1554...  Training loss: 1.5883...  0.3058 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1555...  Training loss: 1.6397...  0.1863 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1556...  Training loss: 1.6279...  0.2908 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1557...  Training loss: 1.6520...  0.2736 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1558...  Training loss: 1.6470...  0.2101 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1559...  Training loss: 1.6059...  0.1654 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1560...  Training loss: 1.5812...  0.1624 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1561...  Training loss: 1.6201...  0.2402 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1562...  Training loss: 1.5977...  0.2432 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1563...  Training loss: 1.6141...  0.2431 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1564...  Training loss: 1.5950...  0.1924 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1565...  Training loss: 1.5996...  0.1597 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1566...  Training loss: 1.6211...  0.2021 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1567...  Training loss: 1.6578...  0.1791 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1568...  Training loss: 1.6395...  0.1953 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1569...  Training loss: 1.6197...  0.2598 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1570...  Training loss: 1.6412...  0.2471 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1571...  Training loss: 1.6352...  0.2087 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1572...  Training loss: 1.6133...  0.1682 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1573...  Training loss: 1.6008...  0.1751 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1574...  Training loss: 1.6705...  0.1569 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1575...  Training loss: 1.6369...  0.1704 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1576...  Training loss: 1.6142...  0.1568 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1577...  Training loss: 1.6489...  0.1589 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1578...  Training loss: 1.6414...  0.1779 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1579...  Training loss: 1.6286...  0.2953 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1580...  Training loss: 1.6394...  0.2541 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1581...  Training loss: 1.6738...  0.2142 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1582...  Training loss: 1.6153...  0.2062 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1583...  Training loss: 1.6001...  0.2678 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1584...  Training loss: 1.6149...  0.2085 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1585...  Training loss: 1.6191...  0.2289 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1586...  Training loss: 1.6419...  0.2264 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1587...  Training loss: 1.6257...  0.2649 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1588...  Training loss: 1.5884...  0.2604 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1589...  Training loss: 1.6295...  0.1896 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1590...  Training loss: 1.6229...  0.1968 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1591...  Training loss: 1.6342...  0.1836 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1592...  Training loss: 1.6231...  0.1675 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1593...  Training loss: 1.6082...  0.2417 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1594...  Training loss: 1.6298...  0.1735 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1595...  Training loss: 1.6476...  0.1563 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1596...  Training loss: 1.6450...  0.2814 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30...  Training Step: 1597...  Training loss: 1.6456...  0.2010 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1598...  Training loss: 1.6100...  0.2768 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1599...  Training loss: 1.6119...  0.3001 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1600...  Training loss: 1.6152...  0.2578 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1601...  Training loss: 1.5864...  0.2546 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1602...  Training loss: 1.6221...  0.3229 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1603...  Training loss: 1.6236...  0.2028 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1604...  Training loss: 1.6400...  0.2530 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1605...  Training loss: 1.6361...  0.1895 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1606...  Training loss: 1.5846...  0.2248 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1607...  Training loss: 1.6155...  0.1675 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1608...  Training loss: 1.6289...  0.2344 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1609...  Training loss: 1.6443...  0.2006 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1610...  Training loss: 1.6398...  0.2122 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1611...  Training loss: 1.6416...  0.1944 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1612...  Training loss: 1.6253...  0.2114 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1613...  Training loss: 1.6287...  0.2469 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1614...  Training loss: 1.6047...  0.1707 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1615...  Training loss: 1.6154...  0.1749 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1616...  Training loss: 1.5956...  0.1527 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1617...  Training loss: 1.6390...  0.2643 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1618...  Training loss: 1.6416...  0.2201 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1619...  Training loss: 1.6357...  0.2378 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1620...  Training loss: 1.5838...  0.1919 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1621...  Training loss: 1.6078...  0.2634 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1622...  Training loss: 1.5805...  0.1873 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1623...  Training loss: 1.6289...  0.2577 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1624...  Training loss: 1.6467...  0.2134 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1625...  Training loss: 1.6101...  0.2257 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1626...  Training loss: 1.5700...  0.1524 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1627...  Training loss: 1.5871...  0.1769 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1628...  Training loss: 1.6004...  0.2409 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1629...  Training loss: 1.6107...  0.2808 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1630...  Training loss: 1.6623...  0.2554 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1631...  Training loss: 1.6435...  0.2752 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1632...  Training loss: 1.6123...  0.2490 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1633...  Training loss: 1.6106...  0.2574 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1634...  Training loss: 1.6176...  0.1992 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1635...  Training loss: 1.6004...  0.2556 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1636...  Training loss: 1.6000...  0.2317 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1637...  Training loss: 1.6149...  0.2617 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1638...  Training loss: 1.5917...  0.2947 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1639...  Training loss: 1.6354...  0.1977 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1640...  Training loss: 1.5953...  0.1609 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1641...  Training loss: 1.5921...  0.1559 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1642...  Training loss: 1.5719...  0.1797 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1643...  Training loss: 1.6076...  0.2474 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1644...  Training loss: 1.6261...  0.1647 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1645...  Training loss: 1.6263...  0.1719 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1646...  Training loss: 1.6147...  0.2489 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1647...  Training loss: 1.5833...  0.1597 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1648...  Training loss: 1.6145...  0.2463 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1649...  Training loss: 1.5854...  0.2958 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1650...  Training loss: 1.6098...  0.2879 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1651...  Training loss: 1.5867...  0.2768 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1652...  Training loss: 1.5951...  0.2355 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1653...  Training loss: 1.6009...  0.1945 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1654...  Training loss: 1.6283...  0.2588 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1655...  Training loss: 1.5879...  0.2016 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1656...  Training loss: 1.5957...  0.1715 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1657...  Training loss: 1.6210...  0.2802 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1658...  Training loss: 1.6066...  0.2820 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1659...  Training loss: 1.6173...  0.2560 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1660...  Training loss: 1.6392...  0.2090 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1661...  Training loss: 1.6596...  0.2765 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1662...  Training loss: 1.5748...  0.1737 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1663...  Training loss: 1.5828...  0.1957 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1664...  Training loss: 1.6384...  0.1520 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1665...  Training loss: 1.6142...  0.1889 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1666...  Training loss: 1.6492...  0.1932 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1667...  Training loss: 1.5759...  0.1910 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1668...  Training loss: 1.5966...  0.2599 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1669...  Training loss: 1.5962...  0.1626 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1670...  Training loss: 1.6093...  0.2201 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1671...  Training loss: 1.5896...  0.2116 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1672...  Training loss: 1.6025...  0.2013 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1673...  Training loss: 1.5881...  0.2154 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1674...  Training loss: 1.5998...  0.1651 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1675...  Training loss: 1.5803...  0.1951 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1676...  Training loss: 1.5576...  0.1887 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1677...  Training loss: 1.6254...  0.1966 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1678...  Training loss: 1.5994...  0.2499 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1679...  Training loss: 1.5903...  0.2718 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1680...  Training loss: 1.5925...  0.2632 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1681...  Training loss: 1.6044...  0.2644 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1682...  Training loss: 1.6179...  0.2389 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1683...  Training loss: 1.5937...  0.2022 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1684...  Training loss: 1.5852...  0.1999 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1685...  Training loss: 1.5781...  0.2615 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1686...  Training loss: 1.5539...  0.2564 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1687...  Training loss: 1.5809...  0.2842 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1688...  Training loss: 1.6041...  0.1830 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1689...  Training loss: 1.5836...  0.1756 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1690...  Training loss: 1.6199...  0.2102 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1691...  Training loss: 1.5882...  0.2783 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1692...  Training loss: 1.6267...  0.2058 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1693...  Training loss: 1.6079...  0.2134 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1694...  Training loss: 1.5850...  0.2626 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30...  Training Step: 1695...  Training loss: 1.5649...  0.2583 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1696...  Training loss: 1.5695...  0.2342 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1697...  Training loss: 1.5885...  0.2432 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1698...  Training loss: 1.5895...  0.2428 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1699...  Training loss: 1.6033...  0.2475 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1700...  Training loss: 1.5742...  0.2302 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1701...  Training loss: 1.6647...  0.1942 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1702...  Training loss: 1.5638...  0.1974 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1703...  Training loss: 1.5824...  0.1652 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1704...  Training loss: 1.6133...  0.2819 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1705...  Training loss: 1.6119...  0.2119 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1706...  Training loss: 1.6265...  0.1901 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1707...  Training loss: 1.6032...  0.1635 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1708...  Training loss: 1.5824...  0.2290 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1709...  Training loss: 1.5564...  0.1773 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1710...  Training loss: 1.5686...  0.1566 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1711...  Training loss: 1.5570...  0.1930 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1712...  Training loss: 1.5851...  0.2097 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1713...  Training loss: 1.5723...  0.1855 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1714...  Training loss: 1.5655...  0.2149 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1715...  Training loss: 1.6335...  0.2354 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1716...  Training loss: 1.6359...  0.1862 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1717...  Training loss: 1.5701...  0.2454 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1718...  Training loss: 1.5893...  0.2137 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1719...  Training loss: 1.5775...  0.1586 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1720...  Training loss: 1.5772...  0.2657 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1721...  Training loss: 1.6001...  0.2054 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1722...  Training loss: 1.5515...  0.2677 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1723...  Training loss: 1.5586...  0.1749 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1724...  Training loss: 1.5415...  0.1887 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1725...  Training loss: 1.5997...  0.2479 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1726...  Training loss: 1.5775...  0.2481 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1727...  Training loss: 1.6107...  0.1846 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1728...  Training loss: 1.6060...  0.2349 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1729...  Training loss: 1.5783...  0.1658 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1730...  Training loss: 1.5606...  0.2224 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1731...  Training loss: 1.5763...  0.2289 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1732...  Training loss: 1.5588...  0.1693 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1733...  Training loss: 1.5692...  0.1898 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1734...  Training loss: 1.5542...  0.2311 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1735...  Training loss: 1.5488...  0.1960 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1736...  Training loss: 1.5866...  0.1879 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1737...  Training loss: 1.6311...  0.2274 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1738...  Training loss: 1.5907...  0.2242 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1739...  Training loss: 1.5849...  0.1995 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1740...  Training loss: 1.5987...  0.2280 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1741...  Training loss: 1.5947...  0.2548 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1742...  Training loss: 1.5763...  0.2198 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1743...  Training loss: 1.5618...  0.2094 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1744...  Training loss: 1.6282...  0.2079 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1745...  Training loss: 1.6067...  0.2144 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1746...  Training loss: 1.5822...  0.2314 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1747...  Training loss: 1.6038...  0.2201 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1748...  Training loss: 1.6044...  0.2129 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1749...  Training loss: 1.5815...  0.1951 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1750...  Training loss: 1.5931...  0.3309 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1751...  Training loss: 1.6370...  0.2520 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1752...  Training loss: 1.5715...  0.3036 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1753...  Training loss: 1.5716...  0.1548 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1754...  Training loss: 1.5810...  0.2367 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1755...  Training loss: 1.5888...  0.2217 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1756...  Training loss: 1.6041...  0.1968 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1757...  Training loss: 1.5971...  0.2435 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1758...  Training loss: 1.5576...  0.2647 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1759...  Training loss: 1.6027...  0.2439 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1760...  Training loss: 1.5856...  0.2683 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1761...  Training loss: 1.5959...  0.1828 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1762...  Training loss: 1.5916...  0.1700 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1763...  Training loss: 1.5777...  0.1774 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1764...  Training loss: 1.5747...  0.2790 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1765...  Training loss: 1.6084...  0.2476 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1766...  Training loss: 1.6052...  0.2314 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1767...  Training loss: 1.6073...  0.3195 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1768...  Training loss: 1.5780...  0.1680 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1769...  Training loss: 1.5839...  0.2044 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1770...  Training loss: 1.5737...  0.2603 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1771...  Training loss: 1.5478...  0.1693 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1772...  Training loss: 1.5815...  0.2095 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1773...  Training loss: 1.5909...  0.2021 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1774...  Training loss: 1.6071...  0.1696 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1775...  Training loss: 1.6022...  0.2272 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1776...  Training loss: 1.5468...  0.2404 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1777...  Training loss: 1.5629...  0.1930 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1778...  Training loss: 1.5836...  0.2870 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1779...  Training loss: 1.6032...  0.1751 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1780...  Training loss: 1.6047...  0.1679 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1781...  Training loss: 1.6018...  0.2367 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1782...  Training loss: 1.5909...  0.1608 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1783...  Training loss: 1.5903...  0.2015 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1784...  Training loss: 1.5679...  0.2353 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1785...  Training loss: 1.5870...  0.2433 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1786...  Training loss: 1.5614...  0.2580 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1787...  Training loss: 1.5966...  0.2492 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1788...  Training loss: 1.6021...  0.2324 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1789...  Training loss: 1.6015...  0.2373 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1790...  Training loss: 1.5598...  0.1965 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1791...  Training loss: 1.5771...  0.2011 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1792...  Training loss: 1.5376...  0.1771 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/30...  Training Step: 1793...  Training loss: 1.5974...  0.2430 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1794...  Training loss: 1.6172...  0.2035 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1795...  Training loss: 1.5715...  0.2758 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1796...  Training loss: 1.5503...  0.2516 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1797...  Training loss: 1.5429...  0.2929 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1798...  Training loss: 1.5628...  0.2576 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1799...  Training loss: 1.5699...  0.2861 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1800...  Training loss: 1.6261...  0.2283 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1801...  Training loss: 1.6161...  0.1795 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1802...  Training loss: 1.5819...  0.2460 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1803...  Training loss: 1.5760...  0.1898 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1804...  Training loss: 1.5823...  0.1628 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1805...  Training loss: 1.5703...  0.1890 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1806...  Training loss: 1.5555...  0.2047 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1807...  Training loss: 1.5801...  0.1906 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1808...  Training loss: 1.5535...  0.1556 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1809...  Training loss: 1.5909...  0.2286 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1810...  Training loss: 1.5680...  0.2646 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1811...  Training loss: 1.5587...  0.2681 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1812...  Training loss: 1.5468...  0.2560 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1813...  Training loss: 1.5629...  0.2660 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1814...  Training loss: 1.5920...  0.1848 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1815...  Training loss: 1.5903...  0.2585 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1816...  Training loss: 1.5682...  0.2618 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1817...  Training loss: 1.5467...  0.2711 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1818...  Training loss: 1.5902...  0.2537 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1819...  Training loss: 1.5416...  0.2437 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1820...  Training loss: 1.5782...  0.2286 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1821...  Training loss: 1.5461...  0.2429 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1822...  Training loss: 1.5694...  0.2693 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1823...  Training loss: 1.5792...  0.2641 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1824...  Training loss: 1.5989...  0.1814 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1825...  Training loss: 1.5530...  0.2275 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1826...  Training loss: 1.5525...  0.1542 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1827...  Training loss: 1.5739...  0.1592 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1828...  Training loss: 1.5771...  0.1781 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1829...  Training loss: 1.5778...  0.2093 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1830...  Training loss: 1.6091...  0.1884 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1831...  Training loss: 1.6290...  0.2380 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1832...  Training loss: 1.5395...  0.2385 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1833...  Training loss: 1.5529...  0.2426 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1834...  Training loss: 1.6005...  0.1791 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1835...  Training loss: 1.5757...  0.1896 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1836...  Training loss: 1.6149...  0.1536 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1837...  Training loss: 1.5489...  0.2142 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1838...  Training loss: 1.5548...  0.1768 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1839...  Training loss: 1.5610...  0.2443 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1840...  Training loss: 1.5680...  0.2020 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1841...  Training loss: 1.5643...  0.2193 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1842...  Training loss: 1.5697...  0.2350 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1843...  Training loss: 1.5616...  0.2618 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1844...  Training loss: 1.5531...  0.2147 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1845...  Training loss: 1.5519...  0.1999 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1846...  Training loss: 1.5167...  0.2052 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1847...  Training loss: 1.5858...  0.2696 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1848...  Training loss: 1.5650...  0.2449 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1849...  Training loss: 1.5646...  0.2586 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1850...  Training loss: 1.5499...  0.1895 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1851...  Training loss: 1.5820...  0.2524 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1852...  Training loss: 1.5837...  0.2713 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1853...  Training loss: 1.5657...  0.2307 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1854...  Training loss: 1.5500...  0.1725 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1855...  Training loss: 1.5381...  0.2586 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1856...  Training loss: 1.5256...  0.1765 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1857...  Training loss: 1.5539...  0.1602 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1858...  Training loss: 1.5666...  0.1543 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1859...  Training loss: 1.5539...  0.2000 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1860...  Training loss: 1.5812...  0.2758 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1861...  Training loss: 1.5614...  0.2684 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1862...  Training loss: 1.5909...  0.2553 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1863...  Training loss: 1.5715...  0.2605 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1864...  Training loss: 1.5507...  0.1831 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1865...  Training loss: 1.5368...  0.2534 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1866...  Training loss: 1.5400...  0.2650 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1867...  Training loss: 1.5517...  0.2663 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1868...  Training loss: 1.5622...  0.2089 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1869...  Training loss: 1.5718...  0.2513 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1870...  Training loss: 1.5384...  0.1782 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1871...  Training loss: 1.6415...  0.1684 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1872...  Training loss: 1.5415...  0.2569 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1873...  Training loss: 1.5480...  0.2183 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1874...  Training loss: 1.5865...  0.1630 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1875...  Training loss: 1.5707...  0.2002 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1876...  Training loss: 1.5853...  0.2623 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1877...  Training loss: 1.5697...  0.2776 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1878...  Training loss: 1.5487...  0.2508 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1879...  Training loss: 1.5138...  0.2669 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1880...  Training loss: 1.5254...  0.1684 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1881...  Training loss: 1.5148...  0.1713 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1882...  Training loss: 1.5407...  0.2163 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1883...  Training loss: 1.5445...  0.1859 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1884...  Training loss: 1.5426...  0.1661 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1885...  Training loss: 1.6031...  0.1975 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1886...  Training loss: 1.6083...  0.2132 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1887...  Training loss: 1.5338...  0.1873 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1888...  Training loss: 1.5644...  0.2275 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1889...  Training loss: 1.5475...  0.1919 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1890...  Training loss: 1.5504...  0.1917 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30...  Training Step: 1891...  Training loss: 1.5587...  0.2017 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1892...  Training loss: 1.5150...  0.3013 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1893...  Training loss: 1.5288...  0.2845 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1894...  Training loss: 1.5204...  0.2333 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1895...  Training loss: 1.5714...  0.2406 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1896...  Training loss: 1.5496...  0.3087 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1897...  Training loss: 1.5756...  0.2437 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1898...  Training loss: 1.5794...  0.2721 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1899...  Training loss: 1.5492...  0.1888 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1900...  Training loss: 1.5241...  0.1598 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1901...  Training loss: 1.5539...  0.1661 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1902...  Training loss: 1.5294...  0.2179 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1903...  Training loss: 1.5432...  0.1986 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1904...  Training loss: 1.5302...  0.1955 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1905...  Training loss: 1.5345...  0.2101 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1906...  Training loss: 1.5585...  0.2109 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1907...  Training loss: 1.5988...  0.2726 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1908...  Training loss: 1.5650...  0.2515 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1909...  Training loss: 1.5574...  0.2109 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1910...  Training loss: 1.5747...  0.2622 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1911...  Training loss: 1.5790...  0.2537 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1912...  Training loss: 1.5480...  0.2637 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1913...  Training loss: 1.5364...  0.2677 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1914...  Training loss: 1.5956...  0.2465 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1915...  Training loss: 1.5757...  0.3122 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1916...  Training loss: 1.5451...  0.2720 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1917...  Training loss: 1.5730...  0.1844 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1918...  Training loss: 1.5710...  0.2480 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1919...  Training loss: 1.5571...  0.2097 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1920...  Training loss: 1.5619...  0.2652 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1921...  Training loss: 1.6005...  0.2556 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1922...  Training loss: 1.5370...  0.1581 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1923...  Training loss: 1.5475...  0.2028 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1924...  Training loss: 1.5387...  0.2505 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1925...  Training loss: 1.5531...  0.1580 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1926...  Training loss: 1.5662...  0.1800 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1927...  Training loss: 1.5770...  0.1693 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1928...  Training loss: 1.5298...  0.2516 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1929...  Training loss: 1.5753...  0.1569 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1930...  Training loss: 1.5608...  0.2250 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1931...  Training loss: 1.5649...  0.3015 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1932...  Training loss: 1.5539...  0.2095 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1933...  Training loss: 1.5355...  0.1901 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1934...  Training loss: 1.5488...  0.1697 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1935...  Training loss: 1.5697...  0.1975 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1936...  Training loss: 1.5700...  0.1837 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1937...  Training loss: 1.5751...  0.2941 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1938...  Training loss: 1.5507...  0.2600 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1939...  Training loss: 1.5496...  0.2346 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1940...  Training loss: 1.5346...  0.2555 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1941...  Training loss: 1.5164...  0.2671 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1942...  Training loss: 1.5523...  0.1862 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1943...  Training loss: 1.5630...  0.1680 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1944...  Training loss: 1.5735...  0.1665 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1945...  Training loss: 1.5722...  0.1768 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1946...  Training loss: 1.5174...  0.1690 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1947...  Training loss: 1.5427...  0.2281 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1948...  Training loss: 1.5531...  0.1550 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1949...  Training loss: 1.5736...  0.2360 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1950...  Training loss: 1.5760...  0.2620 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1951...  Training loss: 1.5665...  0.2278 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1952...  Training loss: 1.5662...  0.1685 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1953...  Training loss: 1.5570...  0.1961 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1954...  Training loss: 1.5343...  0.2177 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1955...  Training loss: 1.5515...  0.1842 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1956...  Training loss: 1.5483...  0.2323 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1957...  Training loss: 1.5582...  0.2067 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1958...  Training loss: 1.5714...  0.1952 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1959...  Training loss: 1.5727...  0.1790 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1960...  Training loss: 1.5225...  0.2162 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1961...  Training loss: 1.5385...  0.1900 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1962...  Training loss: 1.5270...  0.1727 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1963...  Training loss: 1.5617...  0.1969 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1964...  Training loss: 1.5840...  0.2318 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1965...  Training loss: 1.5478...  0.2254 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1966...  Training loss: 1.5162...  0.2477 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1967...  Training loss: 1.5269...  0.2160 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1968...  Training loss: 1.5324...  0.2811 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1969...  Training loss: 1.5473...  0.2745 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1970...  Training loss: 1.5958...  0.2477 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1971...  Training loss: 1.5892...  0.2402 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1972...  Training loss: 1.5513...  0.1611 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1973...  Training loss: 1.5362...  0.1903 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1974...  Training loss: 1.5483...  0.2130 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1975...  Training loss: 1.5463...  0.1635 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1976...  Training loss: 1.5379...  0.1769 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1977...  Training loss: 1.5530...  0.1863 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1978...  Training loss: 1.5197...  0.2352 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1979...  Training loss: 1.5572...  0.1648 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1980...  Training loss: 1.5334...  0.1867 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1981...  Training loss: 1.5271...  0.2731 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1982...  Training loss: 1.5130...  0.3092 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1983...  Training loss: 1.5354...  0.1894 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1984...  Training loss: 1.5589...  0.1791 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1985...  Training loss: 1.5518...  0.1739 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1986...  Training loss: 1.5443...  0.2712 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1987...  Training loss: 1.5137...  0.2129 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1988...  Training loss: 1.5609...  0.2273 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30...  Training Step: 1989...  Training loss: 1.5240...  0.2606 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1990...  Training loss: 1.5480...  0.2535 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1991...  Training loss: 1.5240...  0.2017 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1992...  Training loss: 1.5358...  0.2351 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1993...  Training loss: 1.5521...  0.1957 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1994...  Training loss: 1.5764...  0.1557 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1995...  Training loss: 1.5186...  0.2588 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1996...  Training loss: 1.5269...  0.2325 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1997...  Training loss: 1.5429...  0.2561 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1998...  Training loss: 1.5528...  0.1751 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1999...  Training loss: 1.5439...  0.2815 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2000...  Training loss: 1.5802...  0.2402 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2001...  Training loss: 1.5936...  0.2581 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2002...  Training loss: 1.5097...  0.2147 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2003...  Training loss: 1.5149...  0.2152 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2004...  Training loss: 1.5632...  0.1852 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2005...  Training loss: 1.5517...  0.1976 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2006...  Training loss: 1.5839...  0.2167 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2007...  Training loss: 1.5183...  0.2225 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2008...  Training loss: 1.5236...  0.2534 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2009...  Training loss: 1.5352...  0.2537 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2010...  Training loss: 1.5396...  0.2671 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2011...  Training loss: 1.5308...  0.2353 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2012...  Training loss: 1.5416...  0.2624 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2013...  Training loss: 1.5298...  0.2705 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2014...  Training loss: 1.5375...  0.2353 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2015...  Training loss: 1.5256...  0.2229 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2016...  Training loss: 1.4964...  0.2373 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2017...  Training loss: 1.5582...  0.1580 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2018...  Training loss: 1.5359...  0.2056 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2019...  Training loss: 1.5296...  0.2639 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2020...  Training loss: 1.5295...  0.2976 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2021...  Training loss: 1.5345...  0.3034 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2022...  Training loss: 1.5631...  0.2415 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2023...  Training loss: 1.5355...  0.2573 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2024...  Training loss: 1.5187...  0.2128 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2025...  Training loss: 1.5072...  0.2334 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2026...  Training loss: 1.4950...  0.1925 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2027...  Training loss: 1.5202...  0.1690 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2028...  Training loss: 1.5459...  0.1726 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2029...  Training loss: 1.5169...  0.2702 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2030...  Training loss: 1.5478...  0.2225 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2031...  Training loss: 1.5246...  0.2169 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2032...  Training loss: 1.5524...  0.2344 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2033...  Training loss: 1.5380...  0.2203 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2034...  Training loss: 1.5233...  0.2154 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2035...  Training loss: 1.5099...  0.2565 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2036...  Training loss: 1.5164...  0.2473 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2037...  Training loss: 1.5375...  0.2612 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2038...  Training loss: 1.5239...  0.2448 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2039...  Training loss: 1.5366...  0.2316 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2040...  Training loss: 1.5181...  0.2575 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2041...  Training loss: 1.5967...  0.2431 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2042...  Training loss: 1.4969...  0.2496 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2043...  Training loss: 1.5079...  0.2342 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2044...  Training loss: 1.5551...  0.1725 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2045...  Training loss: 1.5380...  0.2106 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2046...  Training loss: 1.5737...  0.2616 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2047...  Training loss: 1.5391...  0.2043 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2048...  Training loss: 1.5225...  0.3220 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2049...  Training loss: 1.4924...  0.2419 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2050...  Training loss: 1.5031...  0.2289 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2051...  Training loss: 1.4924...  0.2098 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2052...  Training loss: 1.5219...  0.3148 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2053...  Training loss: 1.5140...  0.3048 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2054...  Training loss: 1.5159...  0.2288 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2055...  Training loss: 1.5790...  0.1791 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2056...  Training loss: 1.5680...  0.1628 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2057...  Training loss: 1.5102...  0.2497 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2058...  Training loss: 1.5384...  0.1767 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2059...  Training loss: 1.5258...  0.2380 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2060...  Training loss: 1.5263...  0.2806 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2061...  Training loss: 1.5219...  0.2675 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2062...  Training loss: 1.5001...  0.3096 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2063...  Training loss: 1.4934...  0.2134 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2064...  Training loss: 1.4928...  0.2193 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2065...  Training loss: 1.5516...  0.2359 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2066...  Training loss: 1.5226...  0.2098 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2067...  Training loss: 1.5468...  0.2529 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2068...  Training loss: 1.5448...  0.2286 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2069...  Training loss: 1.5247...  0.2986 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2070...  Training loss: 1.4838...  0.2660 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2071...  Training loss: 1.5188...  0.3054 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2072...  Training loss: 1.5111...  0.1573 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2073...  Training loss: 1.5104...  0.1680 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2074...  Training loss: 1.5023...  0.2271 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2075...  Training loss: 1.5025...  0.2403 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2076...  Training loss: 1.5197...  0.1806 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2077...  Training loss: 1.5644...  0.1735 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2078...  Training loss: 1.5397...  0.2575 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2079...  Training loss: 1.5205...  0.2514 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2080...  Training loss: 1.5419...  0.2479 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2081...  Training loss: 1.5358...  0.2281 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2082...  Training loss: 1.5230...  0.1955 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2083...  Training loss: 1.5024...  0.1594 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2084...  Training loss: 1.5648...  0.1846 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2085...  Training loss: 1.5538...  0.1894 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2086...  Training loss: 1.5177...  0.2044 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2087...  Training loss: 1.5457...  0.1793 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/30...  Training Step: 2088...  Training loss: 1.5483...  0.2532 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2089...  Training loss: 1.5276...  0.1976 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2090...  Training loss: 1.5351...  0.2258 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2091...  Training loss: 1.5862...  0.2459 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2092...  Training loss: 1.5060...  0.2803 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2093...  Training loss: 1.5060...  0.2652 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2094...  Training loss: 1.5061...  0.2289 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2095...  Training loss: 1.5218...  0.2524 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2096...  Training loss: 1.5409...  0.1886 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2097...  Training loss: 1.5292...  0.2158 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2098...  Training loss: 1.5006...  0.2182 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2099...  Training loss: 1.5477...  0.1775 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2100...  Training loss: 1.5321...  0.2608 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2101...  Training loss: 1.5441...  0.2117 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2102...  Training loss: 1.5321...  0.2479 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2103...  Training loss: 1.5010...  0.2379 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2104...  Training loss: 1.5281...  0.2121 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2105...  Training loss: 1.5414...  0.1908 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2106...  Training loss: 1.5322...  0.2752 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2107...  Training loss: 1.5482...  0.1786 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2108...  Training loss: 1.5239...  0.2220 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2109...  Training loss: 1.5199...  0.2306 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2110...  Training loss: 1.5188...  0.2367 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2111...  Training loss: 1.4907...  0.1681 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2112...  Training loss: 1.5268...  0.1987 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2113...  Training loss: 1.5319...  0.2741 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2114...  Training loss: 1.5494...  0.1926 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2115...  Training loss: 1.5494...  0.1849 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2116...  Training loss: 1.5009...  0.1632 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2117...  Training loss: 1.5196...  0.1717 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2118...  Training loss: 1.5289...  0.2059 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2119...  Training loss: 1.5415...  0.2713 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2120...  Training loss: 1.5563...  0.2775 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2121...  Training loss: 1.5492...  0.2015 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2122...  Training loss: 1.5318...  0.2283 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2123...  Training loss: 1.5305...  0.2760 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2124...  Training loss: 1.5188...  0.1889 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2125...  Training loss: 1.5246...  0.2042 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2126...  Training loss: 1.4989...  0.2927 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2127...  Training loss: 1.5363...  0.2652 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2128...  Training loss: 1.5417...  0.2741 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2129...  Training loss: 1.5546...  0.1615 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2130...  Training loss: 1.5113...  0.1812 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2131...  Training loss: 1.5139...  0.2010 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2132...  Training loss: 1.4942...  0.2632 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2133...  Training loss: 1.5346...  0.2947 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2134...  Training loss: 1.5542...  0.2002 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2135...  Training loss: 1.5168...  0.2706 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2136...  Training loss: 1.4844...  0.1663 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2137...  Training loss: 1.4901...  0.1553 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2138...  Training loss: 1.5151...  0.2463 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2139...  Training loss: 1.5177...  0.2795 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2140...  Training loss: 1.5542...  0.2506 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2141...  Training loss: 1.5624...  0.1680 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2142...  Training loss: 1.5209...  0.1652 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2143...  Training loss: 1.5212...  0.1564 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2144...  Training loss: 1.5226...  0.2806 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2145...  Training loss: 1.5194...  0.2740 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2146...  Training loss: 1.5070...  0.1907 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2147...  Training loss: 1.5159...  0.2006 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2148...  Training loss: 1.5003...  0.2119 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2149...  Training loss: 1.5386...  0.1918 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2150...  Training loss: 1.5083...  0.2198 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2151...  Training loss: 1.5004...  0.1545 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2152...  Training loss: 1.4868...  0.2630 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2153...  Training loss: 1.5069...  0.2587 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2154...  Training loss: 1.5319...  0.2560 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2155...  Training loss: 1.5269...  0.2277 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2156...  Training loss: 1.5258...  0.2594 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2157...  Training loss: 1.4892...  0.2352 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2158...  Training loss: 1.5282...  0.1951 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2159...  Training loss: 1.4979...  0.2374 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2160...  Training loss: 1.5338...  0.1892 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2161...  Training loss: 1.4874...  0.1623 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2162...  Training loss: 1.5159...  0.2100 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2163...  Training loss: 1.5174...  0.2347 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2164...  Training loss: 1.5471...  0.2249 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2165...  Training loss: 1.4988...  0.2487 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2166...  Training loss: 1.5056...  0.1934 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2167...  Training loss: 1.5280...  0.2768 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2168...  Training loss: 1.5269...  0.2177 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2169...  Training loss: 1.5219...  0.1549 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2170...  Training loss: 1.5514...  0.1708 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2171...  Training loss: 1.5626...  0.1561 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2172...  Training loss: 1.4729...  0.2300 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2173...  Training loss: 1.4968...  0.3179 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2174...  Training loss: 1.5438...  0.2393 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2175...  Training loss: 1.5142...  0.2361 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2176...  Training loss: 1.5606...  0.2917 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2177...  Training loss: 1.4993...  0.3095 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2178...  Training loss: 1.5006...  0.2029 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2179...  Training loss: 1.5088...  0.2071 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2180...  Training loss: 1.5202...  0.1570 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2181...  Training loss: 1.5024...  0.2291 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2182...  Training loss: 1.5100...  0.1655 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2183...  Training loss: 1.5057...  0.1943 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2184...  Training loss: 1.4997...  0.2613 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2185...  Training loss: 1.4997...  0.2936 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/30...  Training Step: 2186...  Training loss: 1.4677...  0.2402 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2187...  Training loss: 1.5278...  0.1871 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2188...  Training loss: 1.5211...  0.2959 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2189...  Training loss: 1.5115...  0.2295 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2190...  Training loss: 1.4970...  0.2079 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2191...  Training loss: 1.5188...  0.2626 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2192...  Training loss: 1.5297...  0.2117 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2193...  Training loss: 1.5093...  0.1543 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2194...  Training loss: 1.4940...  0.1703 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2195...  Training loss: 1.4932...  0.1973 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2196...  Training loss: 1.4777...  0.1765 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2197...  Training loss: 1.4968...  0.2117 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2198...  Training loss: 1.5218...  0.2318 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2199...  Training loss: 1.4990...  0.1893 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2200...  Training loss: 1.5218...  0.1632 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2201...  Training loss: 1.5110...  0.1628 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2202...  Training loss: 1.5316...  0.1598 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2203...  Training loss: 1.5175...  0.1628 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2204...  Training loss: 1.4925...  0.2537 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2205...  Training loss: 1.4792...  0.2598 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2206...  Training loss: 1.4945...  0.2434 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2207...  Training loss: 1.5033...  0.1603 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2208...  Training loss: 1.5087...  0.1703 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2209...  Training loss: 1.5151...  0.1559 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2210...  Training loss: 1.4913...  0.1739 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2211...  Training loss: 1.5816...  0.1905 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2212...  Training loss: 1.4703...  0.1599 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2213...  Training loss: 1.4837...  0.2553 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2214...  Training loss: 1.5279...  0.2537 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2215...  Training loss: 1.5276...  0.2566 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2216...  Training loss: 1.5498...  0.1927 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2217...  Training loss: 1.5046...  0.1615 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2218...  Training loss: 1.4925...  0.2333 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2219...  Training loss: 1.4721...  0.2912 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2220...  Training loss: 1.4711...  0.2866 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2221...  Training loss: 1.4740...  0.2957 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2222...  Training loss: 1.5059...  0.1618 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2223...  Training loss: 1.4885...  0.1878 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2224...  Training loss: 1.4915...  0.2126 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2225...  Training loss: 1.5508...  0.2250 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2226...  Training loss: 1.5503...  0.1621 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2227...  Training loss: 1.4807...  0.2350 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2228...  Training loss: 1.5049...  0.1645 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2229...  Training loss: 1.4938...  0.1773 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2230...  Training loss: 1.4956...  0.1740 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2231...  Training loss: 1.5049...  0.1605 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2232...  Training loss: 1.4598...  0.2232 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2233...  Training loss: 1.4725...  0.2086 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2234...  Training loss: 1.4649...  0.2218 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2235...  Training loss: 1.5207...  0.1729 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2236...  Training loss: 1.5098...  0.2688 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2237...  Training loss: 1.5233...  0.1799 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2238...  Training loss: 1.5186...  0.2558 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2239...  Training loss: 1.5008...  0.2104 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2240...  Training loss: 1.4703...  0.2586 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2241...  Training loss: 1.4953...  0.1626 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2242...  Training loss: 1.4895...  0.1777 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2243...  Training loss: 1.4945...  0.2221 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2244...  Training loss: 1.4800...  0.1965 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2245...  Training loss: 1.4792...  0.2386 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2246...  Training loss: 1.4994...  0.2662 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2247...  Training loss: 1.5405...  0.1730 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2248...  Training loss: 1.5167...  0.2255 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2249...  Training loss: 1.4980...  0.2026 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2250...  Training loss: 1.5141...  0.2577 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2251...  Training loss: 1.5152...  0.1899 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2252...  Training loss: 1.5030...  0.2343 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2253...  Training loss: 1.4727...  0.2405 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2254...  Training loss: 1.5400...  0.2920 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2255...  Training loss: 1.5271...  0.2109 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2256...  Training loss: 1.4940...  0.1776 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2257...  Training loss: 1.5237...  0.2500 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2258...  Training loss: 1.5278...  0.2463 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2259...  Training loss: 1.4946...  0.1711 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2260...  Training loss: 1.5093...  0.1873 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2261...  Training loss: 1.5631...  0.2336 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2262...  Training loss: 1.4877...  0.2295 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2263...  Training loss: 1.4886...  0.2234 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2264...  Training loss: 1.4847...  0.2675 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2265...  Training loss: 1.5060...  0.2528 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2266...  Training loss: 1.5110...  0.1852 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2267...  Training loss: 1.5141...  0.1850 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2268...  Training loss: 1.4812...  0.2447 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2269...  Training loss: 1.5223...  0.2397 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2270...  Training loss: 1.5062...  0.2038 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2271...  Training loss: 1.5126...  0.2831 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2272...  Training loss: 1.5034...  0.2746 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2273...  Training loss: 1.4822...  0.1840 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2274...  Training loss: 1.5009...  0.2570 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2275...  Training loss: 1.5258...  0.2399 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2276...  Training loss: 1.5188...  0.2240 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2277...  Training loss: 1.5248...  0.1967 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2278...  Training loss: 1.4973...  0.3227 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2279...  Training loss: 1.4917...  0.2762 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2280...  Training loss: 1.4915...  0.2963 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2281...  Training loss: 1.4614...  0.1532 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2282...  Training loss: 1.4933...  0.2468 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2283...  Training loss: 1.5104...  0.1980 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/30...  Training Step: 2284...  Training loss: 1.5348...  0.2692 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2285...  Training loss: 1.5247...  0.1838 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2286...  Training loss: 1.4645...  0.1873 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2287...  Training loss: 1.4803...  0.2339 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2288...  Training loss: 1.5100...  0.2619 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2289...  Training loss: 1.5155...  0.1892 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2290...  Training loss: 1.5274...  0.2430 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2291...  Training loss: 1.5186...  0.2784 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2292...  Training loss: 1.5113...  0.1851 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2293...  Training loss: 1.5120...  0.2205 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2294...  Training loss: 1.4860...  0.1726 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2295...  Training loss: 1.5064...  0.1581 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2296...  Training loss: 1.4856...  0.2130 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2297...  Training loss: 1.5045...  0.1783 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2298...  Training loss: 1.5195...  0.1596 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2299...  Training loss: 1.5167...  0.1674 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2300...  Training loss: 1.4865...  0.2606 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2301...  Training loss: 1.4907...  0.2627 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2302...  Training loss: 1.4848...  0.2477 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2303...  Training loss: 1.5082...  0.1877 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2304...  Training loss: 1.5316...  0.1608 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2305...  Training loss: 1.4866...  0.3268 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2306...  Training loss: 1.4585...  0.2362 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2307...  Training loss: 1.4623...  0.2084 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2308...  Training loss: 1.4895...  0.2626 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2309...  Training loss: 1.4930...  0.1803 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2310...  Training loss: 1.5345...  0.2401 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2311...  Training loss: 1.5328...  0.2681 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2312...  Training loss: 1.4969...  0.2885 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2313...  Training loss: 1.4950...  0.1589 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2314...  Training loss: 1.4947...  0.1711 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2315...  Training loss: 1.4884...  0.1979 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2316...  Training loss: 1.4868...  0.1710 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2317...  Training loss: 1.4953...  0.2639 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2318...  Training loss: 1.4716...  0.2080 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2319...  Training loss: 1.5090...  0.1716 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2320...  Training loss: 1.4772...  0.2705 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2321...  Training loss: 1.4801...  0.2868 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2322...  Training loss: 1.4578...  0.2721 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2323...  Training loss: 1.4832...  0.2228 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2324...  Training loss: 1.5137...  0.2271 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2325...  Training loss: 1.5100...  0.1547 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2326...  Training loss: 1.5021...  0.2233 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2327...  Training loss: 1.4621...  0.1981 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2328...  Training loss: 1.5000...  0.1939 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2329...  Training loss: 1.4765...  0.1832 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2330...  Training loss: 1.4999...  0.2324 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2331...  Training loss: 1.4746...  0.1647 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2332...  Training loss: 1.4832...  0.1882 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2333...  Training loss: 1.5023...  0.2625 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2334...  Training loss: 1.5304...  0.2689 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2335...  Training loss: 1.4683...  0.2138 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2336...  Training loss: 1.4801...  0.2182 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2337...  Training loss: 1.5020...  0.2740 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2338...  Training loss: 1.5037...  0.2888 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2339...  Training loss: 1.5037...  0.2526 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2340...  Training loss: 1.5315...  0.1625 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2341...  Training loss: 1.5469...  0.1784 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2342...  Training loss: 1.4589...  0.2641 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2343...  Training loss: 1.4662...  0.2699 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2344...  Training loss: 1.5219...  0.2034 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2345...  Training loss: 1.4920...  0.1596 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2346...  Training loss: 1.5293...  0.2454 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2347...  Training loss: 1.4733...  0.2884 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2348...  Training loss: 1.4744...  0.2634 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2349...  Training loss: 1.4783...  0.2445 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2350...  Training loss: 1.4939...  0.2628 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2351...  Training loss: 1.4769...  0.1905 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2352...  Training loss: 1.4941...  0.2189 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2353...  Training loss: 1.4783...  0.2289 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2354...  Training loss: 1.4796...  0.1685 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2355...  Training loss: 1.4795...  0.2495 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2356...  Training loss: 1.4473...  0.2398 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2357...  Training loss: 1.5091...  0.2495 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2358...  Training loss: 1.4921...  0.2613 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2359...  Training loss: 1.4905...  0.2510 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2360...  Training loss: 1.4705...  0.2598 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2361...  Training loss: 1.4841...  0.2328 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2362...  Training loss: 1.5132...  0.1576 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2363...  Training loss: 1.4837...  0.1579 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2364...  Training loss: 1.4744...  0.2062 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2365...  Training loss: 1.4614...  0.1748 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2366...  Training loss: 1.4529...  0.2342 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2367...  Training loss: 1.4728...  0.2833 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2368...  Training loss: 1.4849...  0.2317 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2369...  Training loss: 1.4743...  0.1891 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2370...  Training loss: 1.4999...  0.2312 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2371...  Training loss: 1.4724...  0.1916 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2372...  Training loss: 1.5015...  0.1570 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2373...  Training loss: 1.4841...  0.2096 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2374...  Training loss: 1.4713...  0.1607 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2375...  Training loss: 1.4624...  0.2598 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2376...  Training loss: 1.4759...  0.2194 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2377...  Training loss: 1.4734...  0.2488 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2378...  Training loss: 1.4892...  0.1998 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2379...  Training loss: 1.4982...  0.1915 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2380...  Training loss: 1.4716...  0.2414 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2381...  Training loss: 1.5571...  0.2001 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/30...  Training Step: 2382...  Training loss: 1.4531...  0.2511 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2383...  Training loss: 1.4649...  0.2265 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2384...  Training loss: 1.5084...  0.2237 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2385...  Training loss: 1.5011...  0.2492 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2386...  Training loss: 1.5124...  0.2409 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2387...  Training loss: 1.4827...  0.2577 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2388...  Training loss: 1.4851...  0.2236 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2389...  Training loss: 1.4556...  0.2620 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2390...  Training loss: 1.4586...  0.2688 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2391...  Training loss: 1.4510...  0.2335 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2392...  Training loss: 1.4749...  0.1828 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2393...  Training loss: 1.4664...  0.1630 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2394...  Training loss: 1.4607...  0.2641 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2395...  Training loss: 1.5212...  0.2001 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2396...  Training loss: 1.5346...  0.1605 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2397...  Training loss: 1.4654...  0.2198 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2398...  Training loss: 1.4787...  0.2106 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2399...  Training loss: 1.4713...  0.1662 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2400...  Training loss: 1.4737...  0.1743 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2401...  Training loss: 1.4892...  0.2719 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2402...  Training loss: 1.4415...  0.2645 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2403...  Training loss: 1.4539...  0.2577 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2404...  Training loss: 1.4398...  0.1704 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2405...  Training loss: 1.4931...  0.2184 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2406...  Training loss: 1.4786...  0.1849 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2407...  Training loss: 1.5111...  0.2603 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2408...  Training loss: 1.4908...  0.2273 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2409...  Training loss: 1.4786...  0.1971 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2410...  Training loss: 1.4441...  0.2564 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2411...  Training loss: 1.4831...  0.2228 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2412...  Training loss: 1.4580...  0.2396 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2413...  Training loss: 1.4758...  0.2427 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2414...  Training loss: 1.4594...  0.1827 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2415...  Training loss: 1.4663...  0.2838 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2416...  Training loss: 1.4730...  0.2318 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2417...  Training loss: 1.5177...  0.2656 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2418...  Training loss: 1.4915...  0.1599 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2419...  Training loss: 1.4735...  0.2780 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2420...  Training loss: 1.4986...  0.1697 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2421...  Training loss: 1.5026...  0.2161 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2422...  Training loss: 1.4735...  0.1752 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2423...  Training loss: 1.4615...  0.1632 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2424...  Training loss: 1.5271...  0.1649 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2425...  Training loss: 1.5108...  0.2803 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2426...  Training loss: 1.4829...  0.2413 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2427...  Training loss: 1.5021...  0.2664 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2428...  Training loss: 1.4984...  0.2650 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2429...  Training loss: 1.4930...  0.2715 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2430...  Training loss: 1.4910...  0.2693 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2431...  Training loss: 1.5323...  0.2451 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2432...  Training loss: 1.4694...  0.2140 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2433...  Training loss: 1.4672...  0.2625 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2434...  Training loss: 1.4649...  0.2921 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2435...  Training loss: 1.4853...  0.2022 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2436...  Training loss: 1.4936...  0.2766 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2437...  Training loss: 1.4872...  0.1789 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2438...  Training loss: 1.4617...  0.2470 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2439...  Training loss: 1.5070...  0.1893 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2440...  Training loss: 1.4952...  0.2490 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2441...  Training loss: 1.4853...  0.1925 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2442...  Training loss: 1.4861...  0.1696 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2443...  Training loss: 1.4598...  0.2395 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2444...  Training loss: 1.4845...  0.2155 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2445...  Training loss: 1.4940...  0.2194 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2446...  Training loss: 1.4853...  0.2534 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2447...  Training loss: 1.4991...  0.2283 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2448...  Training loss: 1.4739...  0.2295 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2449...  Training loss: 1.4697...  0.2372 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2450...  Training loss: 1.4676...  0.1674 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2451...  Training loss: 1.4491...  0.1575 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2452...  Training loss: 1.4734...  0.1768 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2453...  Training loss: 1.4834...  0.2075 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2454...  Training loss: 1.4903...  0.2387 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2455...  Training loss: 1.4952...  0.1908 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2456...  Training loss: 1.4452...  0.1937 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2457...  Training loss: 1.4636...  0.1799 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2458...  Training loss: 1.4875...  0.1673 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2459...  Training loss: 1.5095...  0.2171 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2460...  Training loss: 1.5029...  0.2358 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2461...  Training loss: 1.4945...  0.2551 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2462...  Training loss: 1.4945...  0.2598 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2463...  Training loss: 1.4889...  0.2408 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2464...  Training loss: 1.4619...  0.2500 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2465...  Training loss: 1.4871...  0.2345 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2466...  Training loss: 1.4669...  0.2287 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2467...  Training loss: 1.4854...  0.1668 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2468...  Training loss: 1.4951...  0.1612 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2469...  Training loss: 1.5010...  0.1890 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2470...  Training loss: 1.4646...  0.2316 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2471...  Training loss: 1.4641...  0.2235 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2472...  Training loss: 1.4509...  0.2601 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2473...  Training loss: 1.4885...  0.2306 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2474...  Training loss: 1.5046...  0.1865 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2475...  Training loss: 1.4681...  0.2563 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2476...  Training loss: 1.4441...  0.2782 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2477...  Training loss: 1.4510...  0.1810 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2478...  Training loss: 1.4671...  0.1834 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2479...  Training loss: 1.4739...  0.1577 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/30...  Training Step: 2480...  Training loss: 1.5155...  0.1789 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2481...  Training loss: 1.5203...  0.2294 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2482...  Training loss: 1.4935...  0.2303 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2483...  Training loss: 1.4777...  0.2335 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2484...  Training loss: 1.4736...  0.2309 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2485...  Training loss: 1.4686...  0.1892 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2486...  Training loss: 1.4681...  0.1993 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2487...  Training loss: 1.4768...  0.2674 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2488...  Training loss: 1.4548...  0.1726 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2489...  Training loss: 1.4936...  0.1699 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2490...  Training loss: 1.4564...  0.2465 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2491...  Training loss: 1.4642...  0.2516 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2492...  Training loss: 1.4362...  0.2134 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2493...  Training loss: 1.4638...  0.2344 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2494...  Training loss: 1.4854...  0.2805 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2495...  Training loss: 1.4961...  0.2075 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2496...  Training loss: 1.4797...  0.2189 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2497...  Training loss: 1.4528...  0.2068 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2498...  Training loss: 1.4775...  0.2395 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2499...  Training loss: 1.4641...  0.2627 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2500...  Training loss: 1.4817...  0.1868 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2501...  Training loss: 1.4484...  0.1838 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2502...  Training loss: 1.4696...  0.2181 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2503...  Training loss: 1.4759...  0.2383 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2504...  Training loss: 1.4972...  0.2394 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2505...  Training loss: 1.4560...  0.2525 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2506...  Training loss: 1.4548...  0.2161 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2507...  Training loss: 1.4819...  0.2180 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2508...  Training loss: 1.4722...  0.2871 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2509...  Training loss: 1.4855...  0.2670 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2510...  Training loss: 1.5053...  0.2709 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2511...  Training loss: 1.5122...  0.2563 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2512...  Training loss: 1.4356...  0.1610 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2513...  Training loss: 1.4442...  0.2629 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2514...  Training loss: 1.4965...  0.2672 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2515...  Training loss: 1.4874...  0.1856 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2516...  Training loss: 1.5026...  0.2351 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2517...  Training loss: 1.4575...  0.1836 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2518...  Training loss: 1.4556...  0.2409 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2519...  Training loss: 1.4701...  0.1818 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2520...  Training loss: 1.4808...  0.2115 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2521...  Training loss: 1.4654...  0.1599 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2522...  Training loss: 1.4686...  0.1580 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2523...  Training loss: 1.4598...  0.1591 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2524...  Training loss: 1.4502...  0.1711 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2525...  Training loss: 1.4601...  0.2219 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2526...  Training loss: 1.4262...  0.2178 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2527...  Training loss: 1.4869...  0.2269 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2528...  Training loss: 1.4695...  0.2215 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2529...  Training loss: 1.4710...  0.2808 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2530...  Training loss: 1.4464...  0.2548 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2531...  Training loss: 1.4752...  0.2751 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2532...  Training loss: 1.4944...  0.2508 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2533...  Training loss: 1.4546...  0.2189 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2534...  Training loss: 1.4478...  0.1549 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2535...  Training loss: 1.4582...  0.1934 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2536...  Training loss: 1.4341...  0.2075 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2537...  Training loss: 1.4554...  0.2065 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2538...  Training loss: 1.4692...  0.1587 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2539...  Training loss: 1.4537...  0.2174 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2540...  Training loss: 1.4834...  0.2280 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2541...  Training loss: 1.4606...  0.2404 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2542...  Training loss: 1.4928...  0.2470 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2543...  Training loss: 1.4666...  0.2093 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2544...  Training loss: 1.4556...  0.2291 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2545...  Training loss: 1.4527...  0.2512 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2546...  Training loss: 1.4607...  0.2461 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2547...  Training loss: 1.4719...  0.2155 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2548...  Training loss: 1.4670...  0.2145 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2549...  Training loss: 1.4736...  0.2077 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2550...  Training loss: 1.4579...  0.2361 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2551...  Training loss: 1.5427...  0.1616 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2552...  Training loss: 1.4393...  0.1915 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2553...  Training loss: 1.4592...  0.2425 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2554...  Training loss: 1.4942...  0.2773 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2555...  Training loss: 1.4800...  0.2389 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2556...  Training loss: 1.4912...  0.2378 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2557...  Training loss: 1.4764...  0.1832 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2558...  Training loss: 1.4577...  0.1860 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2559...  Training loss: 1.4319...  0.2578 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2560...  Training loss: 1.4335...  0.2658 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2561...  Training loss: 1.4463...  0.2260 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2562...  Training loss: 1.4612...  0.1933 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2563...  Training loss: 1.4480...  0.2010 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2564...  Training loss: 1.4520...  0.2271 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2565...  Training loss: 1.5100...  0.2317 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2566...  Training loss: 1.5031...  0.1620 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2567...  Training loss: 1.4432...  0.1638 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2568...  Training loss: 1.4662...  0.2444 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2569...  Training loss: 1.4625...  0.1830 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2570...  Training loss: 1.4517...  0.2380 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2571...  Training loss: 1.4665...  0.2117 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2572...  Training loss: 1.4260...  0.2312 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2573...  Training loss: 1.4287...  0.2397 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2574...  Training loss: 1.4188...  0.1923 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2575...  Training loss: 1.4867...  0.2424 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2576...  Training loss: 1.4617...  0.2676 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2577...  Training loss: 1.4804...  0.1681 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/30...  Training Step: 2578...  Training loss: 1.4805...  0.2208 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2579...  Training loss: 1.4549...  0.1809 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2580...  Training loss: 1.4301...  0.2078 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2581...  Training loss: 1.4569...  0.2071 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2582...  Training loss: 1.4442...  0.2007 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2583...  Training loss: 1.4598...  0.2233 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2584...  Training loss: 1.4386...  0.1767 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2585...  Training loss: 1.4437...  0.2486 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2586...  Training loss: 1.4564...  0.2319 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2587...  Training loss: 1.4955...  0.2415 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2588...  Training loss: 1.4811...  0.2508 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2589...  Training loss: 1.4550...  0.2090 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2590...  Training loss: 1.4822...  0.1957 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2591...  Training loss: 1.4797...  0.2132 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2592...  Training loss: 1.4505...  0.2370 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2593...  Training loss: 1.4384...  0.1750 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2594...  Training loss: 1.5036...  0.1769 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2595...  Training loss: 1.4946...  0.2653 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2596...  Training loss: 1.4502...  0.2318 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2597...  Training loss: 1.4826...  0.1960 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2598...  Training loss: 1.4752...  0.1729 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2599...  Training loss: 1.4673...  0.2685 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2600...  Training loss: 1.4682...  0.2619 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2601...  Training loss: 1.5099...  0.1933 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2602...  Training loss: 1.4458...  0.1918 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2603...  Training loss: 1.4498...  0.1987 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2604...  Training loss: 1.4536...  0.1883 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2605...  Training loss: 1.4635...  0.1963 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2606...  Training loss: 1.4708...  0.1700 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2607...  Training loss: 1.4665...  0.1787 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2608...  Training loss: 1.4423...  0.1650 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2609...  Training loss: 1.4843...  0.2380 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2610...  Training loss: 1.4775...  0.2538 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2611...  Training loss: 1.4744...  0.1778 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2612...  Training loss: 1.4611...  0.2098 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2613...  Training loss: 1.4325...  0.1980 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2614...  Training loss: 1.4582...  0.2496 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2615...  Training loss: 1.4821...  0.2386 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2616...  Training loss: 1.4775...  0.2784 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2617...  Training loss: 1.4932...  0.2761 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2618...  Training loss: 1.4652...  0.2956 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2619...  Training loss: 1.4480...  0.1968 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2620...  Training loss: 1.4553...  0.1981 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2621...  Training loss: 1.4327...  0.2560 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2622...  Training loss: 1.4535...  0.2031 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2623...  Training loss: 1.4641...  0.2316 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2624...  Training loss: 1.4908...  0.2618 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2625...  Training loss: 1.4843...  0.2678 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2626...  Training loss: 1.4255...  0.1627 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2627...  Training loss: 1.4498...  0.2310 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2628...  Training loss: 1.4753...  0.1748 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2629...  Training loss: 1.4886...  0.2183 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2630...  Training loss: 1.4835...  0.2543 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2631...  Training loss: 1.4836...  0.2049 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2632...  Training loss: 1.4651...  0.1983 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2633...  Training loss: 1.4567...  0.2225 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2634...  Training loss: 1.4487...  0.2039 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2635...  Training loss: 1.4569...  0.2254 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2636...  Training loss: 1.4498...  0.2346 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2637...  Training loss: 1.4580...  0.2228 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2638...  Training loss: 1.4800...  0.2237 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2639...  Training loss: 1.4771...  0.2070 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2640...  Training loss: 1.4429...  0.1774 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2641...  Training loss: 1.4430...  0.2229 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2642...  Training loss: 1.4351...  0.2452 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2643...  Training loss: 1.4687...  0.2557 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2644...  Training loss: 1.4914...  0.2190 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2645...  Training loss: 1.4526...  0.2051 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2646...  Training loss: 1.4228...  0.1983 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2647...  Training loss: 1.4375...  0.1815 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2648...  Training loss: 1.4463...  0.2360 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2649...  Training loss: 1.4601...  0.2417 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2650...  Training loss: 1.4978...  0.1827 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2651...  Training loss: 1.4980...  0.1991 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2652...  Training loss: 1.4626...  0.1659 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2653...  Training loss: 1.4560...  0.1596 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2654...  Training loss: 1.4623...  0.1920 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2655...  Training loss: 1.4497...  0.2190 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2656...  Training loss: 1.4538...  0.1539 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2657...  Training loss: 1.4585...  0.2145 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2658...  Training loss: 1.4414...  0.2587 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2659...  Training loss: 1.4721...  0.1767 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2660...  Training loss: 1.4470...  0.1607 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2661...  Training loss: 1.4327...  0.1558 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2662...  Training loss: 1.4199...  0.2185 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2663...  Training loss: 1.4368...  0.1613 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2664...  Training loss: 1.4660...  0.2829 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2665...  Training loss: 1.4663...  0.2336 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2666...  Training loss: 1.4656...  0.1975 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2667...  Training loss: 1.4272...  0.2530 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2668...  Training loss: 1.4651...  0.2008 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2669...  Training loss: 1.4394...  0.1979 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2670...  Training loss: 1.4542...  0.2470 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2671...  Training loss: 1.4359...  0.1917 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2672...  Training loss: 1.4561...  0.1688 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2673...  Training loss: 1.4581...  0.2552 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2674...  Training loss: 1.4835...  0.2474 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2675...  Training loss: 1.4293...  0.2540 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/30...  Training Step: 2676...  Training loss: 1.4425...  0.2015 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2677...  Training loss: 1.4571...  0.2012 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2678...  Training loss: 1.4602...  0.1883 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2679...  Training loss: 1.4649...  0.1662 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2680...  Training loss: 1.4889...  0.2281 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2681...  Training loss: 1.4970...  0.2684 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2682...  Training loss: 1.4249...  0.2506 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2683...  Training loss: 1.4302...  0.2199 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2684...  Training loss: 1.4837...  0.2130 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2685...  Training loss: 1.4511...  0.1697 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2686...  Training loss: 1.4879...  0.1657 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2687...  Training loss: 1.4304...  0.2276 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2688...  Training loss: 1.4443...  0.1999 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2689...  Training loss: 1.4474...  0.2245 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2690...  Training loss: 1.4533...  0.2114 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2691...  Training loss: 1.4376...  0.1632 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2692...  Training loss: 1.4509...  0.2203 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2693...  Training loss: 1.4532...  0.2397 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2694...  Training loss: 1.4418...  0.1735 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2695...  Training loss: 1.4366...  0.1908 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2696...  Training loss: 1.4058...  0.2291 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2697...  Training loss: 1.4662...  0.2100 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2698...  Training loss: 1.4532...  0.2395 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2699...  Training loss: 1.4493...  0.2324 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2700...  Training loss: 1.4322...  0.2319 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2701...  Training loss: 1.4564...  0.1646 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2702...  Training loss: 1.4784...  0.1891 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2703...  Training loss: 1.4466...  0.2114 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2704...  Training loss: 1.4356...  0.1839 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2705...  Training loss: 1.4323...  0.2009 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2706...  Training loss: 1.4154...  0.2049 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2707...  Training loss: 1.4457...  0.1701 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2708...  Training loss: 1.4562...  0.2187 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2709...  Training loss: 1.4291...  0.2069 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2710...  Training loss: 1.4564...  0.2278 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2711...  Training loss: 1.4436...  0.2611 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2712...  Training loss: 1.4826...  0.2650 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2713...  Training loss: 1.4605...  0.1883 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2714...  Training loss: 1.4387...  0.1580 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2715...  Training loss: 1.4313...  0.1649 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2716...  Training loss: 1.4363...  0.2471 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2717...  Training loss: 1.4499...  0.1856 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2718...  Training loss: 1.4465...  0.1945 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2719...  Training loss: 1.4517...  0.2168 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2720...  Training loss: 1.4369...  0.1959 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2721...  Training loss: 1.5146...  0.2427 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2722...  Training loss: 1.4243...  0.2553 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2723...  Training loss: 1.4333...  0.2155 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2724...  Training loss: 1.4777...  0.1597 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2725...  Training loss: 1.4586...  0.2364 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2726...  Training loss: 1.4824...  0.1898 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2727...  Training loss: 1.4459...  0.2128 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2728...  Training loss: 1.4343...  0.1897 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2729...  Training loss: 1.4133...  0.2673 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2730...  Training loss: 1.4197...  0.1710 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2731...  Training loss: 1.4033...  0.2083 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2732...  Training loss: 1.4433...  0.2694 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2733...  Training loss: 1.4236...  0.2576 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2734...  Training loss: 1.4366...  0.2277 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2735...  Training loss: 1.4944...  0.1897 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2736...  Training loss: 1.4852...  0.2845 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2737...  Training loss: 1.4287...  0.2146 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2738...  Training loss: 1.4382...  0.2422 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2739...  Training loss: 1.4461...  0.2503 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2740...  Training loss: 1.4400...  0.3066 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2741...  Training loss: 1.4529...  0.2503 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2742...  Training loss: 1.3972...  0.2158 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2743...  Training loss: 1.4129...  0.2369 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2744...  Training loss: 1.4073...  0.2306 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2745...  Training loss: 1.4531...  0.2474 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2746...  Training loss: 1.4413...  0.2432 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2747...  Training loss: 1.4641...  0.2339 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2748...  Training loss: 1.4717...  0.1876 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2749...  Training loss: 1.4390...  0.1676 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2750...  Training loss: 1.4060...  0.2467 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2751...  Training loss: 1.4413...  0.2743 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2752...  Training loss: 1.4370...  0.2356 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2753...  Training loss: 1.4374...  0.1535 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2754...  Training loss: 1.4216...  0.2344 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2755...  Training loss: 1.4224...  0.2761 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2756...  Training loss: 1.4397...  0.2238 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2757...  Training loss: 1.4727...  0.2483 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2758...  Training loss: 1.4509...  0.1698 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2759...  Training loss: 1.4418...  0.1769 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2760...  Training loss: 1.4569...  0.2471 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2761...  Training loss: 1.4588...  0.2451 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2762...  Training loss: 1.4536...  0.2377 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2763...  Training loss: 1.4197...  0.1800 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2764...  Training loss: 1.4954...  0.2744 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2765...  Training loss: 1.4781...  0.2670 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2766...  Training loss: 1.4406...  0.2096 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2767...  Training loss: 1.4595...  0.2132 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2768...  Training loss: 1.4625...  0.1778 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2769...  Training loss: 1.4446...  0.1896 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2770...  Training loss: 1.4389...  0.2543 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2771...  Training loss: 1.4863...  0.2426 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2772...  Training loss: 1.4309...  0.1910 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2773...  Training loss: 1.4295...  0.1796 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/30...  Training Step: 2774...  Training loss: 1.4278...  0.2417 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2775...  Training loss: 1.4514...  0.2593 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2776...  Training loss: 1.4654...  0.2266 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2777...  Training loss: 1.4515...  0.2270 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2778...  Training loss: 1.4179...  0.2022 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2779...  Training loss: 1.4642...  0.2007 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2780...  Training loss: 1.4579...  0.2643 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2781...  Training loss: 1.4464...  0.1935 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2782...  Training loss: 1.4319...  0.2011 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2783...  Training loss: 1.4266...  0.2335 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2784...  Training loss: 1.4428...  0.2175 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2785...  Training loss: 1.4614...  0.1846 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2786...  Training loss: 1.4507...  0.2356 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2787...  Training loss: 1.4620...  0.2009 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2788...  Training loss: 1.4414...  0.2291 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2789...  Training loss: 1.4303...  0.2660 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2790...  Training loss: 1.4375...  0.1844 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2791...  Training loss: 1.4137...  0.1559 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2792...  Training loss: 1.4326...  0.1688 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2793...  Training loss: 1.4435...  0.2495 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2794...  Training loss: 1.4674...  0.2655 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2795...  Training loss: 1.4596...  0.2261 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2796...  Training loss: 1.4148...  0.2304 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2797...  Training loss: 1.4367...  0.2093 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2798...  Training loss: 1.4547...  0.1867 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2799...  Training loss: 1.4664...  0.3131 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2800...  Training loss: 1.4668...  0.1974 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2801...  Training loss: 1.4687...  0.1572 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2802...  Training loss: 1.4658...  0.2507 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2803...  Training loss: 1.4531...  0.2423 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2804...  Training loss: 1.4277...  0.2302 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2805...  Training loss: 1.4436...  0.1850 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2806...  Training loss: 1.4309...  0.2425 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2807...  Training loss: 1.4458...  0.2463 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2808...  Training loss: 1.4735...  0.2602 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2809...  Training loss: 1.4680...  0.2678 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2810...  Training loss: 1.4215...  0.3060 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2811...  Training loss: 1.4322...  0.2379 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2812...  Training loss: 1.4151...  0.2218 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2813...  Training loss: 1.4524...  0.2071 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2814...  Training loss: 1.4685...  0.2752 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2815...  Training loss: 1.4374...  0.1572 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2816...  Training loss: 1.3987...  0.1595 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2817...  Training loss: 1.4144...  0.2841 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2818...  Training loss: 1.4289...  0.2265 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2819...  Training loss: 1.4376...  0.2614 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2820...  Training loss: 1.4852...  0.3013 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2821...  Training loss: 1.4717...  0.2649 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2822...  Training loss: 1.4482...  0.2948 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2823...  Training loss: 1.4413...  0.1912 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2824...  Training loss: 1.4382...  0.2868 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2825...  Training loss: 1.4317...  0.1688 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2826...  Training loss: 1.4369...  0.1569 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2827...  Training loss: 1.4476...  0.1873 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2828...  Training loss: 1.4228...  0.2360 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2829...  Training loss: 1.4565...  0.2874 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2830...  Training loss: 1.4277...  0.1594 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2831...  Training loss: 1.4185...  0.2471 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2832...  Training loss: 1.4112...  0.2243 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2833...  Training loss: 1.4321...  0.2772 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2834...  Training loss: 1.4471...  0.2223 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2835...  Training loss: 1.4519...  0.1566 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2836...  Training loss: 1.4403...  0.1691 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2837...  Training loss: 1.4069...  0.1857 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2838...  Training loss: 1.4394...  0.2355 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2839...  Training loss: 1.4268...  0.2136 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2840...  Training loss: 1.4509...  0.2598 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2841...  Training loss: 1.4174...  0.2786 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2842...  Training loss: 1.4378...  0.1559 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2843...  Training loss: 1.4338...  0.1968 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2844...  Training loss: 1.4623...  0.2121 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2845...  Training loss: 1.4115...  0.1749 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2846...  Training loss: 1.4174...  0.2565 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2847...  Training loss: 1.4476...  0.2433 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2848...  Training loss: 1.4393...  0.2333 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2849...  Training loss: 1.4471...  0.2304 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2850...  Training loss: 1.4697...  0.1659 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2851...  Training loss: 1.4724...  0.1762 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2852...  Training loss: 1.3925...  0.2200 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2853...  Training loss: 1.4065...  0.2564 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2854...  Training loss: 1.4764...  0.1876 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2855...  Training loss: 1.4367...  0.1657 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2856...  Training loss: 1.4724...  0.1846 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2857...  Training loss: 1.4142...  0.2830 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2858...  Training loss: 1.4303...  0.2053 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2859...  Training loss: 1.4320...  0.2116 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2860...  Training loss: 1.4399...  0.2635 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2861...  Training loss: 1.4334...  0.2151 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2862...  Training loss: 1.4273...  0.2158 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2863...  Training loss: 1.4237...  0.2658 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2864...  Training loss: 1.4228...  0.2726 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2865...  Training loss: 1.4274...  0.2045 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2866...  Training loss: 1.3981...  0.2002 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2867...  Training loss: 1.4614...  0.2335 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2868...  Training loss: 1.4359...  0.2115 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2869...  Training loss: 1.4305...  0.2750 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2870...  Training loss: 1.4247...  0.2688 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2871...  Training loss: 1.4446...  0.2065 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2872...  Training loss: 1.4627...  0.1631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/30...  Training Step: 2873...  Training loss: 1.4346...  0.2119 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2874...  Training loss: 1.4156...  0.1789 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2875...  Training loss: 1.4138...  0.2171 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2876...  Training loss: 1.3976...  0.2849 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2877...  Training loss: 1.4239...  0.1885 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2878...  Training loss: 1.4357...  0.2147 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2879...  Training loss: 1.4149...  0.1570 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2880...  Training loss: 1.4522...  0.2262 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2881...  Training loss: 1.4230...  0.2540 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2882...  Training loss: 1.4554...  0.2389 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2883...  Training loss: 1.4318...  0.1762 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2884...  Training loss: 1.4283...  0.1608 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2885...  Training loss: 1.4118...  0.1583 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2886...  Training loss: 1.4214...  0.2892 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2887...  Training loss: 1.4304...  0.1955 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2888...  Training loss: 1.4331...  0.2544 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2889...  Training loss: 1.4321...  0.2773 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2890...  Training loss: 1.4228...  0.2522 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2891...  Training loss: 1.5049...  0.1627 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2892...  Training loss: 1.3973...  0.2802 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2893...  Training loss: 1.4137...  0.2482 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2894...  Training loss: 1.4512...  0.1834 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2895...  Training loss: 1.4576...  0.2103 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2896...  Training loss: 1.4559...  0.2647 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2897...  Training loss: 1.4226...  0.1880 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2898...  Training loss: 1.4277...  0.2682 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2899...  Training loss: 1.3962...  0.2362 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2900...  Training loss: 1.3936...  0.2366 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2901...  Training loss: 1.3913...  0.1598 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2902...  Training loss: 1.4221...  0.2276 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2903...  Training loss: 1.4171...  0.2308 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2904...  Training loss: 1.4225...  0.1552 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2905...  Training loss: 1.4712...  0.1684 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2906...  Training loss: 1.4703...  0.1900 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2907...  Training loss: 1.4155...  0.1893 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2908...  Training loss: 1.4275...  0.1827 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2909...  Training loss: 1.4216...  0.2794 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2910...  Training loss: 1.4160...  0.2295 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2911...  Training loss: 1.4430...  0.2961 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2912...  Training loss: 1.3893...  0.1992 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2913...  Training loss: 1.4000...  0.2570 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2914...  Training loss: 1.4011...  0.1760 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2915...  Training loss: 1.4470...  0.3052 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2916...  Training loss: 1.4273...  0.2395 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2917...  Training loss: 1.4457...  0.2613 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2918...  Training loss: 1.4348...  0.2300 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2919...  Training loss: 1.4283...  0.1961 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2920...  Training loss: 1.3913...  0.1661 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2921...  Training loss: 1.4179...  0.2975 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2922...  Training loss: 1.4078...  0.2344 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2923...  Training loss: 1.4209...  0.2333 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2924...  Training loss: 1.4074...  0.2465 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2925...  Training loss: 1.4135...  0.2948 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2926...  Training loss: 1.4254...  0.2071 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2927...  Training loss: 1.4565...  0.2729 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2928...  Training loss: 1.4390...  0.2590 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2929...  Training loss: 1.4129...  0.1633 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2930...  Training loss: 1.4467...  0.1575 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2931...  Training loss: 1.4359...  0.2806 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2932...  Training loss: 1.4333...  0.2004 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2933...  Training loss: 1.4108...  0.1968 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2934...  Training loss: 1.4769...  0.2232 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2935...  Training loss: 1.4551...  0.2187 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2936...  Training loss: 1.4299...  0.1976 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2937...  Training loss: 1.4450...  0.1581 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2938...  Training loss: 1.4516...  0.1883 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2939...  Training loss: 1.4332...  0.2182 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2940...  Training loss: 1.4463...  0.1929 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2941...  Training loss: 1.4677...  0.2855 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2942...  Training loss: 1.4171...  0.2411 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2943...  Training loss: 1.4093...  0.1594 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2944...  Training loss: 1.4062...  0.2344 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2945...  Training loss: 1.4300...  0.2081 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2946...  Training loss: 1.4413...  0.2482 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2947...  Training loss: 1.4332...  0.2134 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2948...  Training loss: 1.4064...  0.2522 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2949...  Training loss: 1.4502...  0.2474 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2950...  Training loss: 1.4456...  0.2551 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2951...  Training loss: 1.4429...  0.2408 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2952...  Training loss: 1.4332...  0.2538 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2953...  Training loss: 1.4059...  0.1748 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2954...  Training loss: 1.4261...  0.2192 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2955...  Training loss: 1.4461...  0.1697 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2956...  Training loss: 1.4374...  0.1990 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2957...  Training loss: 1.4485...  0.1988 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2958...  Training loss: 1.4301...  0.2312 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2959...  Training loss: 1.4159...  0.2617 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2960...  Training loss: 1.4272...  0.2130 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2961...  Training loss: 1.3982...  0.2265 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2962...  Training loss: 1.4169...  0.1801 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2963...  Training loss: 1.4434...  0.1987 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2964...  Training loss: 1.4505...  0.2502 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2965...  Training loss: 1.4462...  0.2512 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2966...  Training loss: 1.4006...  0.2285 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2967...  Training loss: 1.4116...  0.1991 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2968...  Training loss: 1.4348...  0.2349 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2969...  Training loss: 1.4587...  0.2322 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2970...  Training loss: 1.4537...  0.2393 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/30...  Training Step: 2971...  Training loss: 1.4451...  0.2435 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2972...  Training loss: 1.4326...  0.1682 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2973...  Training loss: 1.4416...  0.1586 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2974...  Training loss: 1.4038...  0.1959 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2975...  Training loss: 1.4357...  0.2058 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2976...  Training loss: 1.4139...  0.1718 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2977...  Training loss: 1.4355...  0.2481 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2978...  Training loss: 1.4507...  0.2183 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2979...  Training loss: 1.4484...  0.2244 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2980...  Training loss: 1.4039...  0.1541 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2981...  Training loss: 1.4120...  0.2127 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2982...  Training loss: 1.4064...  0.1799 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2983...  Training loss: 1.4373...  0.1968 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2984...  Training loss: 1.4530...  0.1610 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2985...  Training loss: 1.4228...  0.2051 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2986...  Training loss: 1.3970...  0.2103 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2987...  Training loss: 1.4007...  0.2449 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2988...  Training loss: 1.4190...  0.2064 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2989...  Training loss: 1.4286...  0.1990 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2990...  Training loss: 1.4658...  0.1607 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2991...  Training loss: 1.4686...  0.1954 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2992...  Training loss: 1.4339...  0.2145 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2993...  Training loss: 1.4265...  0.2105 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2994...  Training loss: 1.4253...  0.1697 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2995...  Training loss: 1.4209...  0.2374 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2996...  Training loss: 1.4164...  0.2003 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2997...  Training loss: 1.4243...  0.1989 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2998...  Training loss: 1.4028...  0.1690 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2999...  Training loss: 1.4422...  0.1956 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3000...  Training loss: 1.4152...  0.1574 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3001...  Training loss: 1.4161...  0.1684 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3002...  Training loss: 1.3854...  0.2547 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3003...  Training loss: 1.4194...  0.1858 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3004...  Training loss: 1.4301...  0.2233 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3005...  Training loss: 1.4497...  0.2546 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3006...  Training loss: 1.4318...  0.2637 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3007...  Training loss: 1.3961...  0.2465 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3008...  Training loss: 1.4348...  0.2134 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3009...  Training loss: 1.4115...  0.2186 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3010...  Training loss: 1.4343...  0.1795 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3011...  Training loss: 1.4198...  0.1900 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3012...  Training loss: 1.4294...  0.2044 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3013...  Training loss: 1.4228...  0.2399 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3014...  Training loss: 1.4564...  0.1959 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3015...  Training loss: 1.3965...  0.1634 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3016...  Training loss: 1.4129...  0.1859 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3017...  Training loss: 1.4307...  0.2595 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3018...  Training loss: 1.4223...  0.2136 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3019...  Training loss: 1.4277...  0.3031 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3020...  Training loss: 1.4555...  0.1594 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3021...  Training loss: 1.4640...  0.2382 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3022...  Training loss: 1.3824...  0.2882 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3023...  Training loss: 1.3915...  0.1811 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3024...  Training loss: 1.4468...  0.1810 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3025...  Training loss: 1.4286...  0.2195 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3026...  Training loss: 1.4524...  0.2478 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3027...  Training loss: 1.3913...  0.2646 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3028...  Training loss: 1.4097...  0.2617 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3029...  Training loss: 1.4136...  0.1713 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3030...  Training loss: 1.4232...  0.1571 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3031...  Training loss: 1.4110...  0.2692 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3032...  Training loss: 1.4172...  0.3002 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3033...  Training loss: 1.4164...  0.1553 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3034...  Training loss: 1.4020...  0.2261 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3035...  Training loss: 1.4065...  0.2539 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3036...  Training loss: 1.3822...  0.2354 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "# Сохраняться каждый N итераций\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Можно раскомментировать строчку ниже и продолжить обучение с checkpoint'а\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Обучаем сеть\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"Гостиная Анны Павловны начала понемногу наполняться.\"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1000_l512.ckpt\n",
      "Гостиная Анны Павловны начала понемногу наполняться.\n",
      "\n",
      "Семей придужалось в песелица и своех серделовости с теперь не выславится и высольным не выпоснимаясь, и она была одной совершенно вы сказала ничего она сказал на совериенностью в доменной слишкого слиданным полежалим, от намило и пать и ответа и с как было понимал, он не все помочула и, привав им в постерной. Она вишал настольно престили его продотные волости, к тому которые он странное поледу, что предстание оточно столо вставался на том, что она ничестна не мыгатеть. Она все стала оставал вся себя верово не межно и непрастние свои поднавшие стало. Он получал совели встому, что он не случно непись на подошло его не могла. Она было слешить ни слывали, которое приведить ей, на теперь своима и придалась из к него с своим с нимименния, подомнила она сона понавалась своюго стерным, кордаты, как начто беселие возно все друго и в думой весловие все все весело, ни он приедела, крогно он слежил он, в словой и ни перевилесь в себе не в себе. Не сказалась, что она не не мела провет таком по вроме не настала его и не малось.\n",
      "\n",
      "– Ах кажатей, что они в дем скузнеть во можедет нестовот просто и ни могла вы видеть?\n",
      "\n",
      "– Я собе поставаю такого, что он, возновности совершанно в котой вы сказное выражание с негому по собором несколько по вредной, потраловаясь с ним в совойствениим и отначение в слада, и совершение, и все него, что она страствая своей новое предверение в короской, как будто осторнитлинось в своего совсем и потому красным ображилась и столо, когда он ничего не положела, и в собесне в сонные приставаешь себя свое обежалос и не стала предслужил и от перимого и не словилась еми не ни поледном, что предтеравие ее постала стольно, что она стелать слизами, колова они с она в свера все своим возмоление.\n",
      "\n",
      "«Ну, что ян вы придужеси, как бы не могла без него думате изменить, как яспоситье не простать том? – сказала она, но не следал ее, потамая себя восмитилая она из не мого не сказать труго, как не посметрелая на возбрестение, обращенния не стараясь в помнути полать ей перебить, к\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab))\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
