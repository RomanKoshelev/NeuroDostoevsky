{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tolstoy\n",
    "https://habrahabr.ru/post/342738/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ЧАСТЬ ПЕРВАЯ\\n\\n\\n\\nI\\n\\nВсе счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99,  77,  93,  94, 102,   1,  91,  82,  92,  79,  77, 105,   0,\n",
       "         0,   0,   0,  30,   0,   0,  79, 123, 111,   1, 123, 129, 106,\n",
       "       123, 124, 117, 114, 108, 133, 111,   1, 123, 111, 118, 134, 114,\n",
       "         1, 121, 120, 127, 120, 112, 114,   1, 110, 122, 125, 109,   1,\n",
       "       119, 106,   1, 110, 122, 125, 109, 106,   7,   1, 116, 106, 112,\n",
       "       110, 106, 137,   1, 119, 111, 123, 129, 106, 123, 124, 117, 114,\n",
       "       108, 106, 137,   1, 123, 111, 118, 134, 137,   1, 119, 111, 123,\n",
       "       129, 106, 123, 124, 117, 114, 108, 106,   1, 121, 120,   8, 123,\n",
       "       108, 120, 111, 118, 125,   9], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Создаем генератор, который возвращает пакеты размером\n",
    "       n_seqs x n_steps из массива arr.\n",
    "       \n",
    "       Аргументы\n",
    "       ---------\n",
    "       arr: Массив, из которого получаем пакеты\n",
    "       n_seqs: Batch size, количество последовательностей в пакете\n",
    "       n_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "    '''\n",
    "    # Считаем количество символов на пакет и количество пакетов, которое можем сформировать\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Сохраняем в массиве только символы, которые позволяют сформировать целое число пакетов\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Делаем reshape 1D -> 2D, используя n_seqs как число строк, как на картинке\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # пакет данных, который будет подаваться на вход сети\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # целевой пакет, с которым будем сравнивать предсказание, получаем сдвиганием \"x\" на один символ вперед\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 99  77  93  94 102]\n",
      " [  1 110 108 114 112]\n",
      " [ 79 120 124   1 120]\n",
      " [114 119   1 109 120]\n",
      " [123 121 122 106 108]]\n",
      "\n",
      "y\n",
      " [[ 77  93  94 102   1]\n",
      " [110 108 114 112 111]\n",
      " [120 124   1 120 124]\n",
      " [119   1 109 120 108]\n",
      " [121 122 106 108 111]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:5, :5])\n",
    "print('\\ny\\n', y[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Определяем placeholder'ы для входных, целевых данных, а также вероятности drop out\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        batch_size: Batch size, количество последовательностей в пакете\n",
    "        num_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
    "        \n",
    "    '''\n",
    "    # Объявляем placeholder'ы\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Placeholder для вероятности drop out\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Строим LSTM ячейку.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        keep_prob: Скаляр (tf.placeholder) для dropout keep probability\n",
    "        lstm_size: Размер скрытых слоев в LSTM ячейках\n",
    "        num_layers: Количество LSTM слоев\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Строим LSTM ячейку\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Начинаем с базовой LSTM ячейки\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Добавляем dropout к ячейке\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    # Инициализируем начальное состояние LTSM ячейки\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Строим softmax слой и возвращаем результат его работы.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        \n",
    "        x: Входящий от LSTM тензор\n",
    "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
    "        out_size: Размер softmax слоя (объем словаря)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Соединяем результат LTSM слоев с softmax слоем\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Считаем logit-функцию\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    # Используем функцию softmax для получения предсказания\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Считаем функцию потери на основании значений logit-функции и целевых значений.\n",
    "    \n",
    "        Аргументы\n",
    "        ---------\n",
    "        logits: значение logit-функции\n",
    "        targets: целевые значения, с которыми сравниваем предсказания\n",
    "        lstm_size: Количество юнитов в LSTM слое\n",
    "        num_classes: Количество классов в целевых значениях (размер словаря)\n",
    "        \n",
    "    '''\n",
    "    # Делаем one-hot кодирование целевых значений и решейпим по образу и подобию logits\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Считаем значение функции потери softmax cross entropy loss и возвращаем среднее значение\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Строим оптимизатор для обучения, используя обрезку градиента.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: значение функции потери\n",
    "        learning_rate: параметр скорости обучения\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Оптимизатор для обучения, обрезка градиента для контроля \"взрывающихся\" градиентов\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # Мы будем использовать эту же сеть для сэмплирования (генерации текста),\n",
    "        # при этом будем подавать по одному символу за один раз\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Получаем input placeholder'ы\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Строим LSTM ячейку\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Прогоняем данные через RNN слои\n",
    "        # Делаем one-hot кодирование входящих данных\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Прогоняем данные через RNN и собираем результаты\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Получаем предсказания (softmax) и результат logit-функции\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Считаем потери и оптимизируем (с обрезкой градиента)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        # Размер пакета\n",
    "num_steps = 100         # Шагов в пакете\n",
    "lstm_size = 512         # Количество LSTM юнитов в скрытом слое\n",
    "num_layers = 2          # Количество LSTM слоев\n",
    "learning_rate = 0.001   # Скорость обучения\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30...  Training Step: 1...  Training loss: 4.9429...  0.3129 sec/batch\n",
      "Epoch: 1/30...  Training Step: 2...  Training loss: 4.8686...  0.3276 sec/batch\n",
      "Epoch: 1/30...  Training Step: 3...  Training loss: 4.4622...  0.2294 sec/batch\n",
      "Epoch: 1/30...  Training Step: 4...  Training loss: 5.0477...  0.2836 sec/batch\n",
      "Epoch: 1/30...  Training Step: 5...  Training loss: 4.1799...  0.2826 sec/batch\n",
      "Epoch: 1/30...  Training Step: 6...  Training loss: 3.9392...  0.1968 sec/batch\n",
      "Epoch: 1/30...  Training Step: 7...  Training loss: 3.9166...  0.1785 sec/batch\n",
      "Epoch: 1/30...  Training Step: 8...  Training loss: 3.7942...  0.2283 sec/batch\n",
      "Epoch: 1/30...  Training Step: 9...  Training loss: 3.7031...  0.1859 sec/batch\n",
      "Epoch: 1/30...  Training Step: 10...  Training loss: 3.6427...  0.1794 sec/batch\n",
      "Epoch: 1/30...  Training Step: 11...  Training loss: 3.6076...  0.2558 sec/batch\n",
      "Epoch: 1/30...  Training Step: 12...  Training loss: 3.5756...  0.2708 sec/batch\n",
      "Epoch: 1/30...  Training Step: 13...  Training loss: 3.5357...  0.2758 sec/batch\n",
      "Epoch: 1/30...  Training Step: 14...  Training loss: 3.5248...  0.2818 sec/batch\n",
      "Epoch: 1/30...  Training Step: 15...  Training loss: 3.5207...  0.1991 sec/batch\n",
      "Epoch: 1/30...  Training Step: 16...  Training loss: 3.5343...  0.1881 sec/batch\n",
      "Epoch: 1/30...  Training Step: 17...  Training loss: 3.4849...  0.2055 sec/batch\n",
      "Epoch: 1/30...  Training Step: 18...  Training loss: 3.4498...  0.2820 sec/batch\n",
      "Epoch: 1/30...  Training Step: 19...  Training loss: 3.4461...  0.3582 sec/batch\n",
      "Epoch: 1/30...  Training Step: 20...  Training loss: 3.4385...  0.2164 sec/batch\n",
      "Epoch: 1/30...  Training Step: 21...  Training loss: 3.4448...  0.3032 sec/batch\n",
      "Epoch: 1/30...  Training Step: 22...  Training loss: 3.4457...  0.2184 sec/batch\n",
      "Epoch: 1/30...  Training Step: 23...  Training loss: 3.4116...  0.2032 sec/batch\n",
      "Epoch: 1/30...  Training Step: 24...  Training loss: 3.4083...  0.1965 sec/batch\n",
      "Epoch: 1/30...  Training Step: 25...  Training loss: 3.4652...  0.1745 sec/batch\n",
      "Epoch: 1/30...  Training Step: 26...  Training loss: 3.4226...  0.2478 sec/batch\n",
      "Epoch: 1/30...  Training Step: 27...  Training loss: 3.4222...  0.2574 sec/batch\n",
      "Epoch: 1/30...  Training Step: 28...  Training loss: 3.3912...  0.1729 sec/batch\n",
      "Epoch: 1/30...  Training Step: 29...  Training loss: 3.3752...  0.1921 sec/batch\n",
      "Epoch: 1/30...  Training Step: 30...  Training loss: 3.3937...  0.2685 sec/batch\n",
      "Epoch: 1/30...  Training Step: 31...  Training loss: 3.4079...  0.2232 sec/batch\n",
      "Epoch: 1/30...  Training Step: 32...  Training loss: 3.3949...  0.1618 sec/batch\n",
      "Epoch: 1/30...  Training Step: 33...  Training loss: 3.3891...  0.1939 sec/batch\n",
      "Epoch: 1/30...  Training Step: 34...  Training loss: 3.3547...  0.1730 sec/batch\n",
      "Epoch: 1/30...  Training Step: 35...  Training loss: 3.3968...  0.2004 sec/batch\n",
      "Epoch: 1/30...  Training Step: 36...  Training loss: 3.3961...  0.2690 sec/batch\n",
      "Epoch: 1/30...  Training Step: 37...  Training loss: 3.3916...  0.2397 sec/batch\n",
      "Epoch: 1/30...  Training Step: 38...  Training loss: 3.3850...  0.2376 sec/batch\n",
      "Epoch: 1/30...  Training Step: 39...  Training loss: 3.3735...  0.2977 sec/batch\n",
      "Epoch: 1/30...  Training Step: 40...  Training loss: 3.3397...  0.2716 sec/batch\n",
      "Epoch: 1/30...  Training Step: 41...  Training loss: 3.3408...  0.2658 sec/batch\n",
      "Epoch: 1/30...  Training Step: 42...  Training loss: 3.3752...  0.2773 sec/batch\n",
      "Epoch: 1/30...  Training Step: 43...  Training loss: 3.3516...  0.2746 sec/batch\n",
      "Epoch: 1/30...  Training Step: 44...  Training loss: 3.3670...  0.2186 sec/batch\n",
      "Epoch: 1/30...  Training Step: 45...  Training loss: 3.3602...  0.2044 sec/batch\n",
      "Epoch: 1/30...  Training Step: 46...  Training loss: 3.3584...  0.2394 sec/batch\n",
      "Epoch: 1/30...  Training Step: 47...  Training loss: 3.3314...  0.2898 sec/batch\n",
      "Epoch: 1/30...  Training Step: 48...  Training loss: 3.3323...  0.2440 sec/batch\n",
      "Epoch: 1/30...  Training Step: 49...  Training loss: 3.3509...  0.1571 sec/batch\n",
      "Epoch: 1/30...  Training Step: 50...  Training loss: 3.3179...  0.2390 sec/batch\n",
      "Epoch: 1/30...  Training Step: 51...  Training loss: 3.3154...  0.1903 sec/batch\n",
      "Epoch: 1/30...  Training Step: 52...  Training loss: 3.3232...  0.2344 sec/batch\n",
      "Epoch: 1/30...  Training Step: 53...  Training loss: 3.3081...  0.1925 sec/batch\n",
      "Epoch: 1/30...  Training Step: 54...  Training loss: 3.3022...  0.1798 sec/batch\n",
      "Epoch: 1/30...  Training Step: 55...  Training loss: 3.3164...  0.2713 sec/batch\n",
      "Epoch: 1/30...  Training Step: 56...  Training loss: 3.3125...  0.2021 sec/batch\n",
      "Epoch: 1/30...  Training Step: 57...  Training loss: 3.3118...  0.2261 sec/batch\n",
      "Epoch: 1/30...  Training Step: 58...  Training loss: 3.3276...  0.2181 sec/batch\n",
      "Epoch: 1/30...  Training Step: 59...  Training loss: 3.3166...  0.2635 sec/batch\n",
      "Epoch: 1/30...  Training Step: 60...  Training loss: 3.3329...  0.2599 sec/batch\n",
      "Epoch: 1/30...  Training Step: 61...  Training loss: 3.3074...  0.2012 sec/batch\n",
      "Epoch: 1/30...  Training Step: 62...  Training loss: 3.3115...  0.2012 sec/batch\n",
      "Epoch: 1/30...  Training Step: 63...  Training loss: 3.2902...  0.2111 sec/batch\n",
      "Epoch: 1/30...  Training Step: 64...  Training loss: 3.3217...  0.1689 sec/batch\n",
      "Epoch: 1/30...  Training Step: 65...  Training loss: 3.3322...  0.1975 sec/batch\n",
      "Epoch: 1/30...  Training Step: 66...  Training loss: 3.3192...  0.1672 sec/batch\n",
      "Epoch: 1/30...  Training Step: 67...  Training loss: 3.3020...  0.1765 sec/batch\n",
      "Epoch: 1/30...  Training Step: 68...  Training loss: 3.3085...  0.2002 sec/batch\n",
      "Epoch: 1/30...  Training Step: 69...  Training loss: 3.2822...  0.2834 sec/batch\n",
      "Epoch: 1/30...  Training Step: 70...  Training loss: 3.3000...  0.3013 sec/batch\n",
      "Epoch: 1/30...  Training Step: 71...  Training loss: 3.3045...  0.2626 sec/batch\n",
      "Epoch: 1/30...  Training Step: 72...  Training loss: 3.3089...  0.2106 sec/batch\n",
      "Epoch: 1/30...  Training Step: 73...  Training loss: 3.2943...  0.1983 sec/batch\n",
      "Epoch: 1/30...  Training Step: 74...  Training loss: 3.2960...  0.1711 sec/batch\n",
      "Epoch: 1/30...  Training Step: 75...  Training loss: 3.2780...  0.1957 sec/batch\n",
      "Epoch: 1/30...  Training Step: 76...  Training loss: 3.3042...  0.3045 sec/batch\n",
      "Epoch: 1/30...  Training Step: 77...  Training loss: 3.2931...  0.2978 sec/batch\n",
      "Epoch: 1/30...  Training Step: 78...  Training loss: 3.2693...  0.2369 sec/batch\n",
      "Epoch: 1/30...  Training Step: 79...  Training loss: 3.3109...  0.2127 sec/batch\n",
      "Epoch: 1/30...  Training Step: 80...  Training loss: 3.3028...  0.2232 sec/batch\n",
      "Epoch: 1/30...  Training Step: 81...  Training loss: 3.2746...  0.1935 sec/batch\n",
      "Epoch: 1/30...  Training Step: 82...  Training loss: 3.2679...  0.1671 sec/batch\n",
      "Epoch: 1/30...  Training Step: 83...  Training loss: 3.2747...  0.1628 sec/batch\n",
      "Epoch: 1/30...  Training Step: 84...  Training loss: 3.3275...  0.2425 sec/batch\n",
      "Epoch: 1/30...  Training Step: 85...  Training loss: 3.2988...  0.2274 sec/batch\n",
      "Epoch: 1/30...  Training Step: 86...  Training loss: 3.2919...  0.2369 sec/batch\n",
      "Epoch: 1/30...  Training Step: 87...  Training loss: 3.2746...  0.2908 sec/batch\n",
      "Epoch: 1/30...  Training Step: 88...  Training loss: 3.2743...  0.2449 sec/batch\n",
      "Epoch: 1/30...  Training Step: 89...  Training loss: 3.2746...  0.2530 sec/batch\n",
      "Epoch: 1/30...  Training Step: 90...  Training loss: 3.2442...  0.2088 sec/batch\n",
      "Epoch: 1/30...  Training Step: 91...  Training loss: 3.2393...  0.2774 sec/batch\n",
      "Epoch: 1/30...  Training Step: 92...  Training loss: 3.2504...  0.2313 sec/batch\n",
      "Epoch: 1/30...  Training Step: 93...  Training loss: 3.2554...  0.2018 sec/batch\n",
      "Epoch: 1/30...  Training Step: 94...  Training loss: 3.2492...  0.2326 sec/batch\n",
      "Epoch: 1/30...  Training Step: 95...  Training loss: 3.2584...  0.1792 sec/batch\n",
      "Epoch: 1/30...  Training Step: 96...  Training loss: 3.2205...  0.2468 sec/batch\n",
      "Epoch: 1/30...  Training Step: 97...  Training loss: 3.2195...  0.2207 sec/batch\n",
      "Epoch: 1/30...  Training Step: 98...  Training loss: 3.2326...  0.1899 sec/batch\n",
      "Epoch: 1/30...  Training Step: 99...  Training loss: 3.2235...  0.2490 sec/batch\n",
      "Epoch: 1/30...  Training Step: 100...  Training loss: 3.2277...  0.2567 sec/batch\n",
      "Epoch: 1/30...  Training Step: 101...  Training loss: 3.2394...  0.2215 sec/batch\n",
      "Epoch: 1/30...  Training Step: 102...  Training loss: 3.5148...  0.1766 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30...  Training Step: 103...  Training loss: 3.8288...  0.2437 sec/batch\n",
      "Epoch: 1/30...  Training Step: 104...  Training loss: 3.7700...  0.1580 sec/batch\n",
      "Epoch: 1/30...  Training Step: 105...  Training loss: 3.6463...  0.2126 sec/batch\n",
      "Epoch: 1/30...  Training Step: 106...  Training loss: 3.5880...  0.1955 sec/batch\n",
      "Epoch: 1/30...  Training Step: 107...  Training loss: 3.4745...  0.1558 sec/batch\n",
      "Epoch: 1/30...  Training Step: 108...  Training loss: 3.3973...  0.2311 sec/batch\n",
      "Epoch: 1/30...  Training Step: 109...  Training loss: 3.2636...  0.1685 sec/batch\n",
      "Epoch: 1/30...  Training Step: 110...  Training loss: 3.2536...  0.2097 sec/batch\n",
      "Epoch: 1/30...  Training Step: 111...  Training loss: 3.2231...  0.1792 sec/batch\n",
      "Epoch: 1/30...  Training Step: 112...  Training loss: 3.2347...  0.2429 sec/batch\n",
      "Epoch: 1/30...  Training Step: 113...  Training loss: 3.2414...  0.2803 sec/batch\n",
      "Epoch: 1/30...  Training Step: 114...  Training loss: 3.2465...  0.2693 sec/batch\n",
      "Epoch: 1/30...  Training Step: 115...  Training loss: 3.2435...  0.2496 sec/batch\n",
      "Epoch: 1/30...  Training Step: 116...  Training loss: 3.2291...  0.2409 sec/batch\n",
      "Epoch: 1/30...  Training Step: 117...  Training loss: 3.1935...  0.2367 sec/batch\n",
      "Epoch: 1/30...  Training Step: 118...  Training loss: 3.2121...  0.2207 sec/batch\n",
      "Epoch: 1/30...  Training Step: 119...  Training loss: 3.1833...  0.2569 sec/batch\n",
      "Epoch: 1/30...  Training Step: 120...  Training loss: 3.1931...  0.2502 sec/batch\n",
      "Epoch: 1/30...  Training Step: 121...  Training loss: 3.1693...  0.3740 sec/batch\n",
      "Epoch: 1/30...  Training Step: 122...  Training loss: 3.1702...  0.2990 sec/batch\n",
      "Epoch: 1/30...  Training Step: 123...  Training loss: 3.1792...  0.2049 sec/batch\n",
      "Epoch: 1/30...  Training Step: 124...  Training loss: 3.1667...  0.2496 sec/batch\n",
      "Epoch: 1/30...  Training Step: 125...  Training loss: 3.1974...  0.2835 sec/batch\n",
      "Epoch: 1/30...  Training Step: 126...  Training loss: 3.2760...  0.2107 sec/batch\n",
      "Epoch: 1/30...  Training Step: 127...  Training loss: 3.2312...  0.2196 sec/batch\n",
      "Epoch: 1/30...  Training Step: 128...  Training loss: 3.1912...  0.2301 sec/batch\n",
      "Epoch: 1/30...  Training Step: 129...  Training loss: 3.1697...  0.1579 sec/batch\n",
      "Epoch: 1/30...  Training Step: 130...  Training loss: 3.1811...  0.1704 sec/batch\n",
      "Epoch: 1/30...  Training Step: 131...  Training loss: 3.1821...  0.2128 sec/batch\n",
      "Epoch: 1/30...  Training Step: 132...  Training loss: 3.1430...  0.1969 sec/batch\n",
      "Epoch: 1/30...  Training Step: 133...  Training loss: 3.1360...  0.2122 sec/batch\n",
      "Epoch: 1/30...  Training Step: 134...  Training loss: 3.1548...  0.2501 sec/batch\n",
      "Epoch: 1/30...  Training Step: 135...  Training loss: 3.1735...  0.2613 sec/batch\n",
      "Epoch: 1/30...  Training Step: 136...  Training loss: 3.1408...  0.1940 sec/batch\n",
      "Epoch: 1/30...  Training Step: 137...  Training loss: 3.1364...  0.1894 sec/batch\n",
      "Epoch: 1/30...  Training Step: 138...  Training loss: 3.1131...  0.2352 sec/batch\n",
      "Epoch: 1/30...  Training Step: 139...  Training loss: 3.1474...  0.2531 sec/batch\n",
      "Epoch: 1/30...  Training Step: 140...  Training loss: 3.0976...  0.1619 sec/batch\n",
      "Epoch: 1/30...  Training Step: 141...  Training loss: 3.1120...  0.1912 sec/batch\n",
      "Epoch: 1/30...  Training Step: 142...  Training loss: 3.1072...  0.2528 sec/batch\n",
      "Epoch: 1/30...  Training Step: 143...  Training loss: 3.0979...  0.1669 sec/batch\n",
      "Epoch: 1/30...  Training Step: 144...  Training loss: 3.1111...  0.1916 sec/batch\n",
      "Epoch: 1/30...  Training Step: 145...  Training loss: 3.0881...  0.2071 sec/batch\n",
      "Epoch: 1/30...  Training Step: 146...  Training loss: 3.0812...  0.2192 sec/batch\n",
      "Epoch: 1/30...  Training Step: 147...  Training loss: 3.0901...  0.1599 sec/batch\n",
      "Epoch: 1/30...  Training Step: 148...  Training loss: 3.0864...  0.1835 sec/batch\n",
      "Epoch: 1/30...  Training Step: 149...  Training loss: 3.0920...  0.1704 sec/batch\n",
      "Epoch: 1/30...  Training Step: 150...  Training loss: 3.0617...  0.2144 sec/batch\n",
      "Epoch: 1/30...  Training Step: 151...  Training loss: 3.0993...  0.2359 sec/batch\n",
      "Epoch: 1/30...  Training Step: 152...  Training loss: 3.1492...  0.2804 sec/batch\n",
      "Epoch: 1/30...  Training Step: 153...  Training loss: 3.0734...  0.2424 sec/batch\n",
      "Epoch: 1/30...  Training Step: 154...  Training loss: 3.0454...  0.2345 sec/batch\n",
      "Epoch: 1/30...  Training Step: 155...  Training loss: 3.0315...  0.2305 sec/batch\n",
      "Epoch: 1/30...  Training Step: 156...  Training loss: 3.0211...  0.1687 sec/batch\n",
      "Epoch: 1/30...  Training Step: 157...  Training loss: 3.0358...  0.1862 sec/batch\n",
      "Epoch: 1/30...  Training Step: 158...  Training loss: 3.0371...  0.1915 sec/batch\n",
      "Epoch: 1/30...  Training Step: 159...  Training loss: 3.0249...  0.1689 sec/batch\n",
      "Epoch: 1/30...  Training Step: 160...  Training loss: 3.0246...  0.1851 sec/batch\n",
      "Epoch: 1/30...  Training Step: 161...  Training loss: 3.0064...  0.2267 sec/batch\n",
      "Epoch: 1/30...  Training Step: 162...  Training loss: 2.9823...  0.2212 sec/batch\n",
      "Epoch: 1/30...  Training Step: 163...  Training loss: 2.9863...  0.2015 sec/batch\n",
      "Epoch: 1/30...  Training Step: 164...  Training loss: 2.9905...  0.2735 sec/batch\n",
      "Epoch: 1/30...  Training Step: 165...  Training loss: 2.9597...  0.2535 sec/batch\n",
      "Epoch: 1/30...  Training Step: 166...  Training loss: 2.9624...  0.2552 sec/batch\n",
      "Epoch: 1/30...  Training Step: 167...  Training loss: 2.9777...  0.1562 sec/batch\n",
      "Epoch: 1/30...  Training Step: 168...  Training loss: 2.9482...  0.2053 sec/batch\n",
      "Epoch: 1/30...  Training Step: 169...  Training loss: 2.9655...  0.1988 sec/batch\n",
      "Epoch: 1/30...  Training Step: 170...  Training loss: 2.9409...  0.1860 sec/batch\n",
      "Epoch: 2/30...  Training Step: 171...  Training loss: 3.0372...  0.2024 sec/batch\n",
      "Epoch: 2/30...  Training Step: 172...  Training loss: 2.9517...  0.2173 sec/batch\n",
      "Epoch: 2/30...  Training Step: 173...  Training loss: 2.9303...  0.3053 sec/batch\n",
      "Epoch: 2/30...  Training Step: 174...  Training loss: 2.9638...  0.1956 sec/batch\n",
      "Epoch: 2/30...  Training Step: 175...  Training loss: 2.9391...  0.1871 sec/batch\n",
      "Epoch: 2/30...  Training Step: 176...  Training loss: 2.9477...  0.2698 sec/batch\n",
      "Epoch: 2/30...  Training Step: 177...  Training loss: 2.9355...  0.2701 sec/batch\n",
      "Epoch: 2/30...  Training Step: 178...  Training loss: 2.9165...  0.2791 sec/batch\n",
      "Epoch: 2/30...  Training Step: 179...  Training loss: 2.9015...  0.3577 sec/batch\n",
      "Epoch: 2/30...  Training Step: 180...  Training loss: 2.8771...  0.1907 sec/batch\n",
      "Epoch: 2/30...  Training Step: 181...  Training loss: 2.8951...  0.3028 sec/batch\n",
      "Epoch: 2/30...  Training Step: 182...  Training loss: 2.8912...  0.1596 sec/batch\n",
      "Epoch: 2/30...  Training Step: 183...  Training loss: 2.8815...  0.1629 sec/batch\n",
      "Epoch: 2/30...  Training Step: 184...  Training loss: 2.8617...  0.2060 sec/batch\n",
      "Epoch: 2/30...  Training Step: 185...  Training loss: 2.8640...  0.2457 sec/batch\n",
      "Epoch: 2/30...  Training Step: 186...  Training loss: 2.8773...  0.2236 sec/batch\n",
      "Epoch: 2/30...  Training Step: 187...  Training loss: 2.8394...  0.1884 sec/batch\n",
      "Epoch: 2/30...  Training Step: 188...  Training loss: 2.8378...  0.1617 sec/batch\n",
      "Epoch: 2/30...  Training Step: 189...  Training loss: 2.8531...  0.1917 sec/batch\n",
      "Epoch: 2/30...  Training Step: 190...  Training loss: 2.8385...  0.2500 sec/batch\n",
      "Epoch: 2/30...  Training Step: 191...  Training loss: 2.8199...  0.2160 sec/batch\n",
      "Epoch: 2/30...  Training Step: 192...  Training loss: 2.8264...  0.2391 sec/batch\n",
      "Epoch: 2/30...  Training Step: 193...  Training loss: 2.8047...  0.2546 sec/batch\n",
      "Epoch: 2/30...  Training Step: 194...  Training loss: 2.7924...  0.1589 sec/batch\n",
      "Epoch: 2/30...  Training Step: 195...  Training loss: 2.8206...  0.2070 sec/batch\n",
      "Epoch: 2/30...  Training Step: 196...  Training loss: 2.8081...  0.2945 sec/batch\n",
      "Epoch: 2/30...  Training Step: 197...  Training loss: 2.8015...  0.2357 sec/batch\n",
      "Epoch: 2/30...  Training Step: 198...  Training loss: 2.7738...  0.1970 sec/batch\n",
      "Epoch: 2/30...  Training Step: 199...  Training loss: 2.7652...  0.2172 sec/batch\n",
      "Epoch: 2/30...  Training Step: 200...  Training loss: 2.7374...  0.2132 sec/batch\n",
      "Epoch: 2/30...  Training Step: 201...  Training loss: 2.7599...  0.2362 sec/batch\n",
      "Epoch: 2/30...  Training Step: 202...  Training loss: 2.7497...  0.2254 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30...  Training Step: 203...  Training loss: 2.7434...  0.2690 sec/batch\n",
      "Epoch: 2/30...  Training Step: 204...  Training loss: 2.7220...  0.1998 sec/batch\n",
      "Epoch: 2/30...  Training Step: 205...  Training loss: 2.7286...  0.2150 sec/batch\n",
      "Epoch: 2/30...  Training Step: 206...  Training loss: 2.7461...  0.2697 sec/batch\n",
      "Epoch: 2/30...  Training Step: 207...  Training loss: 2.7331...  0.2774 sec/batch\n",
      "Epoch: 2/30...  Training Step: 208...  Training loss: 2.7159...  0.2515 sec/batch\n",
      "Epoch: 2/30...  Training Step: 209...  Training loss: 2.7216...  0.3241 sec/batch\n",
      "Epoch: 2/30...  Training Step: 210...  Training loss: 2.6989...  0.2039 sec/batch\n",
      "Epoch: 2/30...  Training Step: 211...  Training loss: 2.6953...  0.1740 sec/batch\n",
      "Epoch: 2/30...  Training Step: 212...  Training loss: 2.6995...  0.2502 sec/batch\n",
      "Epoch: 2/30...  Training Step: 213...  Training loss: 2.6720...  0.1954 sec/batch\n",
      "Epoch: 2/30...  Training Step: 214...  Training loss: 2.6732...  0.2288 sec/batch\n",
      "Epoch: 2/30...  Training Step: 215...  Training loss: 2.6613...  0.1558 sec/batch\n",
      "Epoch: 2/30...  Training Step: 216...  Training loss: 2.6883...  0.2474 sec/batch\n",
      "Epoch: 2/30...  Training Step: 217...  Training loss: 2.6518...  0.2242 sec/batch\n",
      "Epoch: 2/30...  Training Step: 218...  Training loss: 2.6536...  0.1766 sec/batch\n",
      "Epoch: 2/30...  Training Step: 219...  Training loss: 2.6651...  0.2074 sec/batch\n",
      "Epoch: 2/30...  Training Step: 220...  Training loss: 2.6468...  0.2506 sec/batch\n",
      "Epoch: 2/30...  Training Step: 221...  Training loss: 2.6574...  0.2161 sec/batch\n",
      "Epoch: 2/30...  Training Step: 222...  Training loss: 2.6471...  0.1690 sec/batch\n",
      "Epoch: 2/30...  Training Step: 223...  Training loss: 2.6237...  0.2330 sec/batch\n",
      "Epoch: 2/30...  Training Step: 224...  Training loss: 2.6542...  0.1750 sec/batch\n",
      "Epoch: 2/30...  Training Step: 225...  Training loss: 2.6311...  0.2653 sec/batch\n",
      "Epoch: 2/30...  Training Step: 226...  Training loss: 2.6430...  0.2780 sec/batch\n",
      "Epoch: 2/30...  Training Step: 227...  Training loss: 2.6404...  0.2691 sec/batch\n",
      "Epoch: 2/30...  Training Step: 228...  Training loss: 2.6312...  0.2657 sec/batch\n",
      "Epoch: 2/30...  Training Step: 229...  Training loss: 2.6361...  0.2232 sec/batch\n",
      "Epoch: 2/30...  Training Step: 230...  Training loss: 2.6303...  0.2614 sec/batch\n",
      "Epoch: 2/30...  Training Step: 231...  Training loss: 2.6234...  0.2078 sec/batch\n",
      "Epoch: 2/30...  Training Step: 232...  Training loss: 2.6102...  0.2183 sec/batch\n",
      "Epoch: 2/30...  Training Step: 233...  Training loss: 2.5989...  0.1705 sec/batch\n",
      "Epoch: 2/30...  Training Step: 234...  Training loss: 2.6029...  0.2754 sec/batch\n",
      "Epoch: 2/30...  Training Step: 235...  Training loss: 2.6070...  0.2543 sec/batch\n",
      "Epoch: 2/30...  Training Step: 236...  Training loss: 2.5958...  0.2122 sec/batch\n",
      "Epoch: 2/30...  Training Step: 237...  Training loss: 2.5911...  0.2302 sec/batch\n",
      "Epoch: 2/30...  Training Step: 238...  Training loss: 2.5872...  0.2263 sec/batch\n",
      "Epoch: 2/30...  Training Step: 239...  Training loss: 2.5991...  0.2576 sec/batch\n",
      "Epoch: 2/30...  Training Step: 240...  Training loss: 2.5706...  0.2615 sec/batch\n",
      "Epoch: 2/30...  Training Step: 241...  Training loss: 2.5876...  0.2444 sec/batch\n",
      "Epoch: 2/30...  Training Step: 242...  Training loss: 2.5847...  0.1776 sec/batch\n",
      "Epoch: 2/30...  Training Step: 243...  Training loss: 2.5863...  0.1735 sec/batch\n",
      "Epoch: 2/30...  Training Step: 244...  Training loss: 2.6086...  0.1981 sec/batch\n",
      "Epoch: 2/30...  Training Step: 245...  Training loss: 2.5840...  0.3063 sec/batch\n",
      "Epoch: 2/30...  Training Step: 246...  Training loss: 2.5824...  0.1978 sec/batch\n",
      "Epoch: 2/30...  Training Step: 247...  Training loss: 2.5789...  0.1840 sec/batch\n",
      "Epoch: 2/30...  Training Step: 248...  Training loss: 2.5673...  0.1562 sec/batch\n",
      "Epoch: 2/30...  Training Step: 249...  Training loss: 2.5751...  0.1901 sec/batch\n",
      "Epoch: 2/30...  Training Step: 250...  Training loss: 2.5832...  0.1944 sec/batch\n",
      "Epoch: 2/30...  Training Step: 251...  Training loss: 2.5482...  0.2753 sec/batch\n",
      "Epoch: 2/30...  Training Step: 252...  Training loss: 2.5749...  0.2469 sec/batch\n",
      "Epoch: 2/30...  Training Step: 253...  Training loss: 2.5642...  0.1584 sec/batch\n",
      "Epoch: 2/30...  Training Step: 254...  Training loss: 2.5827...  0.2833 sec/batch\n",
      "Epoch: 2/30...  Training Step: 255...  Training loss: 2.5830...  0.2135 sec/batch\n",
      "Epoch: 2/30...  Training Step: 256...  Training loss: 2.5602...  0.1740 sec/batch\n",
      "Epoch: 2/30...  Training Step: 257...  Training loss: 2.5549...  0.2690 sec/batch\n",
      "Epoch: 2/30...  Training Step: 258...  Training loss: 2.5475...  0.1856 sec/batch\n",
      "Epoch: 2/30...  Training Step: 259...  Training loss: 2.5460...  0.2154 sec/batch\n",
      "Epoch: 2/30...  Training Step: 260...  Training loss: 2.5096...  0.2678 sec/batch\n",
      "Epoch: 2/30...  Training Step: 261...  Training loss: 2.5340...  0.2153 sec/batch\n",
      "Epoch: 2/30...  Training Step: 262...  Training loss: 2.5324...  0.1562 sec/batch\n",
      "Epoch: 2/30...  Training Step: 263...  Training loss: 2.5610...  0.1573 sec/batch\n",
      "Epoch: 2/30...  Training Step: 264...  Training loss: 2.5639...  0.2537 sec/batch\n",
      "Epoch: 2/30...  Training Step: 265...  Training loss: 2.5524...  0.2106 sec/batch\n",
      "Epoch: 2/30...  Training Step: 266...  Training loss: 2.5102...  0.2647 sec/batch\n",
      "Epoch: 2/30...  Training Step: 267...  Training loss: 2.5192...  0.1956 sec/batch\n",
      "Epoch: 2/30...  Training Step: 268...  Training loss: 2.5455...  0.1933 sec/batch\n",
      "Epoch: 2/30...  Training Step: 269...  Training loss: 2.5203...  0.1544 sec/batch\n",
      "Epoch: 2/30...  Training Step: 270...  Training loss: 2.5377...  0.1879 sec/batch\n",
      "Epoch: 2/30...  Training Step: 271...  Training loss: 2.5282...  0.2349 sec/batch\n",
      "Epoch: 2/30...  Training Step: 272...  Training loss: 2.5028...  0.2196 sec/batch\n",
      "Epoch: 2/30...  Training Step: 273...  Training loss: 2.5068...  0.2041 sec/batch\n",
      "Epoch: 2/30...  Training Step: 274...  Training loss: 2.5186...  0.2119 sec/batch\n",
      "Epoch: 2/30...  Training Step: 275...  Training loss: 2.4970...  0.2508 sec/batch\n",
      "Epoch: 2/30...  Training Step: 276...  Training loss: 2.4972...  0.1996 sec/batch\n",
      "Epoch: 2/30...  Training Step: 277...  Training loss: 2.4900...  0.2580 sec/batch\n",
      "Epoch: 2/30...  Training Step: 278...  Training loss: 2.4813...  0.2603 sec/batch\n",
      "Epoch: 2/30...  Training Step: 279...  Training loss: 2.4844...  0.2567 sec/batch\n",
      "Epoch: 2/30...  Training Step: 280...  Training loss: 2.4914...  0.1915 sec/batch\n",
      "Epoch: 2/30...  Training Step: 281...  Training loss: 2.4790...  0.2487 sec/batch\n",
      "Epoch: 2/30...  Training Step: 282...  Training loss: 2.4829...  0.2397 sec/batch\n",
      "Epoch: 2/30...  Training Step: 283...  Training loss: 2.4780...  0.2530 sec/batch\n",
      "Epoch: 2/30...  Training Step: 284...  Training loss: 2.5165...  0.2433 sec/batch\n",
      "Epoch: 2/30...  Training Step: 285...  Training loss: 2.5085...  0.1657 sec/batch\n",
      "Epoch: 2/30...  Training Step: 286...  Training loss: 2.4839...  0.2208 sec/batch\n",
      "Epoch: 2/30...  Training Step: 287...  Training loss: 2.4854...  0.2097 sec/batch\n",
      "Epoch: 2/30...  Training Step: 288...  Training loss: 2.4721...  0.2466 sec/batch\n",
      "Epoch: 2/30...  Training Step: 289...  Training loss: 2.4725...  0.1834 sec/batch\n",
      "Epoch: 2/30...  Training Step: 290...  Training loss: 2.4840...  0.3155 sec/batch\n",
      "Epoch: 2/30...  Training Step: 291...  Training loss: 2.4612...  0.2748 sec/batch\n",
      "Epoch: 2/30...  Training Step: 292...  Training loss: 2.4706...  0.2051 sec/batch\n",
      "Epoch: 2/30...  Training Step: 293...  Training loss: 2.4801...  0.2513 sec/batch\n",
      "Epoch: 2/30...  Training Step: 294...  Training loss: 2.4829...  0.2539 sec/batch\n",
      "Epoch: 2/30...  Training Step: 295...  Training loss: 2.4741...  0.1961 sec/batch\n",
      "Epoch: 2/30...  Training Step: 296...  Training loss: 2.4771...  0.1798 sec/batch\n",
      "Epoch: 2/30...  Training Step: 297...  Training loss: 2.4871...  0.2825 sec/batch\n",
      "Epoch: 2/30...  Training Step: 298...  Training loss: 2.4661...  0.3152 sec/batch\n",
      "Epoch: 2/30...  Training Step: 299...  Training loss: 2.4778...  0.2526 sec/batch\n",
      "Epoch: 2/30...  Training Step: 300...  Training loss: 2.4786...  0.2581 sec/batch\n",
      "Epoch: 2/30...  Training Step: 301...  Training loss: 2.4875...  0.2102 sec/batch\n",
      "Epoch: 2/30...  Training Step: 302...  Training loss: 2.4326...  0.2461 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/30...  Training Step: 303...  Training loss: 2.4387...  0.2277 sec/batch\n",
      "Epoch: 2/30...  Training Step: 304...  Training loss: 2.4836...  0.2720 sec/batch\n",
      "Epoch: 2/30...  Training Step: 305...  Training loss: 2.4754...  0.2317 sec/batch\n",
      "Epoch: 2/30...  Training Step: 306...  Training loss: 2.4751...  0.2181 sec/batch\n",
      "Epoch: 2/30...  Training Step: 307...  Training loss: 2.4387...  0.1569 sec/batch\n",
      "Epoch: 2/30...  Training Step: 308...  Training loss: 2.4396...  0.2585 sec/batch\n",
      "Epoch: 2/30...  Training Step: 309...  Training loss: 2.4463...  0.2909 sec/batch\n",
      "Epoch: 2/30...  Training Step: 310...  Training loss: 2.4377...  0.1860 sec/batch\n",
      "Epoch: 2/30...  Training Step: 311...  Training loss: 2.4330...  0.2384 sec/batch\n",
      "Epoch: 2/30...  Training Step: 312...  Training loss: 2.4429...  0.2065 sec/batch\n",
      "Epoch: 2/30...  Training Step: 313...  Training loss: 2.4358...  0.1637 sec/batch\n",
      "Epoch: 2/30...  Training Step: 314...  Training loss: 2.4515...  0.1892 sec/batch\n",
      "Epoch: 2/30...  Training Step: 315...  Training loss: 2.4318...  0.1618 sec/batch\n",
      "Epoch: 2/30...  Training Step: 316...  Training loss: 2.4146...  0.2029 sec/batch\n",
      "Epoch: 2/30...  Training Step: 317...  Training loss: 2.4645...  0.1682 sec/batch\n",
      "Epoch: 2/30...  Training Step: 318...  Training loss: 2.4204...  0.2479 sec/batch\n",
      "Epoch: 2/30...  Training Step: 319...  Training loss: 2.4321...  0.2381 sec/batch\n",
      "Epoch: 2/30...  Training Step: 320...  Training loss: 2.4355...  0.2065 sec/batch\n",
      "Epoch: 2/30...  Training Step: 321...  Training loss: 2.4594...  0.1687 sec/batch\n",
      "Epoch: 2/30...  Training Step: 322...  Training loss: 2.4538...  0.2656 sec/batch\n",
      "Epoch: 2/30...  Training Step: 323...  Training loss: 2.4313...  0.1746 sec/batch\n",
      "Epoch: 2/30...  Training Step: 324...  Training loss: 2.4247...  0.2369 sec/batch\n",
      "Epoch: 2/30...  Training Step: 325...  Training loss: 2.4085...  0.2750 sec/batch\n",
      "Epoch: 2/30...  Training Step: 326...  Training loss: 2.4016...  0.2336 sec/batch\n",
      "Epoch: 2/30...  Training Step: 327...  Training loss: 2.4224...  0.2420 sec/batch\n",
      "Epoch: 2/30...  Training Step: 328...  Training loss: 2.4403...  0.2494 sec/batch\n",
      "Epoch: 2/30...  Training Step: 329...  Training loss: 2.4082...  0.2519 sec/batch\n",
      "Epoch: 2/30...  Training Step: 330...  Training loss: 2.4212...  0.1700 sec/batch\n",
      "Epoch: 2/30...  Training Step: 331...  Training loss: 2.4097...  0.2444 sec/batch\n",
      "Epoch: 2/30...  Training Step: 332...  Training loss: 2.4041...  0.1720 sec/batch\n",
      "Epoch: 2/30...  Training Step: 333...  Training loss: 2.4032...  0.2484 sec/batch\n",
      "Epoch: 2/30...  Training Step: 334...  Training loss: 2.3899...  0.2036 sec/batch\n",
      "Epoch: 2/30...  Training Step: 335...  Training loss: 2.3926...  0.2578 sec/batch\n",
      "Epoch: 2/30...  Training Step: 336...  Training loss: 2.3823...  0.2488 sec/batch\n",
      "Epoch: 2/30...  Training Step: 337...  Training loss: 2.3909...  0.1543 sec/batch\n",
      "Epoch: 2/30...  Training Step: 338...  Training loss: 2.3817...  0.2773 sec/batch\n",
      "Epoch: 2/30...  Training Step: 339...  Training loss: 2.3968...  0.2549 sec/batch\n",
      "Epoch: 2/30...  Training Step: 340...  Training loss: 2.3641...  0.1828 sec/batch\n",
      "Epoch: 3/30...  Training Step: 341...  Training loss: 2.4893...  0.2315 sec/batch\n",
      "Epoch: 3/30...  Training Step: 342...  Training loss: 2.3937...  0.2334 sec/batch\n",
      "Epoch: 3/30...  Training Step: 343...  Training loss: 2.3781...  0.1892 sec/batch\n",
      "Epoch: 3/30...  Training Step: 344...  Training loss: 2.4187...  0.2841 sec/batch\n",
      "Epoch: 3/30...  Training Step: 345...  Training loss: 2.3848...  0.2084 sec/batch\n",
      "Epoch: 3/30...  Training Step: 346...  Training loss: 2.4226...  0.1889 sec/batch\n",
      "Epoch: 3/30...  Training Step: 347...  Training loss: 2.4029...  0.2807 sec/batch\n",
      "Epoch: 3/30...  Training Step: 348...  Training loss: 2.3825...  0.2826 sec/batch\n",
      "Epoch: 3/30...  Training Step: 349...  Training loss: 2.3639...  0.2464 sec/batch\n",
      "Epoch: 3/30...  Training Step: 350...  Training loss: 2.3701...  0.1693 sec/batch\n",
      "Epoch: 3/30...  Training Step: 351...  Training loss: 2.3656...  0.2076 sec/batch\n",
      "Epoch: 3/30...  Training Step: 352...  Training loss: 2.3833...  0.1569 sec/batch\n",
      "Epoch: 3/30...  Training Step: 353...  Training loss: 2.3762...  0.2367 sec/batch\n",
      "Epoch: 3/30...  Training Step: 354...  Training loss: 2.3586...  0.2740 sec/batch\n",
      "Epoch: 3/30...  Training Step: 355...  Training loss: 2.3880...  0.2853 sec/batch\n",
      "Epoch: 3/30...  Training Step: 356...  Training loss: 2.3958...  0.2493 sec/batch\n",
      "Epoch: 3/30...  Training Step: 357...  Training loss: 2.3311...  0.2134 sec/batch\n",
      "Epoch: 3/30...  Training Step: 358...  Training loss: 2.3753...  0.2619 sec/batch\n",
      "Epoch: 3/30...  Training Step: 359...  Training loss: 2.3841...  0.2085 sec/batch\n",
      "Epoch: 3/30...  Training Step: 360...  Training loss: 2.3585...  0.2446 sec/batch\n",
      "Epoch: 3/30...  Training Step: 361...  Training loss: 2.3504...  0.2746 sec/batch\n",
      "Epoch: 3/30...  Training Step: 362...  Training loss: 2.3281...  0.2540 sec/batch\n",
      "Epoch: 3/30...  Training Step: 363...  Training loss: 2.3535...  0.2689 sec/batch\n",
      "Epoch: 3/30...  Training Step: 364...  Training loss: 2.3271...  0.2384 sec/batch\n",
      "Epoch: 3/30...  Training Step: 365...  Training loss: 2.3743...  0.2166 sec/batch\n",
      "Epoch: 3/30...  Training Step: 366...  Training loss: 2.3544...  0.2588 sec/batch\n",
      "Epoch: 3/30...  Training Step: 367...  Training loss: 2.3799...  0.1592 sec/batch\n",
      "Epoch: 3/30...  Training Step: 368...  Training loss: 2.3400...  0.2428 sec/batch\n",
      "Epoch: 3/30...  Training Step: 369...  Training loss: 2.3568...  0.1601 sec/batch\n",
      "Epoch: 3/30...  Training Step: 370...  Training loss: 2.3196...  0.2633 sec/batch\n",
      "Epoch: 3/30...  Training Step: 371...  Training loss: 2.3454...  0.2507 sec/batch\n",
      "Epoch: 3/30...  Training Step: 372...  Training loss: 2.3561...  0.2865 sec/batch\n",
      "Epoch: 3/30...  Training Step: 373...  Training loss: 2.3393...  0.2707 sec/batch\n",
      "Epoch: 3/30...  Training Step: 374...  Training loss: 2.3235...  0.2388 sec/batch\n",
      "Epoch: 3/30...  Training Step: 375...  Training loss: 2.3341...  0.2716 sec/batch\n",
      "Epoch: 3/30...  Training Step: 376...  Training loss: 2.3544...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 377...  Training loss: 2.3431...  0.1912 sec/batch\n",
      "Epoch: 3/30...  Training Step: 378...  Training loss: 2.3345...  0.2616 sec/batch\n",
      "Epoch: 3/30...  Training Step: 379...  Training loss: 2.3477...  0.2364 sec/batch\n",
      "Epoch: 3/30...  Training Step: 380...  Training loss: 2.3370...  0.1527 sec/batch\n",
      "Epoch: 3/30...  Training Step: 381...  Training loss: 2.3477...  0.2279 sec/batch\n",
      "Epoch: 3/30...  Training Step: 382...  Training loss: 2.3164...  0.2214 sec/batch\n",
      "Epoch: 3/30...  Training Step: 383...  Training loss: 2.3105...  0.2664 sec/batch\n",
      "Epoch: 3/30...  Training Step: 384...  Training loss: 2.3420...  0.3004 sec/batch\n",
      "Epoch: 3/30...  Training Step: 385...  Training loss: 2.3150...  0.2511 sec/batch\n",
      "Epoch: 3/30...  Training Step: 386...  Training loss: 2.3279...  0.1954 sec/batch\n",
      "Epoch: 3/30...  Training Step: 387...  Training loss: 2.3164...  0.1614 sec/batch\n",
      "Epoch: 3/30...  Training Step: 388...  Training loss: 2.3133...  0.1812 sec/batch\n",
      "Epoch: 3/30...  Training Step: 389...  Training loss: 2.3213...  0.2515 sec/batch\n",
      "Epoch: 3/30...  Training Step: 390...  Training loss: 2.3243...  0.1777 sec/batch\n",
      "Epoch: 3/30...  Training Step: 391...  Training loss: 2.3436...  0.1583 sec/batch\n",
      "Epoch: 3/30...  Training Step: 392...  Training loss: 2.3293...  0.1782 sec/batch\n",
      "Epoch: 3/30...  Training Step: 393...  Training loss: 2.2944...  0.2926 sec/batch\n",
      "Epoch: 3/30...  Training Step: 394...  Training loss: 2.3340...  0.2478 sec/batch\n",
      "Epoch: 3/30...  Training Step: 395...  Training loss: 2.3044...  0.2378 sec/batch\n",
      "Epoch: 3/30...  Training Step: 396...  Training loss: 2.3269...  0.2344 sec/batch\n",
      "Epoch: 3/30...  Training Step: 397...  Training loss: 2.3174...  0.1977 sec/batch\n",
      "Epoch: 3/30...  Training Step: 398...  Training loss: 2.3053...  0.2171 sec/batch\n",
      "Epoch: 3/30...  Training Step: 399...  Training loss: 2.3310...  0.2724 sec/batch\n",
      "Epoch: 3/30...  Training Step: 400...  Training loss: 2.3098...  0.2759 sec/batch\n",
      "Epoch: 3/30...  Training Step: 401...  Training loss: 2.3155...  0.1696 sec/batch\n",
      "Epoch: 3/30...  Training Step: 402...  Training loss: 2.3024...  0.1788 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30...  Training Step: 403...  Training loss: 2.2942...  0.1646 sec/batch\n",
      "Epoch: 3/30...  Training Step: 404...  Training loss: 2.2840...  0.2340 sec/batch\n",
      "Epoch: 3/30...  Training Step: 405...  Training loss: 2.3145...  0.2608 sec/batch\n",
      "Epoch: 3/30...  Training Step: 406...  Training loss: 2.2991...  0.2679 sec/batch\n",
      "Epoch: 3/30...  Training Step: 407...  Training loss: 2.3105...  0.1857 sec/batch\n",
      "Epoch: 3/30...  Training Step: 408...  Training loss: 2.2730...  0.1899 sec/batch\n",
      "Epoch: 3/30...  Training Step: 409...  Training loss: 2.3078...  0.2445 sec/batch\n",
      "Epoch: 3/30...  Training Step: 410...  Training loss: 2.2686...  0.2447 sec/batch\n",
      "Epoch: 3/30...  Training Step: 411...  Training loss: 2.2840...  0.1581 sec/batch\n",
      "Epoch: 3/30...  Training Step: 412...  Training loss: 2.2755...  0.1713 sec/batch\n",
      "Epoch: 3/30...  Training Step: 413...  Training loss: 2.2984...  0.1938 sec/batch\n",
      "Epoch: 3/30...  Training Step: 414...  Training loss: 2.3180...  0.2541 sec/batch\n",
      "Epoch: 3/30...  Training Step: 415...  Training loss: 2.2920...  0.2384 sec/batch\n",
      "Epoch: 3/30...  Training Step: 416...  Training loss: 2.2749...  0.1686 sec/batch\n",
      "Epoch: 3/30...  Training Step: 417...  Training loss: 2.2797...  0.2373 sec/batch\n",
      "Epoch: 3/30...  Training Step: 418...  Training loss: 2.2895...  0.2392 sec/batch\n",
      "Epoch: 3/30...  Training Step: 419...  Training loss: 2.2832...  0.1847 sec/batch\n",
      "Epoch: 3/30...  Training Step: 420...  Training loss: 2.2971...  0.2395 sec/batch\n",
      "Epoch: 3/30...  Training Step: 421...  Training loss: 2.2789...  0.1589 sec/batch\n",
      "Epoch: 3/30...  Training Step: 422...  Training loss: 2.2983...  0.1566 sec/batch\n",
      "Epoch: 3/30...  Training Step: 423...  Training loss: 2.2866...  0.1876 sec/batch\n",
      "Epoch: 3/30...  Training Step: 424...  Training loss: 2.2611...  0.1610 sec/batch\n",
      "Epoch: 3/30...  Training Step: 425...  Training loss: 2.3051...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 426...  Training loss: 2.2585...  0.2691 sec/batch\n",
      "Epoch: 3/30...  Training Step: 427...  Training loss: 2.2736...  0.2809 sec/batch\n",
      "Epoch: 3/30...  Training Step: 428...  Training loss: 2.2808...  0.2586 sec/batch\n",
      "Epoch: 3/30...  Training Step: 429...  Training loss: 2.2639...  0.2350 sec/batch\n",
      "Epoch: 3/30...  Training Step: 430...  Training loss: 2.2399...  0.2406 sec/batch\n",
      "Epoch: 3/30...  Training Step: 431...  Training loss: 2.2647...  0.1609 sec/batch\n",
      "Epoch: 3/30...  Training Step: 432...  Training loss: 2.2605...  0.1630 sec/batch\n",
      "Epoch: 3/30...  Training Step: 433...  Training loss: 2.2747...  0.2497 sec/batch\n",
      "Epoch: 3/30...  Training Step: 434...  Training loss: 2.2964...  0.2871 sec/batch\n",
      "Epoch: 3/30...  Training Step: 435...  Training loss: 2.2672...  0.1874 sec/batch\n",
      "Epoch: 3/30...  Training Step: 436...  Training loss: 2.2485...  0.2864 sec/batch\n",
      "Epoch: 3/30...  Training Step: 437...  Training loss: 2.2511...  0.2694 sec/batch\n",
      "Epoch: 3/30...  Training Step: 438...  Training loss: 2.2758...  0.2040 sec/batch\n",
      "Epoch: 3/30...  Training Step: 439...  Training loss: 2.2501...  0.1628 sec/batch\n",
      "Epoch: 3/30...  Training Step: 440...  Training loss: 2.2841...  0.1726 sec/batch\n",
      "Epoch: 3/30...  Training Step: 441...  Training loss: 2.2768...  0.1538 sec/batch\n",
      "Epoch: 3/30...  Training Step: 442...  Training loss: 2.2375...  0.2733 sec/batch\n",
      "Epoch: 3/30...  Training Step: 443...  Training loss: 2.2438...  0.3005 sec/batch\n",
      "Epoch: 3/30...  Training Step: 444...  Training loss: 2.2444...  0.2762 sec/batch\n",
      "Epoch: 3/30...  Training Step: 445...  Training loss: 2.2473...  0.3263 sec/batch\n",
      "Epoch: 3/30...  Training Step: 446...  Training loss: 2.2298...  0.1730 sec/batch\n",
      "Epoch: 3/30...  Training Step: 447...  Training loss: 2.2406...  0.2561 sec/batch\n",
      "Epoch: 3/30...  Training Step: 448...  Training loss: 2.2165...  0.1813 sec/batch\n",
      "Epoch: 3/30...  Training Step: 449...  Training loss: 2.2271...  0.1831 sec/batch\n",
      "Epoch: 3/30...  Training Step: 450...  Training loss: 2.2280...  0.1985 sec/batch\n",
      "Epoch: 3/30...  Training Step: 451...  Training loss: 2.2199...  0.2319 sec/batch\n",
      "Epoch: 3/30...  Training Step: 452...  Training loss: 2.1972...  0.2754 sec/batch\n",
      "Epoch: 3/30...  Training Step: 453...  Training loss: 2.2235...  0.1562 sec/batch\n",
      "Epoch: 3/30...  Training Step: 454...  Training loss: 2.2627...  0.2746 sec/batch\n",
      "Epoch: 3/30...  Training Step: 455...  Training loss: 2.2545...  0.1657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 456...  Training loss: 2.2215...  0.1626 sec/batch\n",
      "Epoch: 3/30...  Training Step: 457...  Training loss: 2.2332...  0.2274 sec/batch\n",
      "Epoch: 3/30...  Training Step: 458...  Training loss: 2.2338...  0.2685 sec/batch\n",
      "Epoch: 3/30...  Training Step: 459...  Training loss: 2.2012...  0.2520 sec/batch\n",
      "Epoch: 3/30...  Training Step: 460...  Training loss: 2.2390...  0.2146 sec/batch\n",
      "Epoch: 3/30...  Training Step: 461...  Training loss: 2.2098...  0.2272 sec/batch\n",
      "Epoch: 3/30...  Training Step: 462...  Training loss: 2.2190...  0.2344 sec/batch\n",
      "Epoch: 3/30...  Training Step: 463...  Training loss: 2.2110...  0.1577 sec/batch\n",
      "Epoch: 3/30...  Training Step: 464...  Training loss: 2.2452...  0.2638 sec/batch\n",
      "Epoch: 3/30...  Training Step: 465...  Training loss: 2.2085...  0.1967 sec/batch\n",
      "Epoch: 3/30...  Training Step: 466...  Training loss: 2.2213...  0.1684 sec/batch\n",
      "Epoch: 3/30...  Training Step: 467...  Training loss: 2.2368...  0.2264 sec/batch\n",
      "Epoch: 3/30...  Training Step: 468...  Training loss: 2.2141...  0.2955 sec/batch\n",
      "Epoch: 3/30...  Training Step: 469...  Training loss: 2.2250...  0.2525 sec/batch\n",
      "Epoch: 3/30...  Training Step: 470...  Training loss: 2.2416...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 471...  Training loss: 2.2653...  0.3602 sec/batch\n",
      "Epoch: 3/30...  Training Step: 472...  Training loss: 2.1934...  0.2569 sec/batch\n",
      "Epoch: 3/30...  Training Step: 473...  Training loss: 2.2035...  0.2606 sec/batch\n",
      "Epoch: 3/30...  Training Step: 474...  Training loss: 2.2395...  0.1554 sec/batch\n",
      "Epoch: 3/30...  Training Step: 475...  Training loss: 2.2287...  0.2455 sec/batch\n",
      "Epoch: 3/30...  Training Step: 476...  Training loss: 2.2503...  0.2851 sec/batch\n",
      "Epoch: 3/30...  Training Step: 477...  Training loss: 2.1876...  0.2505 sec/batch\n",
      "Epoch: 3/30...  Training Step: 478...  Training loss: 2.1963...  0.2506 sec/batch\n",
      "Epoch: 3/30...  Training Step: 479...  Training loss: 2.1901...  0.2574 sec/batch\n",
      "Epoch: 3/30...  Training Step: 480...  Training loss: 2.1980...  0.2635 sec/batch\n",
      "Epoch: 3/30...  Training Step: 481...  Training loss: 2.1959...  0.2900 sec/batch\n",
      "Epoch: 3/30...  Training Step: 482...  Training loss: 2.1974...  0.2827 sec/batch\n",
      "Epoch: 3/30...  Training Step: 483...  Training loss: 2.1863...  0.2728 sec/batch\n",
      "Epoch: 3/30...  Training Step: 484...  Training loss: 2.2030...  0.2014 sec/batch\n",
      "Epoch: 3/30...  Training Step: 485...  Training loss: 2.1828...  0.1647 sec/batch\n",
      "Epoch: 3/30...  Training Step: 486...  Training loss: 2.1730...  0.2285 sec/batch\n",
      "Epoch: 3/30...  Training Step: 487...  Training loss: 2.2243...  0.1863 sec/batch\n",
      "Epoch: 3/30...  Training Step: 488...  Training loss: 2.1891...  0.2198 sec/batch\n",
      "Epoch: 3/30...  Training Step: 489...  Training loss: 2.1971...  0.1994 sec/batch\n",
      "Epoch: 3/30...  Training Step: 490...  Training loss: 2.1982...  0.2606 sec/batch\n",
      "Epoch: 3/30...  Training Step: 491...  Training loss: 2.2113...  0.2837 sec/batch\n",
      "Epoch: 3/30...  Training Step: 492...  Training loss: 2.1929...  0.2878 sec/batch\n",
      "Epoch: 3/30...  Training Step: 493...  Training loss: 2.2022...  0.2530 sec/batch\n",
      "Epoch: 3/30...  Training Step: 494...  Training loss: 2.1739...  0.2251 sec/batch\n",
      "Epoch: 3/30...  Training Step: 495...  Training loss: 2.1776...  0.2029 sec/batch\n",
      "Epoch: 3/30...  Training Step: 496...  Training loss: 2.1447...  0.2038 sec/batch\n",
      "Epoch: 3/30...  Training Step: 497...  Training loss: 2.1758...  0.2332 sec/batch\n",
      "Epoch: 3/30...  Training Step: 498...  Training loss: 2.2066...  0.2194 sec/batch\n",
      "Epoch: 3/30...  Training Step: 499...  Training loss: 2.1706...  0.2792 sec/batch\n",
      "Epoch: 3/30...  Training Step: 500...  Training loss: 2.2014...  0.3133 sec/batch\n",
      "Epoch: 3/30...  Training Step: 501...  Training loss: 2.1797...  0.2789 sec/batch\n",
      "Epoch: 3/30...  Training Step: 502...  Training loss: 2.1774...  0.2763 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/30...  Training Step: 503...  Training loss: 2.1761...  0.2585 sec/batch\n",
      "Epoch: 3/30...  Training Step: 504...  Training loss: 2.1593...  0.2349 sec/batch\n",
      "Epoch: 3/30...  Training Step: 505...  Training loss: 2.1649...  0.1739 sec/batch\n",
      "Epoch: 3/30...  Training Step: 506...  Training loss: 2.1488...  0.1903 sec/batch\n",
      "Epoch: 3/30...  Training Step: 507...  Training loss: 2.1626...  0.2363 sec/batch\n",
      "Epoch: 3/30...  Training Step: 508...  Training loss: 2.1603...  0.2912 sec/batch\n",
      "Epoch: 3/30...  Training Step: 509...  Training loss: 2.1852...  0.2657 sec/batch\n",
      "Epoch: 3/30...  Training Step: 510...  Training loss: 2.1337...  0.2784 sec/batch\n",
      "Epoch: 4/30...  Training Step: 511...  Training loss: 2.2658...  0.2355 sec/batch\n",
      "Epoch: 4/30...  Training Step: 512...  Training loss: 2.1531...  0.2786 sec/batch\n",
      "Epoch: 4/30...  Training Step: 513...  Training loss: 2.1655...  0.1884 sec/batch\n",
      "Epoch: 4/30...  Training Step: 514...  Training loss: 2.1926...  0.1727 sec/batch\n",
      "Epoch: 4/30...  Training Step: 515...  Training loss: 2.1773...  0.1598 sec/batch\n",
      "Epoch: 4/30...  Training Step: 516...  Training loss: 2.1942...  0.2351 sec/batch\n",
      "Epoch: 4/30...  Training Step: 517...  Training loss: 2.1634...  0.2134 sec/batch\n",
      "Epoch: 4/30...  Training Step: 518...  Training loss: 2.1405...  0.2165 sec/batch\n",
      "Epoch: 4/30...  Training Step: 519...  Training loss: 2.1379...  0.2462 sec/batch\n",
      "Epoch: 4/30...  Training Step: 520...  Training loss: 2.1418...  0.1986 sec/batch\n",
      "Epoch: 4/30...  Training Step: 521...  Training loss: 2.1306...  0.2488 sec/batch\n",
      "Epoch: 4/30...  Training Step: 522...  Training loss: 2.1684...  0.2365 sec/batch\n",
      "Epoch: 4/30...  Training Step: 523...  Training loss: 2.1452...  0.1757 sec/batch\n",
      "Epoch: 4/30...  Training Step: 524...  Training loss: 2.1402...  0.2043 sec/batch\n",
      "Epoch: 4/30...  Training Step: 525...  Training loss: 2.1750...  0.1577 sec/batch\n",
      "Epoch: 4/30...  Training Step: 526...  Training loss: 2.1836...  0.1855 sec/batch\n",
      "Epoch: 4/30...  Training Step: 527...  Training loss: 2.1144...  0.1782 sec/batch\n",
      "Epoch: 4/30...  Training Step: 528...  Training loss: 2.1591...  0.2479 sec/batch\n",
      "Epoch: 4/30...  Training Step: 529...  Training loss: 2.1583...  0.2024 sec/batch\n",
      "Epoch: 4/30...  Training Step: 530...  Training loss: 2.1492...  0.2116 sec/batch\n",
      "Epoch: 4/30...  Training Step: 531...  Training loss: 2.1341...  0.2051 sec/batch\n",
      "Epoch: 4/30...  Training Step: 532...  Training loss: 2.1081...  0.2426 sec/batch\n",
      "Epoch: 4/30...  Training Step: 533...  Training loss: 2.1235...  0.2821 sec/batch\n",
      "Epoch: 4/30...  Training Step: 534...  Training loss: 2.0996...  0.2330 sec/batch\n",
      "Epoch: 4/30...  Training Step: 535...  Training loss: 2.1634...  0.1558 sec/batch\n",
      "Epoch: 4/30...  Training Step: 536...  Training loss: 2.1334...  0.2000 sec/batch\n",
      "Epoch: 4/30...  Training Step: 537...  Training loss: 2.1749...  0.2400 sec/batch\n",
      "Epoch: 4/30...  Training Step: 538...  Training loss: 2.1489...  0.1863 sec/batch\n",
      "Epoch: 4/30...  Training Step: 539...  Training loss: 2.1431...  0.2294 sec/batch\n",
      "Epoch: 4/30...  Training Step: 540...  Training loss: 2.0983...  0.1869 sec/batch\n",
      "Epoch: 4/30...  Training Step: 541...  Training loss: 2.1461...  0.2430 sec/batch\n",
      "Epoch: 4/30...  Training Step: 542...  Training loss: 2.1396...  0.2072 sec/batch\n",
      "Epoch: 4/30...  Training Step: 543...  Training loss: 2.1298...  0.1773 sec/batch\n",
      "Epoch: 4/30...  Training Step: 544...  Training loss: 2.0998...  0.1551 sec/batch\n",
      "Epoch: 4/30...  Training Step: 545...  Training loss: 2.1261...  0.1593 sec/batch\n",
      "Epoch: 4/30...  Training Step: 546...  Training loss: 2.1344...  0.1648 sec/batch\n",
      "Epoch: 4/30...  Training Step: 547...  Training loss: 2.1543...  0.1526 sec/batch\n",
      "Epoch: 4/30...  Training Step: 548...  Training loss: 2.1352...  0.2016 sec/batch\n",
      "Epoch: 4/30...  Training Step: 549...  Training loss: 2.1320...  0.2580 sec/batch\n",
      "Epoch: 4/30...  Training Step: 550...  Training loss: 2.1333...  0.2765 sec/batch\n",
      "Epoch: 4/30...  Training Step: 551...  Training loss: 2.1593...  0.1825 sec/batch\n",
      "Epoch: 4/30...  Training Step: 552...  Training loss: 2.1168...  0.2028 sec/batch\n",
      "Epoch: 4/30...  Training Step: 553...  Training loss: 2.1104...  0.2496 sec/batch\n",
      "Epoch: 4/30...  Training Step: 554...  Training loss: 2.1557...  0.2017 sec/batch\n",
      "Epoch: 4/30...  Training Step: 555...  Training loss: 2.1130...  0.1993 sec/batch\n",
      "Epoch: 4/30...  Training Step: 556...  Training loss: 2.1201...  0.1740 sec/batch\n",
      "Epoch: 4/30...  Training Step: 557...  Training loss: 2.1108...  0.1920 sec/batch\n",
      "Epoch: 4/30...  Training Step: 558...  Training loss: 2.1344...  0.2060 sec/batch\n",
      "Epoch: 4/30...  Training Step: 559...  Training loss: 2.1143...  0.1767 sec/batch\n",
      "Epoch: 4/30...  Training Step: 560...  Training loss: 2.1358...  0.3452 sec/batch\n",
      "Epoch: 4/30...  Training Step: 561...  Training loss: 2.1662...  0.2431 sec/batch\n",
      "Epoch: 4/30...  Training Step: 562...  Training loss: 2.1317...  0.2099 sec/batch\n",
      "Epoch: 4/30...  Training Step: 563...  Training loss: 2.0917...  0.2310 sec/batch\n",
      "Epoch: 4/30...  Training Step: 564...  Training loss: 2.1332...  0.2741 sec/batch\n",
      "Epoch: 4/30...  Training Step: 565...  Training loss: 2.0986...  0.2606 sec/batch\n",
      "Epoch: 4/30...  Training Step: 566...  Training loss: 2.1261...  0.1711 sec/batch\n",
      "Epoch: 4/30...  Training Step: 567...  Training loss: 2.1302...  0.2438 sec/batch\n",
      "Epoch: 4/30...  Training Step: 568...  Training loss: 2.0943...  0.2750 sec/batch\n",
      "Epoch: 4/30...  Training Step: 569...  Training loss: 2.1325...  0.2855 sec/batch\n",
      "Epoch: 4/30...  Training Step: 570...  Training loss: 2.1208...  0.2585 sec/batch\n",
      "Epoch: 4/30...  Training Step: 571...  Training loss: 2.1209...  0.2053 sec/batch\n",
      "Epoch: 4/30...  Training Step: 572...  Training loss: 2.1167...  0.2350 sec/batch\n",
      "Epoch: 4/30...  Training Step: 573...  Training loss: 2.0954...  0.2147 sec/batch\n",
      "Epoch: 4/30...  Training Step: 574...  Training loss: 2.0924...  0.1903 sec/batch\n",
      "Epoch: 4/30...  Training Step: 575...  Training loss: 2.1336...  0.1699 sec/batch\n",
      "Epoch: 4/30...  Training Step: 576...  Training loss: 2.1102...  0.2312 sec/batch\n",
      "Epoch: 4/30...  Training Step: 577...  Training loss: 2.1311...  0.2544 sec/batch\n",
      "Epoch: 4/30...  Training Step: 578...  Training loss: 2.0738...  0.2901 sec/batch\n",
      "Epoch: 4/30...  Training Step: 579...  Training loss: 2.1217...  0.2124 sec/batch\n",
      "Epoch: 4/30...  Training Step: 580...  Training loss: 2.0829...  0.2570 sec/batch\n",
      "Epoch: 4/30...  Training Step: 581...  Training loss: 2.0871...  0.2887 sec/batch\n",
      "Epoch: 4/30...  Training Step: 582...  Training loss: 2.0845...  0.2406 sec/batch\n",
      "Epoch: 4/30...  Training Step: 583...  Training loss: 2.1023...  0.2165 sec/batch\n",
      "Epoch: 4/30...  Training Step: 584...  Training loss: 2.1283...  0.2536 sec/batch\n",
      "Epoch: 4/30...  Training Step: 585...  Training loss: 2.0940...  0.2879 sec/batch\n",
      "Epoch: 4/30...  Training Step: 586...  Training loss: 2.0653...  0.2410 sec/batch\n",
      "Epoch: 4/30...  Training Step: 587...  Training loss: 2.0944...  0.2337 sec/batch\n",
      "Epoch: 4/30...  Training Step: 588...  Training loss: 2.0994...  0.2554 sec/batch\n",
      "Epoch: 4/30...  Training Step: 589...  Training loss: 2.0962...  0.2469 sec/batch\n",
      "Epoch: 4/30...  Training Step: 590...  Training loss: 2.1199...  0.2152 sec/batch\n",
      "Epoch: 4/30...  Training Step: 591...  Training loss: 2.1035...  0.2560 sec/batch\n",
      "Epoch: 4/30...  Training Step: 592...  Training loss: 2.1047...  0.2706 sec/batch\n",
      "Epoch: 4/30...  Training Step: 593...  Training loss: 2.0935...  0.2105 sec/batch\n",
      "Epoch: 4/30...  Training Step: 594...  Training loss: 2.0702...  0.2263 sec/batch\n",
      "Epoch: 4/30...  Training Step: 595...  Training loss: 2.1142...  0.1945 sec/batch\n",
      "Epoch: 4/30...  Training Step: 596...  Training loss: 2.0847...  0.2497 sec/batch\n",
      "Epoch: 4/30...  Training Step: 597...  Training loss: 2.0972...  0.2468 sec/batch\n",
      "Epoch: 4/30...  Training Step: 598...  Training loss: 2.1113...  0.2486 sec/batch\n",
      "Epoch: 4/30...  Training Step: 599...  Training loss: 2.0946...  0.2675 sec/batch\n",
      "Epoch: 4/30...  Training Step: 600...  Training loss: 2.0599...  0.1848 sec/batch\n",
      "Epoch: 4/30...  Training Step: 601...  Training loss: 2.0737...  0.1552 sec/batch\n",
      "Epoch: 4/30...  Training Step: 602...  Training loss: 2.0675...  0.2040 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/30...  Training Step: 603...  Training loss: 2.0972...  0.2297 sec/batch\n",
      "Epoch: 4/30...  Training Step: 604...  Training loss: 2.1233...  0.2472 sec/batch\n",
      "Epoch: 4/30...  Training Step: 605...  Training loss: 2.0740...  0.1830 sec/batch\n",
      "Epoch: 4/30...  Training Step: 606...  Training loss: 2.0579...  0.1722 sec/batch\n",
      "Epoch: 4/30...  Training Step: 607...  Training loss: 2.0699...  0.2218 sec/batch\n",
      "Epoch: 4/30...  Training Step: 608...  Training loss: 2.0788...  0.1553 sec/batch\n",
      "Epoch: 4/30...  Training Step: 609...  Training loss: 2.0810...  0.1989 sec/batch\n",
      "Epoch: 4/30...  Training Step: 610...  Training loss: 2.1181...  0.2434 sec/batch\n",
      "Epoch: 4/30...  Training Step: 611...  Training loss: 2.1126...  0.3072 sec/batch\n",
      "Epoch: 4/30...  Training Step: 612...  Training loss: 2.0646...  0.2683 sec/batch\n",
      "Epoch: 4/30...  Training Step: 613...  Training loss: 2.0645...  0.2703 sec/batch\n",
      "Epoch: 4/30...  Training Step: 614...  Training loss: 2.0584...  0.2516 sec/batch\n",
      "Epoch: 4/30...  Training Step: 615...  Training loss: 2.0676...  0.2628 sec/batch\n",
      "Epoch: 4/30...  Training Step: 616...  Training loss: 2.0575...  0.1978 sec/batch\n",
      "Epoch: 4/30...  Training Step: 617...  Training loss: 2.0670...  0.1576 sec/batch\n",
      "Epoch: 4/30...  Training Step: 618...  Training loss: 2.0451...  0.2435 sec/batch\n",
      "Epoch: 4/30...  Training Step: 619...  Training loss: 2.0587...  0.2489 sec/batch\n",
      "Epoch: 4/30...  Training Step: 620...  Training loss: 2.0603...  0.1557 sec/batch\n",
      "Epoch: 4/30...  Training Step: 621...  Training loss: 2.0459...  0.2312 sec/batch\n",
      "Epoch: 4/30...  Training Step: 622...  Training loss: 2.0318...  0.2690 sec/batch\n",
      "Epoch: 4/30...  Training Step: 623...  Training loss: 2.0533...  0.2589 sec/batch\n",
      "Epoch: 4/30...  Training Step: 624...  Training loss: 2.0798...  0.2025 sec/batch\n",
      "Epoch: 4/30...  Training Step: 625...  Training loss: 2.0773...  0.2708 sec/batch\n",
      "Epoch: 4/30...  Training Step: 626...  Training loss: 2.0639...  0.2388 sec/batch\n",
      "Epoch: 4/30...  Training Step: 627...  Training loss: 2.0664...  0.1743 sec/batch\n",
      "Epoch: 4/30...  Training Step: 628...  Training loss: 2.0626...  0.2546 sec/batch\n",
      "Epoch: 4/30...  Training Step: 629...  Training loss: 2.0392...  0.2398 sec/batch\n",
      "Epoch: 4/30...  Training Step: 630...  Training loss: 2.0740...  0.2454 sec/batch\n",
      "Epoch: 4/30...  Training Step: 631...  Training loss: 2.0219...  0.2666 sec/batch\n",
      "Epoch: 4/30...  Training Step: 632...  Training loss: 2.0480...  0.2717 sec/batch\n",
      "Epoch: 4/30...  Training Step: 633...  Training loss: 2.0567...  0.2457 sec/batch\n",
      "Epoch: 4/30...  Training Step: 634...  Training loss: 2.0808...  0.1548 sec/batch\n",
      "Epoch: 4/30...  Training Step: 635...  Training loss: 2.0439...  0.1840 sec/batch\n",
      "Epoch: 4/30...  Training Step: 636...  Training loss: 2.0513...  0.2105 sec/batch\n",
      "Epoch: 4/30...  Training Step: 637...  Training loss: 2.0859...  0.2324 sec/batch\n",
      "Epoch: 4/30...  Training Step: 638...  Training loss: 2.0523...  0.2575 sec/batch\n",
      "Epoch: 4/30...  Training Step: 639...  Training loss: 2.0735...  0.2614 sec/batch\n",
      "Epoch: 4/30...  Training Step: 640...  Training loss: 2.0832...  0.2880 sec/batch\n",
      "Epoch: 4/30...  Training Step: 641...  Training loss: 2.1111...  0.2254 sec/batch\n",
      "Epoch: 4/30...  Training Step: 642...  Training loss: 2.0282...  0.2831 sec/batch\n",
      "Epoch: 4/30...  Training Step: 643...  Training loss: 2.0357...  0.2690 sec/batch\n",
      "Epoch: 4/30...  Training Step: 644...  Training loss: 2.0679...  0.2723 sec/batch\n",
      "Epoch: 4/30...  Training Step: 645...  Training loss: 2.0489...  0.2575 sec/batch\n",
      "Epoch: 4/30...  Training Step: 646...  Training loss: 2.0838...  0.2686 sec/batch\n",
      "Epoch: 4/30...  Training Step: 647...  Training loss: 2.0154...  0.2280 sec/batch\n",
      "Epoch: 4/30...  Training Step: 648...  Training loss: 2.0375...  0.2637 sec/batch\n",
      "Epoch: 4/30...  Training Step: 649...  Training loss: 2.0228...  0.2460 sec/batch\n",
      "Epoch: 4/30...  Training Step: 650...  Training loss: 2.0293...  0.2071 sec/batch\n",
      "Epoch: 4/30...  Training Step: 651...  Training loss: 2.0384...  0.2253 sec/batch\n",
      "Epoch: 4/30...  Training Step: 652...  Training loss: 2.0375...  0.1593 sec/batch\n",
      "Epoch: 4/30...  Training Step: 653...  Training loss: 2.0136...  0.2662 sec/batch\n",
      "Epoch: 4/30...  Training Step: 654...  Training loss: 2.0303...  0.1790 sec/batch\n",
      "Epoch: 4/30...  Training Step: 655...  Training loss: 2.0253...  0.2173 sec/batch\n",
      "Epoch: 4/30...  Training Step: 656...  Training loss: 1.9966...  0.2717 sec/batch\n",
      "Epoch: 4/30...  Training Step: 657...  Training loss: 2.0563...  0.2802 sec/batch\n",
      "Epoch: 4/30...  Training Step: 658...  Training loss: 2.0332...  0.2418 sec/batch\n",
      "Epoch: 4/30...  Training Step: 659...  Training loss: 2.0363...  0.2355 sec/batch\n",
      "Epoch: 4/30...  Training Step: 660...  Training loss: 2.0362...  0.2723 sec/batch\n",
      "Epoch: 4/30...  Training Step: 661...  Training loss: 2.0643...  0.2498 sec/batch\n",
      "Epoch: 4/30...  Training Step: 662...  Training loss: 2.0491...  0.2486 sec/batch\n",
      "Epoch: 4/30...  Training Step: 663...  Training loss: 2.0456...  0.1682 sec/batch\n",
      "Epoch: 4/30...  Training Step: 664...  Training loss: 2.0062...  0.1978 sec/batch\n",
      "Epoch: 4/30...  Training Step: 665...  Training loss: 2.0219...  0.2384 sec/batch\n",
      "Epoch: 4/30...  Training Step: 666...  Training loss: 1.9809...  0.2504 sec/batch\n",
      "Epoch: 4/30...  Training Step: 667...  Training loss: 2.0217...  0.2773 sec/batch\n",
      "Epoch: 4/30...  Training Step: 668...  Training loss: 2.0444...  0.2707 sec/batch\n",
      "Epoch: 4/30...  Training Step: 669...  Training loss: 2.0030...  0.1977 sec/batch\n",
      "Epoch: 4/30...  Training Step: 670...  Training loss: 2.0381...  0.2095 sec/batch\n",
      "Epoch: 4/30...  Training Step: 671...  Training loss: 2.0289...  0.2799 sec/batch\n",
      "Epoch: 4/30...  Training Step: 672...  Training loss: 2.0375...  0.2030 sec/batch\n",
      "Epoch: 4/30...  Training Step: 673...  Training loss: 2.0277...  0.2338 sec/batch\n",
      "Epoch: 4/30...  Training Step: 674...  Training loss: 1.9955...  0.2173 sec/batch\n",
      "Epoch: 4/30...  Training Step: 675...  Training loss: 1.9939...  0.1820 sec/batch\n",
      "Epoch: 4/30...  Training Step: 676...  Training loss: 1.9839...  0.2179 sec/batch\n",
      "Epoch: 4/30...  Training Step: 677...  Training loss: 2.0057...  0.2616 sec/batch\n",
      "Epoch: 4/30...  Training Step: 678...  Training loss: 2.0060...  0.3050 sec/batch\n",
      "Epoch: 4/30...  Training Step: 679...  Training loss: 2.0364...  0.2082 sec/batch\n",
      "Epoch: 4/30...  Training Step: 680...  Training loss: 2.0014...  0.2602 sec/batch\n",
      "Epoch: 5/30...  Training Step: 681...  Training loss: 2.1102...  0.2621 sec/batch\n",
      "Epoch: 5/30...  Training Step: 682...  Training loss: 1.9904...  0.1646 sec/batch\n",
      "Epoch: 5/30...  Training Step: 683...  Training loss: 1.9999...  0.1810 sec/batch\n",
      "Epoch: 5/30...  Training Step: 684...  Training loss: 2.0446...  0.2446 sec/batch\n",
      "Epoch: 5/30...  Training Step: 685...  Training loss: 2.0231...  0.2375 sec/batch\n",
      "Epoch: 5/30...  Training Step: 686...  Training loss: 2.0352...  0.2208 sec/batch\n",
      "Epoch: 5/30...  Training Step: 687...  Training loss: 2.0079...  0.2329 sec/batch\n",
      "Epoch: 5/30...  Training Step: 688...  Training loss: 1.9873...  0.2252 sec/batch\n",
      "Epoch: 5/30...  Training Step: 689...  Training loss: 1.9762...  0.2969 sec/batch\n",
      "Epoch: 5/30...  Training Step: 690...  Training loss: 1.9853...  0.2219 sec/batch\n",
      "Epoch: 5/30...  Training Step: 691...  Training loss: 1.9718...  0.1736 sec/batch\n",
      "Epoch: 5/30...  Training Step: 692...  Training loss: 2.0075...  0.1544 sec/batch\n",
      "Epoch: 5/30...  Training Step: 693...  Training loss: 1.9897...  0.1849 sec/batch\n",
      "Epoch: 5/30...  Training Step: 694...  Training loss: 1.9809...  0.2944 sec/batch\n",
      "Epoch: 5/30...  Training Step: 695...  Training loss: 2.0323...  0.2526 sec/batch\n",
      "Epoch: 5/30...  Training Step: 696...  Training loss: 2.0228...  0.1976 sec/batch\n",
      "Epoch: 5/30...  Training Step: 697...  Training loss: 1.9697...  0.1551 sec/batch\n",
      "Epoch: 5/30...  Training Step: 698...  Training loss: 2.0105...  0.2162 sec/batch\n",
      "Epoch: 5/30...  Training Step: 699...  Training loss: 1.9961...  0.2093 sec/batch\n",
      "Epoch: 5/30...  Training Step: 700...  Training loss: 1.9872...  0.2280 sec/batch\n",
      "Epoch: 5/30...  Training Step: 701...  Training loss: 2.0002...  0.2180 sec/batch\n",
      "Epoch: 5/30...  Training Step: 702...  Training loss: 1.9547...  0.1773 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30...  Training Step: 703...  Training loss: 1.9769...  0.1838 sec/batch\n",
      "Epoch: 5/30...  Training Step: 704...  Training loss: 1.9390...  0.1582 sec/batch\n",
      "Epoch: 5/30...  Training Step: 705...  Training loss: 2.0007...  0.1948 sec/batch\n",
      "Epoch: 5/30...  Training Step: 706...  Training loss: 1.9868...  0.2859 sec/batch\n",
      "Epoch: 5/30...  Training Step: 707...  Training loss: 2.0248...  0.2783 sec/batch\n",
      "Epoch: 5/30...  Training Step: 708...  Training loss: 2.0001...  0.2503 sec/batch\n",
      "Epoch: 5/30...  Training Step: 709...  Training loss: 2.0001...  0.2172 sec/batch\n",
      "Epoch: 5/30...  Training Step: 710...  Training loss: 1.9527...  0.2699 sec/batch\n",
      "Epoch: 5/30...  Training Step: 711...  Training loss: 2.0038...  0.1817 sec/batch\n",
      "Epoch: 5/30...  Training Step: 712...  Training loss: 1.9890...  0.1626 sec/batch\n",
      "Epoch: 5/30...  Training Step: 713...  Training loss: 1.9686...  0.2474 sec/batch\n",
      "Epoch: 5/30...  Training Step: 714...  Training loss: 1.9490...  0.2766 sec/batch\n",
      "Epoch: 5/30...  Training Step: 715...  Training loss: 1.9825...  0.2415 sec/batch\n",
      "Epoch: 5/30...  Training Step: 716...  Training loss: 1.9862...  0.2060 sec/batch\n",
      "Epoch: 5/30...  Training Step: 717...  Training loss: 2.0194...  0.2573 sec/batch\n",
      "Epoch: 5/30...  Training Step: 718...  Training loss: 1.9868...  0.2682 sec/batch\n",
      "Epoch: 5/30...  Training Step: 719...  Training loss: 1.9907...  0.2677 sec/batch\n",
      "Epoch: 5/30...  Training Step: 720...  Training loss: 1.9952...  0.2685 sec/batch\n",
      "Epoch: 5/30...  Training Step: 721...  Training loss: 2.0127...  0.1671 sec/batch\n",
      "Epoch: 5/30...  Training Step: 722...  Training loss: 1.9793...  0.2221 sec/batch\n",
      "Epoch: 5/30...  Training Step: 723...  Training loss: 1.9530...  0.2063 sec/batch\n",
      "Epoch: 5/30...  Training Step: 724...  Training loss: 2.0186...  0.2555 sec/batch\n",
      "Epoch: 5/30...  Training Step: 725...  Training loss: 1.9796...  0.2941 sec/batch\n",
      "Epoch: 5/30...  Training Step: 726...  Training loss: 1.9691...  0.2414 sec/batch\n",
      "Epoch: 5/30...  Training Step: 727...  Training loss: 1.9868...  0.2118 sec/batch\n",
      "Epoch: 5/30...  Training Step: 728...  Training loss: 1.9970...  0.2328 sec/batch\n",
      "Epoch: 5/30...  Training Step: 729...  Training loss: 1.9872...  0.3191 sec/batch\n",
      "Epoch: 5/30...  Training Step: 730...  Training loss: 1.9852...  0.3759 sec/batch\n",
      "Epoch: 5/30...  Training Step: 731...  Training loss: 2.0229...  0.1714 sec/batch\n",
      "Epoch: 5/30...  Training Step: 732...  Training loss: 1.9762...  0.2347 sec/batch\n",
      "Epoch: 5/30...  Training Step: 733...  Training loss: 1.9556...  0.1578 sec/batch\n",
      "Epoch: 5/30...  Training Step: 734...  Training loss: 1.9847...  0.1711 sec/batch\n",
      "Epoch: 5/30...  Training Step: 735...  Training loss: 1.9573...  0.1917 sec/batch\n",
      "Epoch: 5/30...  Training Step: 736...  Training loss: 1.9970...  0.2740 sec/batch\n",
      "Epoch: 5/30...  Training Step: 737...  Training loss: 1.9937...  0.2568 sec/batch\n",
      "Epoch: 5/30...  Training Step: 738...  Training loss: 1.9559...  0.2707 sec/batch\n",
      "Epoch: 5/30...  Training Step: 739...  Training loss: 1.9882...  0.1887 sec/batch\n",
      "Epoch: 5/30...  Training Step: 740...  Training loss: 1.9783...  0.2178 sec/batch\n",
      "Epoch: 5/30...  Training Step: 741...  Training loss: 1.9887...  0.2335 sec/batch\n",
      "Epoch: 5/30...  Training Step: 742...  Training loss: 1.9802...  0.1965 sec/batch\n",
      "Epoch: 5/30...  Training Step: 743...  Training loss: 1.9608...  0.2695 sec/batch\n",
      "Epoch: 5/30...  Training Step: 744...  Training loss: 1.9483...  0.2652 sec/batch\n",
      "Epoch: 5/30...  Training Step: 745...  Training loss: 2.0103...  0.1784 sec/batch\n",
      "Epoch: 5/30...  Training Step: 746...  Training loss: 1.9721...  0.2396 sec/batch\n",
      "Epoch: 5/30...  Training Step: 747...  Training loss: 1.9912...  0.1839 sec/batch\n",
      "Epoch: 5/30...  Training Step: 748...  Training loss: 1.9375...  0.2249 sec/batch\n",
      "Epoch: 5/30...  Training Step: 749...  Training loss: 1.9822...  0.1683 sec/batch\n",
      "Epoch: 5/30...  Training Step: 750...  Training loss: 1.9693...  0.2400 sec/batch\n",
      "Epoch: 5/30...  Training Step: 751...  Training loss: 1.9349...  0.1672 sec/batch\n",
      "Epoch: 5/30...  Training Step: 752...  Training loss: 1.9560...  0.2755 sec/batch\n",
      "Epoch: 5/30...  Training Step: 753...  Training loss: 1.9603...  0.2713 sec/batch\n",
      "Epoch: 5/30...  Training Step: 754...  Training loss: 1.9983...  0.1785 sec/batch\n",
      "Epoch: 5/30...  Training Step: 755...  Training loss: 1.9615...  0.2742 sec/batch\n",
      "Epoch: 5/30...  Training Step: 756...  Training loss: 1.9396...  0.2738 sec/batch\n",
      "Epoch: 5/30...  Training Step: 757...  Training loss: 1.9523...  0.2762 sec/batch\n",
      "Epoch: 5/30...  Training Step: 758...  Training loss: 1.9799...  0.2239 sec/batch\n",
      "Epoch: 5/30...  Training Step: 759...  Training loss: 1.9734...  0.1916 sec/batch\n",
      "Epoch: 5/30...  Training Step: 760...  Training loss: 1.9941...  0.1901 sec/batch\n",
      "Epoch: 5/30...  Training Step: 761...  Training loss: 1.9715...  0.2599 sec/batch\n",
      "Epoch: 5/30...  Training Step: 762...  Training loss: 1.9766...  0.1771 sec/batch\n",
      "Epoch: 5/30...  Training Step: 763...  Training loss: 1.9658...  0.1587 sec/batch\n",
      "Epoch: 5/30...  Training Step: 764...  Training loss: 1.9421...  0.2047 sec/batch\n",
      "Epoch: 5/30...  Training Step: 765...  Training loss: 1.9764...  0.2347 sec/batch\n",
      "Epoch: 5/30...  Training Step: 766...  Training loss: 1.9399...  0.1627 sec/batch\n",
      "Epoch: 5/30...  Training Step: 767...  Training loss: 1.9707...  0.1940 sec/batch\n",
      "Epoch: 5/30...  Training Step: 768...  Training loss: 1.9799...  0.1824 sec/batch\n",
      "Epoch: 5/30...  Training Step: 769...  Training loss: 1.9705...  0.1719 sec/batch\n",
      "Epoch: 5/30...  Training Step: 770...  Training loss: 1.9262...  0.1657 sec/batch\n",
      "Epoch: 5/30...  Training Step: 771...  Training loss: 1.9370...  0.1571 sec/batch\n",
      "Epoch: 5/30...  Training Step: 772...  Training loss: 1.9264...  0.2020 sec/batch\n",
      "Epoch: 5/30...  Training Step: 773...  Training loss: 1.9688...  0.2589 sec/batch\n",
      "Epoch: 5/30...  Training Step: 774...  Training loss: 1.9842...  0.2836 sec/batch\n",
      "Epoch: 5/30...  Training Step: 775...  Training loss: 1.9495...  0.1676 sec/batch\n",
      "Epoch: 5/30...  Training Step: 776...  Training loss: 1.9129...  0.1886 sec/batch\n",
      "Epoch: 5/30...  Training Step: 777...  Training loss: 1.9144...  0.2477 sec/batch\n",
      "Epoch: 5/30...  Training Step: 778...  Training loss: 1.9435...  0.2733 sec/batch\n",
      "Epoch: 5/30...  Training Step: 779...  Training loss: 1.9378...  0.1903 sec/batch\n",
      "Epoch: 5/30...  Training Step: 780...  Training loss: 1.9900...  0.2308 sec/batch\n",
      "Epoch: 5/30...  Training Step: 781...  Training loss: 1.9732...  0.2507 sec/batch\n",
      "Epoch: 5/30...  Training Step: 782...  Training loss: 1.9236...  0.1824 sec/batch\n",
      "Epoch: 5/30...  Training Step: 783...  Training loss: 1.9400...  0.2648 sec/batch\n",
      "Epoch: 5/30...  Training Step: 784...  Training loss: 1.9395...  0.2609 sec/batch\n",
      "Epoch: 5/30...  Training Step: 785...  Training loss: 1.9407...  0.2335 sec/batch\n",
      "Epoch: 5/30...  Training Step: 786...  Training loss: 1.9323...  0.2544 sec/batch\n",
      "Epoch: 5/30...  Training Step: 787...  Training loss: 1.9403...  0.2525 sec/batch\n",
      "Epoch: 5/30...  Training Step: 788...  Training loss: 1.9132...  0.2570 sec/batch\n",
      "Epoch: 5/30...  Training Step: 789...  Training loss: 1.9493...  0.1755 sec/batch\n",
      "Epoch: 5/30...  Training Step: 790...  Training loss: 1.9389...  0.2349 sec/batch\n",
      "Epoch: 5/30...  Training Step: 791...  Training loss: 1.9235...  0.2198 sec/batch\n",
      "Epoch: 5/30...  Training Step: 792...  Training loss: 1.8920...  0.1721 sec/batch\n",
      "Epoch: 5/30...  Training Step: 793...  Training loss: 1.9293...  0.1667 sec/batch\n",
      "Epoch: 5/30...  Training Step: 794...  Training loss: 1.9498...  0.2529 sec/batch\n",
      "Epoch: 5/30...  Training Step: 795...  Training loss: 1.9541...  0.2616 sec/batch\n",
      "Epoch: 5/30...  Training Step: 796...  Training loss: 1.9311...  0.2743 sec/batch\n",
      "Epoch: 5/30...  Training Step: 797...  Training loss: 1.9301...  0.2529 sec/batch\n",
      "Epoch: 5/30...  Training Step: 798...  Training loss: 1.9427...  0.1879 sec/batch\n",
      "Epoch: 5/30...  Training Step: 799...  Training loss: 1.9133...  0.2401 sec/batch\n",
      "Epoch: 5/30...  Training Step: 800...  Training loss: 1.9388...  0.2445 sec/batch\n",
      "Epoch: 5/30...  Training Step: 801...  Training loss: 1.8915...  0.1888 sec/batch\n",
      "Epoch: 5/30...  Training Step: 802...  Training loss: 1.9165...  0.1615 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/30...  Training Step: 803...  Training loss: 1.9364...  0.2510 sec/batch\n",
      "Epoch: 5/30...  Training Step: 804...  Training loss: 1.9634...  0.2235 sec/batch\n",
      "Epoch: 5/30...  Training Step: 805...  Training loss: 1.9092...  0.2458 sec/batch\n",
      "Epoch: 5/30...  Training Step: 806...  Training loss: 1.9225...  0.2868 sec/batch\n",
      "Epoch: 5/30...  Training Step: 807...  Training loss: 1.9431...  0.2166 sec/batch\n",
      "Epoch: 5/30...  Training Step: 808...  Training loss: 1.9307...  0.1553 sec/batch\n",
      "Epoch: 5/30...  Training Step: 809...  Training loss: 1.9446...  0.1850 sec/batch\n",
      "Epoch: 5/30...  Training Step: 810...  Training loss: 1.9599...  0.1587 sec/batch\n",
      "Epoch: 5/30...  Training Step: 811...  Training loss: 1.9857...  0.2592 sec/batch\n",
      "Epoch: 5/30...  Training Step: 812...  Training loss: 1.9138...  0.2367 sec/batch\n",
      "Epoch: 5/30...  Training Step: 813...  Training loss: 1.9231...  0.2945 sec/batch\n",
      "Epoch: 5/30...  Training Step: 814...  Training loss: 1.9533...  0.2410 sec/batch\n",
      "Epoch: 5/30...  Training Step: 815...  Training loss: 1.9380...  0.2559 sec/batch\n",
      "Epoch: 5/30...  Training Step: 816...  Training loss: 1.9548...  0.2497 sec/batch\n",
      "Epoch: 5/30...  Training Step: 817...  Training loss: 1.8993...  0.2517 sec/batch\n",
      "Epoch: 5/30...  Training Step: 818...  Training loss: 1.9266...  0.2097 sec/batch\n",
      "Epoch: 5/30...  Training Step: 819...  Training loss: 1.9066...  0.2900 sec/batch\n",
      "Epoch: 5/30...  Training Step: 820...  Training loss: 1.9058...  0.3022 sec/batch\n",
      "Epoch: 5/30...  Training Step: 821...  Training loss: 1.9181...  0.1722 sec/batch\n",
      "Epoch: 5/30...  Training Step: 822...  Training loss: 1.9172...  0.2017 sec/batch\n",
      "Epoch: 5/30...  Training Step: 823...  Training loss: 1.8895...  0.1595 sec/batch\n",
      "Epoch: 5/30...  Training Step: 824...  Training loss: 1.9128...  0.1938 sec/batch\n",
      "Epoch: 5/30...  Training Step: 825...  Training loss: 1.8990...  0.2619 sec/batch\n",
      "Epoch: 5/30...  Training Step: 826...  Training loss: 1.8709...  0.2696 sec/batch\n",
      "Epoch: 5/30...  Training Step: 827...  Training loss: 1.9400...  0.1990 sec/batch\n",
      "Epoch: 5/30...  Training Step: 828...  Training loss: 1.9109...  0.1607 sec/batch\n",
      "Epoch: 5/30...  Training Step: 829...  Training loss: 1.9103...  0.2574 sec/batch\n",
      "Epoch: 5/30...  Training Step: 830...  Training loss: 1.9137...  0.1950 sec/batch\n",
      "Epoch: 5/30...  Training Step: 831...  Training loss: 1.9280...  0.2698 sec/batch\n",
      "Epoch: 5/30...  Training Step: 832...  Training loss: 1.9252...  0.2395 sec/batch\n",
      "Epoch: 5/30...  Training Step: 833...  Training loss: 1.9178...  0.1925 sec/batch\n",
      "Epoch: 5/30...  Training Step: 834...  Training loss: 1.8826...  0.2768 sec/batch\n",
      "Epoch: 5/30...  Training Step: 835...  Training loss: 1.8890...  0.3275 sec/batch\n",
      "Epoch: 5/30...  Training Step: 836...  Training loss: 1.8545...  0.1865 sec/batch\n",
      "Epoch: 5/30...  Training Step: 837...  Training loss: 1.8888...  0.2538 sec/batch\n",
      "Epoch: 5/30...  Training Step: 838...  Training loss: 1.9240...  0.2292 sec/batch\n",
      "Epoch: 5/30...  Training Step: 839...  Training loss: 1.8971...  0.1569 sec/batch\n",
      "Epoch: 5/30...  Training Step: 840...  Training loss: 1.9246...  0.1727 sec/batch\n",
      "Epoch: 5/30...  Training Step: 841...  Training loss: 1.9007...  0.1700 sec/batch\n",
      "Epoch: 5/30...  Training Step: 842...  Training loss: 1.9199...  0.2107 sec/batch\n",
      "Epoch: 5/30...  Training Step: 843...  Training loss: 1.9027...  0.1896 sec/batch\n",
      "Epoch: 5/30...  Training Step: 844...  Training loss: 1.8912...  0.2833 sec/batch\n",
      "Epoch: 5/30...  Training Step: 845...  Training loss: 1.8799...  0.2855 sec/batch\n",
      "Epoch: 5/30...  Training Step: 846...  Training loss: 1.8783...  0.2847 sec/batch\n",
      "Epoch: 5/30...  Training Step: 847...  Training loss: 1.8808...  0.2718 sec/batch\n",
      "Epoch: 5/30...  Training Step: 848...  Training loss: 1.8964...  0.1996 sec/batch\n",
      "Epoch: 5/30...  Training Step: 849...  Training loss: 1.9144...  0.1575 sec/batch\n",
      "Epoch: 5/30...  Training Step: 850...  Training loss: 1.8776...  0.2337 sec/batch\n",
      "Epoch: 6/30...  Training Step: 851...  Training loss: 1.9808...  0.2920 sec/batch\n",
      "Epoch: 6/30...  Training Step: 852...  Training loss: 1.8682...  0.2208 sec/batch\n",
      "Epoch: 6/30...  Training Step: 853...  Training loss: 1.8851...  0.1540 sec/batch\n",
      "Epoch: 6/30...  Training Step: 854...  Training loss: 1.9198...  0.1817 sec/batch\n",
      "Epoch: 6/30...  Training Step: 855...  Training loss: 1.9135...  0.2211 sec/batch\n",
      "Epoch: 6/30...  Training Step: 856...  Training loss: 1.9196...  0.1974 sec/batch\n",
      "Epoch: 6/30...  Training Step: 857...  Training loss: 1.8992...  0.1784 sec/batch\n",
      "Epoch: 6/30...  Training Step: 858...  Training loss: 1.8761...  0.2215 sec/batch\n",
      "Epoch: 6/30...  Training Step: 859...  Training loss: 1.8576...  0.1659 sec/batch\n",
      "Epoch: 6/30...  Training Step: 860...  Training loss: 1.8635...  0.2051 sec/batch\n",
      "Epoch: 6/30...  Training Step: 861...  Training loss: 1.8667...  0.2336 sec/batch\n",
      "Epoch: 6/30...  Training Step: 862...  Training loss: 1.8879...  0.2217 sec/batch\n",
      "Epoch: 6/30...  Training Step: 863...  Training loss: 1.8727...  0.2010 sec/batch\n",
      "Epoch: 6/30...  Training Step: 864...  Training loss: 1.8754...  0.2673 sec/batch\n",
      "Epoch: 6/30...  Training Step: 865...  Training loss: 1.9323...  0.2156 sec/batch\n",
      "Epoch: 6/30...  Training Step: 866...  Training loss: 1.9269...  0.2402 sec/batch\n",
      "Epoch: 6/30...  Training Step: 867...  Training loss: 1.8565...  0.2275 sec/batch\n",
      "Epoch: 6/30...  Training Step: 868...  Training loss: 1.8975...  0.1729 sec/batch\n",
      "Epoch: 6/30...  Training Step: 869...  Training loss: 1.8921...  0.2175 sec/batch\n",
      "Epoch: 6/30...  Training Step: 870...  Training loss: 1.8747...  0.1563 sec/batch\n",
      "Epoch: 6/30...  Training Step: 871...  Training loss: 1.8855...  0.2394 sec/batch\n",
      "Epoch: 6/30...  Training Step: 872...  Training loss: 1.8422...  0.1935 sec/batch\n",
      "Epoch: 6/30...  Training Step: 873...  Training loss: 1.8472...  0.1584 sec/batch\n",
      "Epoch: 6/30...  Training Step: 874...  Training loss: 1.8314...  0.1701 sec/batch\n",
      "Epoch: 6/30...  Training Step: 875...  Training loss: 1.8990...  0.1686 sec/batch\n",
      "Epoch: 6/30...  Training Step: 876...  Training loss: 1.8739...  0.2409 sec/batch\n",
      "Epoch: 6/30...  Training Step: 877...  Training loss: 1.9088...  0.1863 sec/batch\n",
      "Epoch: 6/30...  Training Step: 878...  Training loss: 1.8988...  0.2547 sec/batch\n",
      "Epoch: 6/30...  Training Step: 879...  Training loss: 1.8809...  0.2762 sec/batch\n",
      "Epoch: 6/30...  Training Step: 880...  Training loss: 1.8259...  0.1832 sec/batch\n",
      "Epoch: 6/30...  Training Step: 881...  Training loss: 1.8727...  0.2404 sec/batch\n",
      "Epoch: 6/30...  Training Step: 882...  Training loss: 1.8686...  0.1528 sec/batch\n",
      "Epoch: 6/30...  Training Step: 883...  Training loss: 1.8699...  0.1569 sec/batch\n",
      "Epoch: 6/30...  Training Step: 884...  Training loss: 1.8423...  0.1956 sec/batch\n",
      "Epoch: 6/30...  Training Step: 885...  Training loss: 1.8584...  0.2333 sec/batch\n",
      "Epoch: 6/30...  Training Step: 886...  Training loss: 1.8825...  0.1733 sec/batch\n",
      "Epoch: 6/30...  Training Step: 887...  Training loss: 1.9112...  0.1682 sec/batch\n",
      "Epoch: 6/30...  Training Step: 888...  Training loss: 1.8730...  0.1752 sec/batch\n",
      "Epoch: 6/30...  Training Step: 889...  Training loss: 1.8806...  0.1748 sec/batch\n",
      "Epoch: 6/30...  Training Step: 890...  Training loss: 1.8850...  0.1691 sec/batch\n",
      "Epoch: 6/30...  Training Step: 891...  Training loss: 1.8983...  0.2216 sec/batch\n",
      "Epoch: 6/30...  Training Step: 892...  Training loss: 1.8534...  0.1772 sec/batch\n",
      "Epoch: 6/30...  Training Step: 893...  Training loss: 1.8444...  0.1768 sec/batch\n",
      "Epoch: 6/30...  Training Step: 894...  Training loss: 1.9170...  0.2733 sec/batch\n",
      "Epoch: 6/30...  Training Step: 895...  Training loss: 1.8756...  0.2143 sec/batch\n",
      "Epoch: 6/30...  Training Step: 896...  Training loss: 1.8591...  0.2202 sec/batch\n",
      "Epoch: 6/30...  Training Step: 897...  Training loss: 1.8835...  0.1693 sec/batch\n",
      "Epoch: 6/30...  Training Step: 898...  Training loss: 1.8843...  0.2245 sec/batch\n",
      "Epoch: 6/30...  Training Step: 899...  Training loss: 1.8706...  0.1856 sec/batch\n",
      "Epoch: 6/30...  Training Step: 900...  Training loss: 1.8857...  0.2236 sec/batch\n",
      "Epoch: 6/30...  Training Step: 901...  Training loss: 1.9281...  0.2521 sec/batch\n",
      "Epoch: 6/30...  Training Step: 902...  Training loss: 1.8609...  0.1838 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30...  Training Step: 903...  Training loss: 1.8460...  0.2285 sec/batch\n",
      "Epoch: 6/30...  Training Step: 904...  Training loss: 1.8767...  0.3295 sec/batch\n",
      "Epoch: 6/30...  Training Step: 905...  Training loss: 1.8567...  0.1976 sec/batch\n",
      "Epoch: 6/30...  Training Step: 906...  Training loss: 1.8901...  0.2687 sec/batch\n",
      "Epoch: 6/30...  Training Step: 907...  Training loss: 1.8780...  0.2934 sec/batch\n",
      "Epoch: 6/30...  Training Step: 908...  Training loss: 1.8365...  0.2598 sec/batch\n",
      "Epoch: 6/30...  Training Step: 909...  Training loss: 1.8759...  0.2275 sec/batch\n",
      "Epoch: 6/30...  Training Step: 910...  Training loss: 1.8647...  0.2554 sec/batch\n",
      "Epoch: 6/30...  Training Step: 911...  Training loss: 1.8715...  0.2484 sec/batch\n",
      "Epoch: 6/30...  Training Step: 912...  Training loss: 1.8606...  0.2955 sec/batch\n",
      "Epoch: 6/30...  Training Step: 913...  Training loss: 1.8457...  0.2638 sec/batch\n",
      "Epoch: 6/30...  Training Step: 914...  Training loss: 1.8530...  0.2652 sec/batch\n",
      "Epoch: 6/30...  Training Step: 915...  Training loss: 1.9009...  0.2574 sec/batch\n",
      "Epoch: 6/30...  Training Step: 916...  Training loss: 1.8824...  0.2175 sec/batch\n",
      "Epoch: 6/30...  Training Step: 917...  Training loss: 1.8941...  0.2315 sec/batch\n",
      "Epoch: 6/30...  Training Step: 918...  Training loss: 1.8370...  0.2840 sec/batch\n",
      "Epoch: 6/30...  Training Step: 919...  Training loss: 1.8619...  0.2519 sec/batch\n",
      "Epoch: 6/30...  Training Step: 920...  Training loss: 1.8567...  0.2568 sec/batch\n",
      "Epoch: 6/30...  Training Step: 921...  Training loss: 1.8350...  0.1852 sec/batch\n",
      "Epoch: 6/30...  Training Step: 922...  Training loss: 1.8560...  0.2630 sec/batch\n",
      "Epoch: 6/30...  Training Step: 923...  Training loss: 1.8621...  0.2496 sec/batch\n",
      "Epoch: 6/30...  Training Step: 924...  Training loss: 1.8874...  0.2146 sec/batch\n",
      "Epoch: 6/30...  Training Step: 925...  Training loss: 1.8653...  0.1996 sec/batch\n",
      "Epoch: 6/30...  Training Step: 926...  Training loss: 1.8219...  0.2148 sec/batch\n",
      "Epoch: 6/30...  Training Step: 927...  Training loss: 1.8408...  0.2806 sec/batch\n",
      "Epoch: 6/30...  Training Step: 928...  Training loss: 1.8629...  0.1837 sec/batch\n",
      "Epoch: 6/30...  Training Step: 929...  Training loss: 1.8736...  0.1611 sec/batch\n",
      "Epoch: 6/30...  Training Step: 930...  Training loss: 1.8789...  0.2050 sec/batch\n",
      "Epoch: 6/30...  Training Step: 931...  Training loss: 1.8699...  0.1990 sec/batch\n",
      "Epoch: 6/30...  Training Step: 932...  Training loss: 1.8768...  0.2414 sec/batch\n",
      "Epoch: 6/30...  Training Step: 933...  Training loss: 1.8675...  0.2342 sec/batch\n",
      "Epoch: 6/30...  Training Step: 934...  Training loss: 1.8421...  0.2716 sec/batch\n",
      "Epoch: 6/30...  Training Step: 935...  Training loss: 1.8685...  0.3057 sec/batch\n",
      "Epoch: 6/30...  Training Step: 936...  Training loss: 1.8364...  0.2436 sec/batch\n",
      "Epoch: 6/30...  Training Step: 937...  Training loss: 1.8713...  0.2430 sec/batch\n",
      "Epoch: 6/30...  Training Step: 938...  Training loss: 1.8702...  0.1677 sec/batch\n",
      "Epoch: 6/30...  Training Step: 939...  Training loss: 1.8718...  0.2733 sec/batch\n",
      "Epoch: 6/30...  Training Step: 940...  Training loss: 1.8158...  0.2863 sec/batch\n",
      "Epoch: 6/30...  Training Step: 941...  Training loss: 1.8469...  0.2767 sec/batch\n",
      "Epoch: 6/30...  Training Step: 942...  Training loss: 1.8247...  0.2752 sec/batch\n",
      "Epoch: 6/30...  Training Step: 943...  Training loss: 1.8740...  0.2280 sec/batch\n",
      "Epoch: 6/30...  Training Step: 944...  Training loss: 1.8918...  0.2399 sec/batch\n",
      "Epoch: 6/30...  Training Step: 945...  Training loss: 1.8431...  0.1593 sec/batch\n",
      "Epoch: 6/30...  Training Step: 946...  Training loss: 1.8136...  0.3045 sec/batch\n",
      "Epoch: 6/30...  Training Step: 947...  Training loss: 1.8171...  0.2197 sec/batch\n",
      "Epoch: 6/30...  Training Step: 948...  Training loss: 1.8350...  0.3147 sec/batch\n",
      "Epoch: 6/30...  Training Step: 949...  Training loss: 1.8330...  0.1989 sec/batch\n",
      "Epoch: 6/30...  Training Step: 950...  Training loss: 1.8844...  0.1763 sec/batch\n",
      "Epoch: 6/30...  Training Step: 951...  Training loss: 1.8747...  0.2586 sec/batch\n",
      "Epoch: 6/30...  Training Step: 952...  Training loss: 1.8372...  0.2719 sec/batch\n",
      "Epoch: 6/30...  Training Step: 953...  Training loss: 1.8450...  0.2308 sec/batch\n",
      "Epoch: 6/30...  Training Step: 954...  Training loss: 1.8316...  0.3217 sec/batch\n",
      "Epoch: 6/30...  Training Step: 955...  Training loss: 1.8377...  0.2483 sec/batch\n",
      "Epoch: 6/30...  Training Step: 956...  Training loss: 1.8285...  0.2854 sec/batch\n",
      "Epoch: 6/30...  Training Step: 957...  Training loss: 1.8339...  0.1911 sec/batch\n",
      "Epoch: 6/30...  Training Step: 958...  Training loss: 1.8169...  0.2603 sec/batch\n",
      "Epoch: 6/30...  Training Step: 959...  Training loss: 1.8521...  0.2542 sec/batch\n",
      "Epoch: 6/30...  Training Step: 960...  Training loss: 1.8286...  0.2306 sec/batch\n",
      "Epoch: 6/30...  Training Step: 961...  Training loss: 1.8296...  0.1716 sec/batch\n",
      "Epoch: 6/30...  Training Step: 962...  Training loss: 1.8059...  0.2004 sec/batch\n",
      "Epoch: 6/30...  Training Step: 963...  Training loss: 1.8286...  0.1727 sec/batch\n",
      "Epoch: 6/30...  Training Step: 964...  Training loss: 1.8595...  0.2740 sec/batch\n",
      "Epoch: 6/30...  Training Step: 965...  Training loss: 1.8591...  0.1620 sec/batch\n",
      "Epoch: 6/30...  Training Step: 966...  Training loss: 1.8421...  0.1930 sec/batch\n",
      "Epoch: 6/30...  Training Step: 967...  Training loss: 1.8301...  0.1704 sec/batch\n",
      "Epoch: 6/30...  Training Step: 968...  Training loss: 1.8421...  0.1623 sec/batch\n",
      "Epoch: 6/30...  Training Step: 969...  Training loss: 1.8081...  0.1896 sec/batch\n",
      "Epoch: 6/30...  Training Step: 970...  Training loss: 1.8575...  0.1727 sec/batch\n",
      "Epoch: 6/30...  Training Step: 971...  Training loss: 1.8046...  0.1752 sec/batch\n",
      "Epoch: 6/30...  Training Step: 972...  Training loss: 1.8164...  0.2310 sec/batch\n",
      "Epoch: 6/30...  Training Step: 973...  Training loss: 1.8370...  0.1980 sec/batch\n",
      "Epoch: 6/30...  Training Step: 974...  Training loss: 1.8642...  0.2349 sec/batch\n",
      "Epoch: 6/30...  Training Step: 975...  Training loss: 1.8132...  0.1964 sec/batch\n",
      "Epoch: 6/30...  Training Step: 976...  Training loss: 1.8251...  0.2467 sec/batch\n",
      "Epoch: 6/30...  Training Step: 977...  Training loss: 1.8484...  0.1575 sec/batch\n",
      "Epoch: 6/30...  Training Step: 978...  Training loss: 1.8291...  0.1731 sec/batch\n",
      "Epoch: 6/30...  Training Step: 979...  Training loss: 1.8363...  0.1857 sec/batch\n",
      "Epoch: 6/30...  Training Step: 980...  Training loss: 1.8624...  0.2994 sec/batch\n",
      "Epoch: 6/30...  Training Step: 981...  Training loss: 1.8822...  0.2626 sec/batch\n",
      "Epoch: 6/30...  Training Step: 982...  Training loss: 1.8030...  0.2874 sec/batch\n",
      "Epoch: 6/30...  Training Step: 983...  Training loss: 1.8164...  0.2627 sec/batch\n",
      "Epoch: 6/30...  Training Step: 984...  Training loss: 1.8556...  0.2445 sec/batch\n",
      "Epoch: 6/30...  Training Step: 985...  Training loss: 1.8396...  0.2094 sec/batch\n",
      "Epoch: 6/30...  Training Step: 986...  Training loss: 1.8695...  0.1624 sec/batch\n",
      "Epoch: 6/30...  Training Step: 987...  Training loss: 1.7914...  0.2139 sec/batch\n",
      "Epoch: 6/30...  Training Step: 988...  Training loss: 1.8203...  0.2477 sec/batch\n",
      "Epoch: 6/30...  Training Step: 989...  Training loss: 1.8087...  0.2392 sec/batch\n",
      "Epoch: 6/30...  Training Step: 990...  Training loss: 1.8168...  0.2118 sec/batch\n",
      "Epoch: 6/30...  Training Step: 991...  Training loss: 1.8135...  0.1874 sec/batch\n",
      "Epoch: 6/30...  Training Step: 992...  Training loss: 1.8287...  0.1674 sec/batch\n",
      "Epoch: 6/30...  Training Step: 993...  Training loss: 1.7969...  0.2681 sec/batch\n",
      "Epoch: 6/30...  Training Step: 994...  Training loss: 1.8268...  0.2136 sec/batch\n",
      "Epoch: 6/30...  Training Step: 995...  Training loss: 1.8096...  0.2437 sec/batch\n",
      "Epoch: 6/30...  Training Step: 996...  Training loss: 1.7731...  0.1860 sec/batch\n",
      "Epoch: 6/30...  Training Step: 997...  Training loss: 1.8395...  0.2594 sec/batch\n",
      "Epoch: 6/30...  Training Step: 998...  Training loss: 1.8212...  0.2222 sec/batch\n",
      "Epoch: 6/30...  Training Step: 999...  Training loss: 1.8157...  0.1516 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1000...  Training loss: 1.8203...  0.2818 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1001...  Training loss: 1.8320...  0.2609 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1002...  Training loss: 1.8373...  0.2059 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1003...  Training loss: 1.8340...  0.1585 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/30...  Training Step: 1004...  Training loss: 1.7936...  0.1634 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1005...  Training loss: 1.8112...  0.3037 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1006...  Training loss: 1.7750...  0.1747 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1007...  Training loss: 1.8102...  0.1588 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1008...  Training loss: 1.8396...  0.1782 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1009...  Training loss: 1.8021...  0.2616 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1010...  Training loss: 1.8427...  0.2002 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1011...  Training loss: 1.8153...  0.2303 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1012...  Training loss: 1.8340...  0.2393 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1013...  Training loss: 1.8255...  0.2741 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1014...  Training loss: 1.7919...  0.2983 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1015...  Training loss: 1.7877...  0.2583 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1016...  Training loss: 1.7984...  0.2662 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1017...  Training loss: 1.7970...  0.2555 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1018...  Training loss: 1.7984...  0.2781 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1019...  Training loss: 1.8365...  0.2544 sec/batch\n",
      "Epoch: 6/30...  Training Step: 1020...  Training loss: 1.7936...  0.2564 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1021...  Training loss: 1.8975...  0.2974 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1022...  Training loss: 1.7879...  0.2605 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1023...  Training loss: 1.7891...  0.2586 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1024...  Training loss: 1.8278...  0.2606 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1025...  Training loss: 1.8086...  0.2432 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1026...  Training loss: 1.8305...  0.2660 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1027...  Training loss: 1.8146...  0.2261 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1028...  Training loss: 1.7756...  0.1588 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1029...  Training loss: 1.7652...  0.1798 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1030...  Training loss: 1.7725...  0.2428 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1031...  Training loss: 1.7663...  0.2373 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1032...  Training loss: 1.8060...  0.1974 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1033...  Training loss: 1.7882...  0.2603 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1034...  Training loss: 1.7866...  0.2850 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1035...  Training loss: 1.8389...  0.2162 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1036...  Training loss: 1.8431...  0.2847 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1037...  Training loss: 1.7800...  0.2506 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1038...  Training loss: 1.8009...  0.2572 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1039...  Training loss: 1.8002...  0.2331 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1040...  Training loss: 1.7882...  0.2676 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1041...  Training loss: 1.7929...  0.2454 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1042...  Training loss: 1.7606...  0.2947 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1043...  Training loss: 1.7636...  0.2864 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1044...  Training loss: 1.7468...  0.2958 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1045...  Training loss: 1.8215...  0.2519 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1046...  Training loss: 1.7838...  0.2591 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1047...  Training loss: 1.8238...  0.2607 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1048...  Training loss: 1.8068...  0.2800 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1049...  Training loss: 1.7916...  0.2672 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1050...  Training loss: 1.7475...  0.1952 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1051...  Training loss: 1.7946...  0.2749 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1052...  Training loss: 1.7806...  0.1728 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1053...  Training loss: 1.7677...  0.2171 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1054...  Training loss: 1.7678...  0.2056 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1055...  Training loss: 1.7698...  0.1633 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1056...  Training loss: 1.7963...  0.2300 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1057...  Training loss: 1.8288...  0.2881 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1058...  Training loss: 1.7957...  0.2004 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1059...  Training loss: 1.7975...  0.1617 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1060...  Training loss: 1.7983...  0.1896 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1061...  Training loss: 1.8138...  0.2023 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1062...  Training loss: 1.7710...  0.2700 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1063...  Training loss: 1.7643...  0.2620 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1064...  Training loss: 1.8210...  0.2186 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1065...  Training loss: 1.8003...  0.1604 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1066...  Training loss: 1.7809...  0.2273 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1067...  Training loss: 1.8016...  0.1576 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1068...  Training loss: 1.7988...  0.2924 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1069...  Training loss: 1.7876...  0.3089 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1070...  Training loss: 1.7914...  0.2134 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1071...  Training loss: 1.8483...  0.1909 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1072...  Training loss: 1.7747...  0.2834 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1073...  Training loss: 1.7647...  0.3108 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1074...  Training loss: 1.7742...  0.1886 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1075...  Training loss: 1.7820...  0.1753 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1076...  Training loss: 1.8113...  0.2210 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1077...  Training loss: 1.7983...  0.2271 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1078...  Training loss: 1.7608...  0.2265 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1079...  Training loss: 1.8085...  0.2050 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1080...  Training loss: 1.7883...  0.1564 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1081...  Training loss: 1.8092...  0.1872 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1082...  Training loss: 1.7840...  0.1787 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1083...  Training loss: 1.7720...  0.2635 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1084...  Training loss: 1.7828...  0.1833 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1085...  Training loss: 1.8146...  0.2260 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1086...  Training loss: 1.7972...  0.2679 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1087...  Training loss: 1.8143...  0.2515 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1088...  Training loss: 1.7601...  0.2878 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1089...  Training loss: 1.7920...  0.2588 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1090...  Training loss: 1.7718...  0.2064 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1091...  Training loss: 1.7515...  0.1911 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1092...  Training loss: 1.7792...  0.2021 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1093...  Training loss: 1.7879...  0.1772 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1094...  Training loss: 1.8121...  0.1598 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1095...  Training loss: 1.7856...  0.2066 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1096...  Training loss: 1.7517...  0.1825 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1097...  Training loss: 1.7746...  0.2512 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1098...  Training loss: 1.7853...  0.1675 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1099...  Training loss: 1.7949...  0.2724 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1100...  Training loss: 1.7964...  0.2571 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1101...  Training loss: 1.7955...  0.2574 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1102...  Training loss: 1.7889...  0.1577 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/30...  Training Step: 1103...  Training loss: 1.7879...  0.2512 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1104...  Training loss: 1.7659...  0.2025 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1105...  Training loss: 1.7872...  0.2601 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1106...  Training loss: 1.7565...  0.2553 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1107...  Training loss: 1.7911...  0.2905 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1108...  Training loss: 1.7878...  0.2540 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1109...  Training loss: 1.7903...  0.2511 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1110...  Training loss: 1.7330...  0.2232 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1111...  Training loss: 1.7602...  0.2227 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1112...  Training loss: 1.7500...  0.1785 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1113...  Training loss: 1.7902...  0.2911 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1114...  Training loss: 1.8060...  0.2552 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1115...  Training loss: 1.7742...  0.3331 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1116...  Training loss: 1.7383...  0.2340 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1117...  Training loss: 1.7366...  0.3017 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1118...  Training loss: 1.7674...  0.2587 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1119...  Training loss: 1.7618...  0.2710 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1120...  Training loss: 1.8020...  0.3199 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1121...  Training loss: 1.8107...  0.2820 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1122...  Training loss: 1.7666...  0.1932 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1123...  Training loss: 1.7636...  0.1897 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1124...  Training loss: 1.7712...  0.2947 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1125...  Training loss: 1.7591...  0.1685 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1126...  Training loss: 1.7528...  0.2028 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1127...  Training loss: 1.7745...  0.2658 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1128...  Training loss: 1.7390...  0.3351 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1129...  Training loss: 1.7846...  0.2327 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1130...  Training loss: 1.7600...  0.1631 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1131...  Training loss: 1.7359...  0.3498 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1132...  Training loss: 1.7275...  0.2041 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1133...  Training loss: 1.7452...  0.2506 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1134...  Training loss: 1.7763...  0.2640 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1135...  Training loss: 1.7720...  0.2122 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1136...  Training loss: 1.7589...  0.2716 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1137...  Training loss: 1.7461...  0.2662 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1138...  Training loss: 1.7801...  0.1621 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1139...  Training loss: 1.7326...  0.1750 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1140...  Training loss: 1.7734...  0.1859 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1141...  Training loss: 1.7319...  0.3224 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1142...  Training loss: 1.7500...  0.1575 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1143...  Training loss: 1.7612...  0.2314 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1144...  Training loss: 1.7923...  0.2793 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1145...  Training loss: 1.7306...  0.2639 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1146...  Training loss: 1.7408...  0.1768 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1147...  Training loss: 1.7721...  0.1711 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1148...  Training loss: 1.7511...  0.2543 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1149...  Training loss: 1.7679...  0.1794 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1150...  Training loss: 1.7933...  0.1937 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1151...  Training loss: 1.8090...  0.2557 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1152...  Training loss: 1.7288...  0.1847 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1153...  Training loss: 1.7429...  0.1965 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1154...  Training loss: 1.7828...  0.2606 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1155...  Training loss: 1.7707...  0.2714 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1156...  Training loss: 1.8011...  0.2547 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1157...  Training loss: 1.7221...  0.1719 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1158...  Training loss: 1.7486...  0.1812 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1159...  Training loss: 1.7375...  0.1819 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1160...  Training loss: 1.7435...  0.2916 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1161...  Training loss: 1.7411...  0.1835 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1162...  Training loss: 1.7562...  0.1774 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1163...  Training loss: 1.7296...  0.1786 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1164...  Training loss: 1.7453...  0.2008 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1165...  Training loss: 1.7289...  0.2732 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1166...  Training loss: 1.6981...  0.2698 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1167...  Training loss: 1.7720...  0.2894 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1168...  Training loss: 1.7497...  0.1950 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1169...  Training loss: 1.7439...  0.2958 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1170...  Training loss: 1.7436...  0.2007 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1171...  Training loss: 1.7560...  0.2039 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1172...  Training loss: 1.7680...  0.2115 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1173...  Training loss: 1.7422...  0.2387 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1174...  Training loss: 1.7229...  0.2699 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1175...  Training loss: 1.7221...  0.2585 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1176...  Training loss: 1.6977...  0.2506 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1177...  Training loss: 1.7222...  0.1743 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1178...  Training loss: 1.7606...  0.2058 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1179...  Training loss: 1.7236...  0.2500 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1180...  Training loss: 1.7692...  0.1642 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1181...  Training loss: 1.7303...  0.1938 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1182...  Training loss: 1.7716...  0.1790 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1183...  Training loss: 1.7554...  0.1792 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1184...  Training loss: 1.7264...  0.2823 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1185...  Training loss: 1.7130...  0.2863 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1186...  Training loss: 1.7107...  0.2107 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1187...  Training loss: 1.7244...  0.1810 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1188...  Training loss: 1.7359...  0.1671 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1189...  Training loss: 1.7556...  0.1709 sec/batch\n",
      "Epoch: 7/30...  Training Step: 1190...  Training loss: 1.7051...  0.1842 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1191...  Training loss: 1.8083...  0.2641 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1192...  Training loss: 1.7123...  0.1835 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1193...  Training loss: 1.7175...  0.1713 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1194...  Training loss: 1.7608...  0.2482 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1195...  Training loss: 1.7468...  0.3304 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1196...  Training loss: 1.7695...  0.2684 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1197...  Training loss: 1.7360...  0.1653 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1198...  Training loss: 1.7153...  0.1783 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1199...  Training loss: 1.7028...  0.2894 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1200...  Training loss: 1.6992...  0.2004 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1201...  Training loss: 1.6953...  0.1907 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30...  Training Step: 1202...  Training loss: 1.7208...  0.1998 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1203...  Training loss: 1.7180...  0.1656 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1204...  Training loss: 1.7084...  0.2093 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1205...  Training loss: 1.7774...  0.2004 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1206...  Training loss: 1.7729...  0.3042 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1207...  Training loss: 1.6980...  0.2991 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1208...  Training loss: 1.7352...  0.2195 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1209...  Training loss: 1.7267...  0.1726 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1210...  Training loss: 1.7193...  0.2203 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1211...  Training loss: 1.7331...  0.2078 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1212...  Training loss: 1.6863...  0.1702 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1213...  Training loss: 1.6971...  0.1579 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1214...  Training loss: 1.6755...  0.2031 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1215...  Training loss: 1.7410...  0.2030 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1216...  Training loss: 1.7244...  0.2450 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1217...  Training loss: 1.7529...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1218...  Training loss: 1.7273...  0.1921 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1219...  Training loss: 1.7177...  0.1921 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1220...  Training loss: 1.6887...  0.1964 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1221...  Training loss: 1.7192...  0.2096 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1222...  Training loss: 1.7158...  0.3081 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1223...  Training loss: 1.7108...  0.2348 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1224...  Training loss: 1.7044...  0.2122 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1225...  Training loss: 1.7003...  0.1694 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1226...  Training loss: 1.7276...  0.2512 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1227...  Training loss: 1.7676...  0.2674 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1228...  Training loss: 1.7432...  0.1706 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1229...  Training loss: 1.7311...  0.2264 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1230...  Training loss: 1.7356...  0.2161 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1231...  Training loss: 1.7384...  0.1681 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1232...  Training loss: 1.7157...  0.2137 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1233...  Training loss: 1.6928...  0.2886 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1234...  Training loss: 1.7559...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1235...  Training loss: 1.7313...  0.1576 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1236...  Training loss: 1.7099...  0.1904 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1237...  Training loss: 1.7412...  0.2063 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1238...  Training loss: 1.7403...  0.2665 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1239...  Training loss: 1.7303...  0.2727 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1240...  Training loss: 1.7380...  0.1814 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1241...  Training loss: 1.7744...  0.1741 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1242...  Training loss: 1.7168...  0.1929 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1243...  Training loss: 1.7068...  0.1675 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1244...  Training loss: 1.7209...  0.1554 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1245...  Training loss: 1.7184...  0.2886 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1246...  Training loss: 1.7469...  0.2621 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1247...  Training loss: 1.7382...  0.2914 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1248...  Training loss: 1.6931...  0.2550 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1249...  Training loss: 1.7324...  0.2462 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1250...  Training loss: 1.7237...  0.2143 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1251...  Training loss: 1.7290...  0.3138 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1252...  Training loss: 1.7211...  0.1756 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1253...  Training loss: 1.7137...  0.2568 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1254...  Training loss: 1.7135...  0.2769 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1255...  Training loss: 1.7457...  0.2024 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1256...  Training loss: 1.7347...  0.1831 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1257...  Training loss: 1.7336...  0.1655 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1258...  Training loss: 1.6990...  0.2646 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1259...  Training loss: 1.7096...  0.2258 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1260...  Training loss: 1.7105...  0.3452 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1261...  Training loss: 1.6879...  0.1559 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1262...  Training loss: 1.7085...  0.2549 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1263...  Training loss: 1.7123...  0.2174 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1264...  Training loss: 1.7315...  0.2371 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1265...  Training loss: 1.7184...  0.2590 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1266...  Training loss: 1.6707...  0.2458 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1267...  Training loss: 1.7005...  0.2615 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1268...  Training loss: 1.7260...  0.2477 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1269...  Training loss: 1.7347...  0.2599 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1270...  Training loss: 1.7362...  0.1879 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1271...  Training loss: 1.7291...  0.2379 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1272...  Training loss: 1.7105...  0.1967 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1273...  Training loss: 1.7252...  0.1725 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1274...  Training loss: 1.7006...  0.2360 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1275...  Training loss: 1.7227...  0.1629 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1276...  Training loss: 1.6975...  0.1702 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1277...  Training loss: 1.7399...  0.2449 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1278...  Training loss: 1.7312...  0.2215 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1279...  Training loss: 1.7245...  0.2944 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1280...  Training loss: 1.6807...  0.2307 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1281...  Training loss: 1.6995...  0.1701 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1282...  Training loss: 1.6733...  0.2160 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1283...  Training loss: 1.7219...  0.2455 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1284...  Training loss: 1.7423...  0.2537 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1285...  Training loss: 1.7079...  0.1747 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1286...  Training loss: 1.6699...  0.2398 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1287...  Training loss: 1.6691...  0.2291 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1288...  Training loss: 1.6924...  0.1947 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1289...  Training loss: 1.6941...  0.2755 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1290...  Training loss: 1.7388...  0.1735 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1291...  Training loss: 1.7489...  0.1615 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1292...  Training loss: 1.7035...  0.2122 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1293...  Training loss: 1.6915...  0.2734 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1294...  Training loss: 1.7093...  0.2793 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1295...  Training loss: 1.7019...  0.2697 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1296...  Training loss: 1.6961...  0.2876 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1297...  Training loss: 1.7140...  0.2863 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1298...  Training loss: 1.6780...  0.2285 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1299...  Training loss: 1.7183...  0.1664 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1300...  Training loss: 1.6907...  0.1590 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/30...  Training Step: 1301...  Training loss: 1.6882...  0.2484 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1302...  Training loss: 1.6665...  0.2628 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1303...  Training loss: 1.6981...  0.2665 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1304...  Training loss: 1.7146...  0.2766 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1305...  Training loss: 1.7181...  0.2289 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1306...  Training loss: 1.7075...  0.2451 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1307...  Training loss: 1.6759...  0.2501 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1308...  Training loss: 1.7156...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1309...  Training loss: 1.6784...  0.2392 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1310...  Training loss: 1.7087...  0.2423 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1311...  Training loss: 1.6708...  0.2057 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1312...  Training loss: 1.6844...  0.2959 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1313...  Training loss: 1.7012...  0.2356 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1314...  Training loss: 1.7316...  0.1645 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1315...  Training loss: 1.6801...  0.2244 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1316...  Training loss: 1.6759...  0.1904 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1317...  Training loss: 1.7127...  0.2482 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1318...  Training loss: 1.7041...  0.2003 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1319...  Training loss: 1.7111...  0.2593 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1320...  Training loss: 1.7298...  0.2157 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1321...  Training loss: 1.7506...  0.2832 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1322...  Training loss: 1.6697...  0.2101 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1323...  Training loss: 1.6785...  0.1674 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1324...  Training loss: 1.7274...  0.2562 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1325...  Training loss: 1.7095...  0.2086 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1326...  Training loss: 1.7356...  0.2745 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1327...  Training loss: 1.6707...  0.2132 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1328...  Training loss: 1.6804...  0.2415 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1329...  Training loss: 1.6759...  0.2669 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1330...  Training loss: 1.6753...  0.2757 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1331...  Training loss: 1.6837...  0.2653 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1332...  Training loss: 1.6905...  0.2931 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1333...  Training loss: 1.6672...  0.2666 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1334...  Training loss: 1.6880...  0.2661 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1335...  Training loss: 1.6712...  0.3489 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1336...  Training loss: 1.6438...  0.2069 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1337...  Training loss: 1.7121...  0.2878 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1338...  Training loss: 1.6849...  0.2116 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1339...  Training loss: 1.6766...  0.1968 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1340...  Training loss: 1.6871...  0.2556 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1341...  Training loss: 1.6864...  0.3094 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1342...  Training loss: 1.7085...  0.1973 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1343...  Training loss: 1.6889...  0.2576 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1344...  Training loss: 1.6753...  0.1660 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1345...  Training loss: 1.6747...  0.1580 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1346...  Training loss: 1.6299...  0.1705 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1347...  Training loss: 1.6724...  0.2850 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1348...  Training loss: 1.6982...  0.2237 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1349...  Training loss: 1.6676...  0.1544 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1350...  Training loss: 1.7088...  0.1682 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1351...  Training loss: 1.6748...  0.2616 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1352...  Training loss: 1.7055...  0.1941 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1353...  Training loss: 1.6973...  0.1556 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1354...  Training loss: 1.6699...  0.1931 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1355...  Training loss: 1.6610...  0.1643 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1356...  Training loss: 1.6581...  0.1537 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1357...  Training loss: 1.6679...  0.2676 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1358...  Training loss: 1.6796...  0.2528 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1359...  Training loss: 1.6867...  0.2835 sec/batch\n",
      "Epoch: 8/30...  Training Step: 1360...  Training loss: 1.6567...  0.2706 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1361...  Training loss: 1.7546...  0.2001 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1362...  Training loss: 1.6447...  0.2875 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1363...  Training loss: 1.6644...  0.2334 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1364...  Training loss: 1.7097...  0.2610 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1365...  Training loss: 1.6881...  0.1820 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1366...  Training loss: 1.7124...  0.2238 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1367...  Training loss: 1.6849...  0.2519 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1368...  Training loss: 1.6621...  0.2774 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1369...  Training loss: 1.6372...  0.2608 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1370...  Training loss: 1.6460...  0.2016 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1371...  Training loss: 1.6314...  0.1611 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1372...  Training loss: 1.6573...  0.2814 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1373...  Training loss: 1.6699...  0.1711 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1374...  Training loss: 1.6609...  0.1703 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1375...  Training loss: 1.7265...  0.1640 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1376...  Training loss: 1.7235...  0.2278 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1377...  Training loss: 1.6370...  0.1843 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1378...  Training loss: 1.6724...  0.2565 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1379...  Training loss: 1.6679...  0.2087 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1380...  Training loss: 1.6656...  0.2166 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1381...  Training loss: 1.6828...  0.2450 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1382...  Training loss: 1.6403...  0.2312 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1383...  Training loss: 1.6462...  0.2852 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1384...  Training loss: 1.6275...  0.1946 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1385...  Training loss: 1.6960...  0.1803 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1386...  Training loss: 1.6648...  0.1941 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1387...  Training loss: 1.6999...  0.3052 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1388...  Training loss: 1.6867...  0.1643 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1389...  Training loss: 1.6523...  0.2306 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1390...  Training loss: 1.6313...  0.3071 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1391...  Training loss: 1.6595...  0.2057 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1392...  Training loss: 1.6623...  0.2012 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1393...  Training loss: 1.6628...  0.3036 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1394...  Training loss: 1.6305...  0.3084 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1395...  Training loss: 1.6506...  0.2308 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1396...  Training loss: 1.6653...  0.2886 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1397...  Training loss: 1.7081...  0.1945 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1398...  Training loss: 1.6776...  0.1639 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1399...  Training loss: 1.6801...  0.1881 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30...  Training Step: 1400...  Training loss: 1.6848...  0.2042 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1401...  Training loss: 1.6859...  0.1825 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1402...  Training loss: 1.6544...  0.2551 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1403...  Training loss: 1.6489...  0.1808 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1404...  Training loss: 1.7095...  0.2233 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1405...  Training loss: 1.6814...  0.2278 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1406...  Training loss: 1.6533...  0.2506 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1407...  Training loss: 1.6876...  0.1853 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1408...  Training loss: 1.6815...  0.1660 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1409...  Training loss: 1.6776...  0.1964 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1410...  Training loss: 1.6838...  0.2802 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1411...  Training loss: 1.7223...  0.2847 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1412...  Training loss: 1.6665...  0.2599 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1413...  Training loss: 1.6461...  0.2310 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1414...  Training loss: 1.6579...  0.2533 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1415...  Training loss: 1.6746...  0.2833 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1416...  Training loss: 1.6840...  0.2819 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1417...  Training loss: 1.6795...  0.1840 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1418...  Training loss: 1.6350...  0.2603 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1419...  Training loss: 1.6895...  0.1806 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1420...  Training loss: 1.6714...  0.1856 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1421...  Training loss: 1.6799...  0.1785 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1422...  Training loss: 1.6684...  0.1577 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1423...  Training loss: 1.6509...  0.2299 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1424...  Training loss: 1.6658...  0.1982 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1425...  Training loss: 1.6931...  0.3246 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1426...  Training loss: 1.6821...  0.2695 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1427...  Training loss: 1.7016...  0.3667 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1428...  Training loss: 1.6526...  0.1702 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1429...  Training loss: 1.6664...  0.2980 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1430...  Training loss: 1.6584...  0.2651 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1431...  Training loss: 1.6234...  0.2036 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1432...  Training loss: 1.6611...  0.2085 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1433...  Training loss: 1.6637...  0.2325 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1434...  Training loss: 1.6902...  0.2365 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1435...  Training loss: 1.6777...  0.2785 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1436...  Training loss: 1.6201...  0.2092 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1437...  Training loss: 1.6527...  0.2533 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1438...  Training loss: 1.6719...  0.1697 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1439...  Training loss: 1.6767...  0.2648 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1440...  Training loss: 1.6878...  0.2148 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1441...  Training loss: 1.6855...  0.2598 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1442...  Training loss: 1.6666...  0.1911 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1443...  Training loss: 1.6769...  0.2625 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1444...  Training loss: 1.6421...  0.2176 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1445...  Training loss: 1.6639...  0.2377 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1446...  Training loss: 1.6545...  0.2342 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1447...  Training loss: 1.6796...  0.2360 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1448...  Training loss: 1.6812...  0.2082 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1449...  Training loss: 1.6779...  0.2588 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1450...  Training loss: 1.6339...  0.2653 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1451...  Training loss: 1.6503...  0.2748 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1452...  Training loss: 1.6314...  0.2734 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1453...  Training loss: 1.6758...  0.2379 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1454...  Training loss: 1.7005...  0.2495 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1455...  Training loss: 1.6463...  0.2038 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1456...  Training loss: 1.6155...  0.2512 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1457...  Training loss: 1.6204...  0.2400 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1458...  Training loss: 1.6435...  0.1986 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1459...  Training loss: 1.6522...  0.2489 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1460...  Training loss: 1.7013...  0.2611 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1461...  Training loss: 1.6886...  0.1925 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1462...  Training loss: 1.6436...  0.1654 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1463...  Training loss: 1.6457...  0.2566 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1464...  Training loss: 1.6644...  0.1603 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1465...  Training loss: 1.6522...  0.2549 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1466...  Training loss: 1.6399...  0.2452 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1467...  Training loss: 1.6535...  0.2438 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1468...  Training loss: 1.6346...  0.2645 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1469...  Training loss: 1.6631...  0.2781 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1470...  Training loss: 1.6486...  0.2012 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1471...  Training loss: 1.6348...  0.2660 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1472...  Training loss: 1.6215...  0.2546 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1473...  Training loss: 1.6451...  0.2420 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1474...  Training loss: 1.6713...  0.2604 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1475...  Training loss: 1.6682...  0.1639 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1476...  Training loss: 1.6486...  0.1709 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1477...  Training loss: 1.6238...  0.1808 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1478...  Training loss: 1.6579...  0.1628 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1479...  Training loss: 1.6260...  0.2446 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1480...  Training loss: 1.6573...  0.2228 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1481...  Training loss: 1.6223...  0.1817 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1482...  Training loss: 1.6388...  0.2685 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1483...  Training loss: 1.6566...  0.2677 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1484...  Training loss: 1.6777...  0.2612 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1485...  Training loss: 1.6339...  0.2590 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1486...  Training loss: 1.6415...  0.1628 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1487...  Training loss: 1.6547...  0.1944 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1488...  Training loss: 1.6548...  0.2025 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1489...  Training loss: 1.6582...  0.1588 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1490...  Training loss: 1.6800...  0.2628 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1491...  Training loss: 1.7002...  0.2910 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1492...  Training loss: 1.6159...  0.2326 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1493...  Training loss: 1.6274...  0.2000 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1494...  Training loss: 1.6780...  0.2143 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1495...  Training loss: 1.6688...  0.2294 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1496...  Training loss: 1.6795...  0.2737 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1497...  Training loss: 1.6231...  0.3579 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1498...  Training loss: 1.6300...  0.2177 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/30...  Training Step: 1499...  Training loss: 1.6259...  0.2452 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1500...  Training loss: 1.6439...  0.2967 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1501...  Training loss: 1.6405...  0.2061 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1502...  Training loss: 1.6422...  0.1885 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1503...  Training loss: 1.6270...  0.2647 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1504...  Training loss: 1.6420...  0.1843 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1505...  Training loss: 1.6321...  0.2723 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1506...  Training loss: 1.5968...  0.2165 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1507...  Training loss: 1.6623...  0.2759 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1508...  Training loss: 1.6353...  0.1991 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1509...  Training loss: 1.6356...  0.2861 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1510...  Training loss: 1.6433...  0.1967 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1511...  Training loss: 1.6487...  0.1610 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1512...  Training loss: 1.6602...  0.2194 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1513...  Training loss: 1.6424...  0.2021 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1514...  Training loss: 1.6287...  0.1601 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1515...  Training loss: 1.6280...  0.2180 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1516...  Training loss: 1.6019...  0.1991 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1517...  Training loss: 1.6209...  0.2937 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1518...  Training loss: 1.6468...  0.1721 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1519...  Training loss: 1.6306...  0.1555 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1520...  Training loss: 1.6650...  0.1933 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1521...  Training loss: 1.6331...  0.2232 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1522...  Training loss: 1.6552...  0.1933 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1523...  Training loss: 1.6544...  0.2167 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1524...  Training loss: 1.6204...  0.2338 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1525...  Training loss: 1.6182...  0.1594 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1526...  Training loss: 1.6191...  0.2747 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1527...  Training loss: 1.6251...  0.2121 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1528...  Training loss: 1.6309...  0.2380 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1529...  Training loss: 1.6351...  0.2717 sec/batch\n",
      "Epoch: 9/30...  Training Step: 1530...  Training loss: 1.6087...  0.2424 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1531...  Training loss: 1.7148...  0.2152 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1532...  Training loss: 1.6023...  0.2030 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1533...  Training loss: 1.6177...  0.2688 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1534...  Training loss: 1.6644...  0.2028 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1535...  Training loss: 1.6425...  0.1774 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1536...  Training loss: 1.6661...  0.2279 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1537...  Training loss: 1.6389...  0.2740 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1538...  Training loss: 1.6177...  0.2418 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1539...  Training loss: 1.6014...  0.2708 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1540...  Training loss: 1.6027...  0.2170 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1541...  Training loss: 1.5914...  0.1678 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1542...  Training loss: 1.6158...  0.2775 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1543...  Training loss: 1.6114...  0.2197 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1544...  Training loss: 1.6024...  0.2489 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1545...  Training loss: 1.6781...  0.2481 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1546...  Training loss: 1.6735...  0.1934 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1547...  Training loss: 1.6067...  0.1991 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1548...  Training loss: 1.6221...  0.1702 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1549...  Training loss: 1.6307...  0.1867 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1550...  Training loss: 1.6260...  0.1980 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1551...  Training loss: 1.6311...  0.1601 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1552...  Training loss: 1.5968...  0.2129 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1553...  Training loss: 1.5912...  0.3046 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1554...  Training loss: 1.5883...  0.3058 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1555...  Training loss: 1.6397...  0.1863 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1556...  Training loss: 1.6279...  0.2908 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1557...  Training loss: 1.6520...  0.2736 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1558...  Training loss: 1.6470...  0.2101 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1559...  Training loss: 1.6059...  0.1654 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1560...  Training loss: 1.5812...  0.1624 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1561...  Training loss: 1.6201...  0.2402 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1562...  Training loss: 1.5977...  0.2432 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1563...  Training loss: 1.6141...  0.2431 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1564...  Training loss: 1.5950...  0.1924 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1565...  Training loss: 1.5996...  0.1597 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1566...  Training loss: 1.6211...  0.2021 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1567...  Training loss: 1.6578...  0.1791 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1568...  Training loss: 1.6395...  0.1953 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1569...  Training loss: 1.6197...  0.2598 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1570...  Training loss: 1.6412...  0.2471 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1571...  Training loss: 1.6352...  0.2087 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1572...  Training loss: 1.6133...  0.1682 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1573...  Training loss: 1.6008...  0.1751 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1574...  Training loss: 1.6705...  0.1569 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1575...  Training loss: 1.6369...  0.1704 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1576...  Training loss: 1.6142...  0.1568 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1577...  Training loss: 1.6489...  0.1589 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1578...  Training loss: 1.6414...  0.1779 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1579...  Training loss: 1.6286...  0.2953 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1580...  Training loss: 1.6394...  0.2541 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1581...  Training loss: 1.6738...  0.2142 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1582...  Training loss: 1.6153...  0.2062 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1583...  Training loss: 1.6001...  0.2678 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1584...  Training loss: 1.6149...  0.2085 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1585...  Training loss: 1.6191...  0.2289 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1586...  Training loss: 1.6419...  0.2264 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1587...  Training loss: 1.6257...  0.2649 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1588...  Training loss: 1.5884...  0.2604 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1589...  Training loss: 1.6295...  0.1896 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1590...  Training loss: 1.6229...  0.1968 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1591...  Training loss: 1.6342...  0.1836 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1592...  Training loss: 1.6231...  0.1675 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1593...  Training loss: 1.6082...  0.2417 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1594...  Training loss: 1.6298...  0.1735 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1595...  Training loss: 1.6476...  0.1563 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1596...  Training loss: 1.6450...  0.2814 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30...  Training Step: 1597...  Training loss: 1.6456...  0.2010 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1598...  Training loss: 1.6100...  0.2768 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1599...  Training loss: 1.6119...  0.3001 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1600...  Training loss: 1.6152...  0.2578 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1601...  Training loss: 1.5864...  0.2546 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1602...  Training loss: 1.6221...  0.3229 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1603...  Training loss: 1.6236...  0.2028 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1604...  Training loss: 1.6400...  0.2530 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1605...  Training loss: 1.6361...  0.1895 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1606...  Training loss: 1.5846...  0.2248 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1607...  Training loss: 1.6155...  0.1675 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1608...  Training loss: 1.6289...  0.2344 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1609...  Training loss: 1.6443...  0.2006 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1610...  Training loss: 1.6398...  0.2122 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1611...  Training loss: 1.6416...  0.1944 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1612...  Training loss: 1.6253...  0.2114 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1613...  Training loss: 1.6287...  0.2469 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1614...  Training loss: 1.6047...  0.1707 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1615...  Training loss: 1.6154...  0.1749 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1616...  Training loss: 1.5956...  0.1527 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1617...  Training loss: 1.6390...  0.2643 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1618...  Training loss: 1.6416...  0.2201 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1619...  Training loss: 1.6357...  0.2378 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1620...  Training loss: 1.5838...  0.1919 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1621...  Training loss: 1.6078...  0.2634 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1622...  Training loss: 1.5805...  0.1873 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1623...  Training loss: 1.6289...  0.2577 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1624...  Training loss: 1.6467...  0.2134 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1625...  Training loss: 1.6101...  0.2257 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1626...  Training loss: 1.5700...  0.1524 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1627...  Training loss: 1.5871...  0.1769 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1628...  Training loss: 1.6004...  0.2409 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1629...  Training loss: 1.6107...  0.2808 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1630...  Training loss: 1.6623...  0.2554 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1631...  Training loss: 1.6435...  0.2752 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1632...  Training loss: 1.6123...  0.2490 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1633...  Training loss: 1.6106...  0.2574 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1634...  Training loss: 1.6176...  0.1992 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1635...  Training loss: 1.6004...  0.2556 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1636...  Training loss: 1.6000...  0.2317 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1637...  Training loss: 1.6149...  0.2617 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1638...  Training loss: 1.5917...  0.2947 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1639...  Training loss: 1.6354...  0.1977 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1640...  Training loss: 1.5953...  0.1609 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1641...  Training loss: 1.5921...  0.1559 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1642...  Training loss: 1.5719...  0.1797 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1643...  Training loss: 1.6076...  0.2474 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1644...  Training loss: 1.6261...  0.1647 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1645...  Training loss: 1.6263...  0.1719 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1646...  Training loss: 1.6147...  0.2489 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1647...  Training loss: 1.5833...  0.1597 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1648...  Training loss: 1.6145...  0.2463 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1649...  Training loss: 1.5854...  0.2958 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1650...  Training loss: 1.6098...  0.2879 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1651...  Training loss: 1.5867...  0.2768 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1652...  Training loss: 1.5951...  0.2355 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1653...  Training loss: 1.6009...  0.1945 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1654...  Training loss: 1.6283...  0.2588 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1655...  Training loss: 1.5879...  0.2016 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1656...  Training loss: 1.5957...  0.1715 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1657...  Training loss: 1.6210...  0.2802 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1658...  Training loss: 1.6066...  0.2820 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1659...  Training loss: 1.6173...  0.2560 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1660...  Training loss: 1.6392...  0.2090 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1661...  Training loss: 1.6596...  0.2765 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1662...  Training loss: 1.5748...  0.1737 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1663...  Training loss: 1.5828...  0.1957 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1664...  Training loss: 1.6384...  0.1520 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1665...  Training loss: 1.6142...  0.1889 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1666...  Training loss: 1.6492...  0.1932 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1667...  Training loss: 1.5759...  0.1910 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1668...  Training loss: 1.5966...  0.2599 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1669...  Training loss: 1.5962...  0.1626 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1670...  Training loss: 1.6093...  0.2201 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1671...  Training loss: 1.5896...  0.2116 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1672...  Training loss: 1.6025...  0.2013 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1673...  Training loss: 1.5881...  0.2154 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1674...  Training loss: 1.5998...  0.1651 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1675...  Training loss: 1.5803...  0.1951 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1676...  Training loss: 1.5576...  0.1887 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1677...  Training loss: 1.6254...  0.1966 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1678...  Training loss: 1.5994...  0.2499 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1679...  Training loss: 1.5903...  0.2718 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1680...  Training loss: 1.5925...  0.2632 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1681...  Training loss: 1.6044...  0.2644 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1682...  Training loss: 1.6179...  0.2389 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1683...  Training loss: 1.5937...  0.2022 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1684...  Training loss: 1.5852...  0.1999 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1685...  Training loss: 1.5781...  0.2615 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1686...  Training loss: 1.5539...  0.2564 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1687...  Training loss: 1.5809...  0.2842 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1688...  Training loss: 1.6041...  0.1830 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1689...  Training loss: 1.5836...  0.1756 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1690...  Training loss: 1.6199...  0.2102 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1691...  Training loss: 1.5882...  0.2783 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1692...  Training loss: 1.6267...  0.2058 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1693...  Training loss: 1.6079...  0.2134 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1694...  Training loss: 1.5850...  0.2626 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30...  Training Step: 1695...  Training loss: 1.5649...  0.2583 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1696...  Training loss: 1.5695...  0.2342 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1697...  Training loss: 1.5885...  0.2432 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1698...  Training loss: 1.5895...  0.2428 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1699...  Training loss: 1.6033...  0.2475 sec/batch\n",
      "Epoch: 10/30...  Training Step: 1700...  Training loss: 1.5742...  0.2302 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1701...  Training loss: 1.6647...  0.1942 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1702...  Training loss: 1.5638...  0.1974 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1703...  Training loss: 1.5824...  0.1652 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1704...  Training loss: 1.6133...  0.2819 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1705...  Training loss: 1.6119...  0.2119 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1706...  Training loss: 1.6265...  0.1901 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1707...  Training loss: 1.6032...  0.1635 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1708...  Training loss: 1.5824...  0.2290 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1709...  Training loss: 1.5564...  0.1773 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1710...  Training loss: 1.5686...  0.1566 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1711...  Training loss: 1.5570...  0.1930 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1712...  Training loss: 1.5851...  0.2097 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1713...  Training loss: 1.5723...  0.1855 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1714...  Training loss: 1.5655...  0.2149 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1715...  Training loss: 1.6335...  0.2354 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1716...  Training loss: 1.6359...  0.1862 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1717...  Training loss: 1.5701...  0.2454 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1718...  Training loss: 1.5893...  0.2137 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1719...  Training loss: 1.5775...  0.1586 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1720...  Training loss: 1.5772...  0.2657 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1721...  Training loss: 1.6001...  0.2054 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1722...  Training loss: 1.5515...  0.2677 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1723...  Training loss: 1.5586...  0.1749 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1724...  Training loss: 1.5415...  0.1887 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1725...  Training loss: 1.5997...  0.2479 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1726...  Training loss: 1.5775...  0.2481 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1727...  Training loss: 1.6107...  0.1846 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1728...  Training loss: 1.6060...  0.2349 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1729...  Training loss: 1.5783...  0.1658 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1730...  Training loss: 1.5606...  0.2224 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1731...  Training loss: 1.5763...  0.2289 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1732...  Training loss: 1.5588...  0.1693 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1733...  Training loss: 1.5692...  0.1898 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1734...  Training loss: 1.5542...  0.2311 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1735...  Training loss: 1.5488...  0.1960 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1736...  Training loss: 1.5866...  0.1879 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1737...  Training loss: 1.6311...  0.2274 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1738...  Training loss: 1.5907...  0.2242 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1739...  Training loss: 1.5849...  0.1995 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1740...  Training loss: 1.5987...  0.2280 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1741...  Training loss: 1.5947...  0.2548 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1742...  Training loss: 1.5763...  0.2198 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1743...  Training loss: 1.5618...  0.2094 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1744...  Training loss: 1.6282...  0.2079 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1745...  Training loss: 1.6067...  0.2144 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1746...  Training loss: 1.5822...  0.2314 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1747...  Training loss: 1.6038...  0.2201 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1748...  Training loss: 1.6044...  0.2129 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1749...  Training loss: 1.5815...  0.1951 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1750...  Training loss: 1.5931...  0.3309 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1751...  Training loss: 1.6370...  0.2520 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1752...  Training loss: 1.5715...  0.3036 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1753...  Training loss: 1.5716...  0.1548 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1754...  Training loss: 1.5810...  0.2367 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1755...  Training loss: 1.5888...  0.2217 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1756...  Training loss: 1.6041...  0.1968 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1757...  Training loss: 1.5971...  0.2435 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1758...  Training loss: 1.5576...  0.2647 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1759...  Training loss: 1.6027...  0.2439 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1760...  Training loss: 1.5856...  0.2683 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1761...  Training loss: 1.5959...  0.1828 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1762...  Training loss: 1.5916...  0.1700 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1763...  Training loss: 1.5777...  0.1774 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1764...  Training loss: 1.5747...  0.2790 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1765...  Training loss: 1.6084...  0.2476 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1766...  Training loss: 1.6052...  0.2314 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1767...  Training loss: 1.6073...  0.3195 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1768...  Training loss: 1.5780...  0.1680 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1769...  Training loss: 1.5839...  0.2044 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1770...  Training loss: 1.5737...  0.2603 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1771...  Training loss: 1.5478...  0.1693 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1772...  Training loss: 1.5815...  0.2095 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1773...  Training loss: 1.5909...  0.2021 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1774...  Training loss: 1.6071...  0.1696 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1775...  Training loss: 1.6022...  0.2272 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1776...  Training loss: 1.5468...  0.2404 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1777...  Training loss: 1.5629...  0.1930 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1778...  Training loss: 1.5836...  0.2870 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1779...  Training loss: 1.6032...  0.1751 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1780...  Training loss: 1.6047...  0.1679 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1781...  Training loss: 1.6018...  0.2367 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1782...  Training loss: 1.5909...  0.1608 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1783...  Training loss: 1.5903...  0.2015 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1784...  Training loss: 1.5679...  0.2353 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1785...  Training loss: 1.5870...  0.2433 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1786...  Training loss: 1.5614...  0.2580 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1787...  Training loss: 1.5966...  0.2492 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1788...  Training loss: 1.6021...  0.2324 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1789...  Training loss: 1.6015...  0.2373 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1790...  Training loss: 1.5598...  0.1965 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1791...  Training loss: 1.5771...  0.2011 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1792...  Training loss: 1.5376...  0.1771 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/30...  Training Step: 1793...  Training loss: 1.5974...  0.2430 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1794...  Training loss: 1.6172...  0.2035 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1795...  Training loss: 1.5715...  0.2758 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1796...  Training loss: 1.5503...  0.2516 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1797...  Training loss: 1.5429...  0.2929 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1798...  Training loss: 1.5628...  0.2576 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1799...  Training loss: 1.5699...  0.2861 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1800...  Training loss: 1.6261...  0.2283 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1801...  Training loss: 1.6161...  0.1795 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1802...  Training loss: 1.5819...  0.2460 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1803...  Training loss: 1.5760...  0.1898 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1804...  Training loss: 1.5823...  0.1628 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1805...  Training loss: 1.5703...  0.1890 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1806...  Training loss: 1.5555...  0.2047 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1807...  Training loss: 1.5801...  0.1906 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1808...  Training loss: 1.5535...  0.1556 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1809...  Training loss: 1.5909...  0.2286 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1810...  Training loss: 1.5680...  0.2646 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1811...  Training loss: 1.5587...  0.2681 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1812...  Training loss: 1.5468...  0.2560 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1813...  Training loss: 1.5629...  0.2660 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1814...  Training loss: 1.5920...  0.1848 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1815...  Training loss: 1.5903...  0.2585 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1816...  Training loss: 1.5682...  0.2618 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1817...  Training loss: 1.5467...  0.2711 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1818...  Training loss: 1.5902...  0.2537 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1819...  Training loss: 1.5416...  0.2437 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1820...  Training loss: 1.5782...  0.2286 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1821...  Training loss: 1.5461...  0.2429 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1822...  Training loss: 1.5694...  0.2693 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1823...  Training loss: 1.5792...  0.2641 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1824...  Training loss: 1.5989...  0.1814 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1825...  Training loss: 1.5530...  0.2275 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1826...  Training loss: 1.5525...  0.1542 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1827...  Training loss: 1.5739...  0.1592 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1828...  Training loss: 1.5771...  0.1781 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1829...  Training loss: 1.5778...  0.2093 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1830...  Training loss: 1.6091...  0.1884 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1831...  Training loss: 1.6290...  0.2380 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1832...  Training loss: 1.5395...  0.2385 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1833...  Training loss: 1.5529...  0.2426 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1834...  Training loss: 1.6005...  0.1791 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1835...  Training loss: 1.5757...  0.1896 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1836...  Training loss: 1.6149...  0.1536 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1837...  Training loss: 1.5489...  0.2142 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1838...  Training loss: 1.5548...  0.1768 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1839...  Training loss: 1.5610...  0.2443 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1840...  Training loss: 1.5680...  0.2020 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1841...  Training loss: 1.5643...  0.2193 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1842...  Training loss: 1.5697...  0.2350 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1843...  Training loss: 1.5616...  0.2618 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1844...  Training loss: 1.5531...  0.2147 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1845...  Training loss: 1.5519...  0.1999 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1846...  Training loss: 1.5167...  0.2052 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1847...  Training loss: 1.5858...  0.2696 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1848...  Training loss: 1.5650...  0.2449 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1849...  Training loss: 1.5646...  0.2586 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1850...  Training loss: 1.5499...  0.1895 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1851...  Training loss: 1.5820...  0.2524 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1852...  Training loss: 1.5837...  0.2713 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1853...  Training loss: 1.5657...  0.2307 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1854...  Training loss: 1.5500...  0.1725 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1855...  Training loss: 1.5381...  0.2586 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1856...  Training loss: 1.5256...  0.1765 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1857...  Training loss: 1.5539...  0.1602 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1858...  Training loss: 1.5666...  0.1543 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1859...  Training loss: 1.5539...  0.2000 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1860...  Training loss: 1.5812...  0.2758 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1861...  Training loss: 1.5614...  0.2684 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1862...  Training loss: 1.5909...  0.2553 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1863...  Training loss: 1.5715...  0.2605 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1864...  Training loss: 1.5507...  0.1831 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1865...  Training loss: 1.5368...  0.2534 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1866...  Training loss: 1.5400...  0.2650 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1867...  Training loss: 1.5517...  0.2663 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1868...  Training loss: 1.5622...  0.2089 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1869...  Training loss: 1.5718...  0.2513 sec/batch\n",
      "Epoch: 11/30...  Training Step: 1870...  Training loss: 1.5384...  0.1782 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1871...  Training loss: 1.6415...  0.1684 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1872...  Training loss: 1.5415...  0.2569 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1873...  Training loss: 1.5480...  0.2183 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1874...  Training loss: 1.5865...  0.1630 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1875...  Training loss: 1.5707...  0.2002 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1876...  Training loss: 1.5853...  0.2623 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1877...  Training loss: 1.5697...  0.2776 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1878...  Training loss: 1.5487...  0.2508 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1879...  Training loss: 1.5138...  0.2669 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1880...  Training loss: 1.5254...  0.1684 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1881...  Training loss: 1.5148...  0.1713 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1882...  Training loss: 1.5407...  0.2163 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1883...  Training loss: 1.5445...  0.1859 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1884...  Training loss: 1.5426...  0.1661 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1885...  Training loss: 1.6031...  0.1975 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1886...  Training loss: 1.6083...  0.2132 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1887...  Training loss: 1.5338...  0.1873 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1888...  Training loss: 1.5644...  0.2275 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1889...  Training loss: 1.5475...  0.1919 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1890...  Training loss: 1.5504...  0.1917 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30...  Training Step: 1891...  Training loss: 1.5587...  0.2017 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1892...  Training loss: 1.5150...  0.3013 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1893...  Training loss: 1.5288...  0.2845 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1894...  Training loss: 1.5204...  0.2333 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1895...  Training loss: 1.5714...  0.2406 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1896...  Training loss: 1.5496...  0.3087 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1897...  Training loss: 1.5756...  0.2437 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1898...  Training loss: 1.5794...  0.2721 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1899...  Training loss: 1.5492...  0.1888 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1900...  Training loss: 1.5241...  0.1598 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1901...  Training loss: 1.5539...  0.1661 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1902...  Training loss: 1.5294...  0.2179 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1903...  Training loss: 1.5432...  0.1986 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1904...  Training loss: 1.5302...  0.1955 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1905...  Training loss: 1.5345...  0.2101 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1906...  Training loss: 1.5585...  0.2109 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1907...  Training loss: 1.5988...  0.2726 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1908...  Training loss: 1.5650...  0.2515 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1909...  Training loss: 1.5574...  0.2109 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1910...  Training loss: 1.5747...  0.2622 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1911...  Training loss: 1.5790...  0.2537 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1912...  Training loss: 1.5480...  0.2637 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1913...  Training loss: 1.5364...  0.2677 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1914...  Training loss: 1.5956...  0.2465 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1915...  Training loss: 1.5757...  0.3122 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1916...  Training loss: 1.5451...  0.2720 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1917...  Training loss: 1.5730...  0.1844 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1918...  Training loss: 1.5710...  0.2480 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1919...  Training loss: 1.5571...  0.2097 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1920...  Training loss: 1.5619...  0.2652 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1921...  Training loss: 1.6005...  0.2556 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1922...  Training loss: 1.5370...  0.1581 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1923...  Training loss: 1.5475...  0.2028 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1924...  Training loss: 1.5387...  0.2505 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1925...  Training loss: 1.5531...  0.1580 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1926...  Training loss: 1.5662...  0.1800 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1927...  Training loss: 1.5770...  0.1693 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1928...  Training loss: 1.5298...  0.2516 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1929...  Training loss: 1.5753...  0.1569 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1930...  Training loss: 1.5608...  0.2250 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1931...  Training loss: 1.5649...  0.3015 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1932...  Training loss: 1.5539...  0.2095 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1933...  Training loss: 1.5355...  0.1901 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1934...  Training loss: 1.5488...  0.1697 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1935...  Training loss: 1.5697...  0.1975 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1936...  Training loss: 1.5700...  0.1837 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1937...  Training loss: 1.5751...  0.2941 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1938...  Training loss: 1.5507...  0.2600 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1939...  Training loss: 1.5496...  0.2346 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1940...  Training loss: 1.5346...  0.2555 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1941...  Training loss: 1.5164...  0.2671 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1942...  Training loss: 1.5523...  0.1862 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1943...  Training loss: 1.5630...  0.1680 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1944...  Training loss: 1.5735...  0.1665 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1945...  Training loss: 1.5722...  0.1768 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1946...  Training loss: 1.5174...  0.1690 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1947...  Training loss: 1.5427...  0.2281 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1948...  Training loss: 1.5531...  0.1550 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1949...  Training loss: 1.5736...  0.2360 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1950...  Training loss: 1.5760...  0.2620 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1951...  Training loss: 1.5665...  0.2278 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1952...  Training loss: 1.5662...  0.1685 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1953...  Training loss: 1.5570...  0.1961 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1954...  Training loss: 1.5343...  0.2177 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1955...  Training loss: 1.5515...  0.1842 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1956...  Training loss: 1.5483...  0.2323 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1957...  Training loss: 1.5582...  0.2067 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1958...  Training loss: 1.5714...  0.1952 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1959...  Training loss: 1.5727...  0.1790 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1960...  Training loss: 1.5225...  0.2162 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1961...  Training loss: 1.5385...  0.1900 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1962...  Training loss: 1.5270...  0.1727 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1963...  Training loss: 1.5617...  0.1969 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1964...  Training loss: 1.5840...  0.2318 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1965...  Training loss: 1.5478...  0.2254 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1966...  Training loss: 1.5162...  0.2477 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1967...  Training loss: 1.5269...  0.2160 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1968...  Training loss: 1.5324...  0.2811 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1969...  Training loss: 1.5473...  0.2745 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1970...  Training loss: 1.5958...  0.2477 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1971...  Training loss: 1.5892...  0.2402 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1972...  Training loss: 1.5513...  0.1611 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1973...  Training loss: 1.5362...  0.1903 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1974...  Training loss: 1.5483...  0.2130 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1975...  Training loss: 1.5463...  0.1635 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1976...  Training loss: 1.5379...  0.1769 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1977...  Training loss: 1.5530...  0.1863 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1978...  Training loss: 1.5197...  0.2352 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1979...  Training loss: 1.5572...  0.1648 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1980...  Training loss: 1.5334...  0.1867 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1981...  Training loss: 1.5271...  0.2731 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1982...  Training loss: 1.5130...  0.3092 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1983...  Training loss: 1.5354...  0.1894 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1984...  Training loss: 1.5589...  0.1791 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1985...  Training loss: 1.5518...  0.1739 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1986...  Training loss: 1.5443...  0.2712 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1987...  Training loss: 1.5137...  0.2129 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1988...  Training loss: 1.5609...  0.2273 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/30...  Training Step: 1989...  Training loss: 1.5240...  0.2606 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1990...  Training loss: 1.5480...  0.2535 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1991...  Training loss: 1.5240...  0.2017 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1992...  Training loss: 1.5358...  0.2351 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1993...  Training loss: 1.5521...  0.1957 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1994...  Training loss: 1.5764...  0.1557 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1995...  Training loss: 1.5186...  0.2588 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1996...  Training loss: 1.5269...  0.2325 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1997...  Training loss: 1.5429...  0.2561 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1998...  Training loss: 1.5528...  0.1751 sec/batch\n",
      "Epoch: 12/30...  Training Step: 1999...  Training loss: 1.5439...  0.2815 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2000...  Training loss: 1.5802...  0.2402 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2001...  Training loss: 1.5936...  0.2581 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2002...  Training loss: 1.5097...  0.2147 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2003...  Training loss: 1.5149...  0.2152 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2004...  Training loss: 1.5632...  0.1852 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2005...  Training loss: 1.5517...  0.1976 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2006...  Training loss: 1.5839...  0.2167 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2007...  Training loss: 1.5183...  0.2225 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2008...  Training loss: 1.5236...  0.2534 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2009...  Training loss: 1.5352...  0.2537 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2010...  Training loss: 1.5396...  0.2671 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2011...  Training loss: 1.5308...  0.2353 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2012...  Training loss: 1.5416...  0.2624 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2013...  Training loss: 1.5298...  0.2705 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2014...  Training loss: 1.5375...  0.2353 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2015...  Training loss: 1.5256...  0.2229 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2016...  Training loss: 1.4964...  0.2373 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2017...  Training loss: 1.5582...  0.1580 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2018...  Training loss: 1.5359...  0.2056 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2019...  Training loss: 1.5296...  0.2639 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2020...  Training loss: 1.5295...  0.2976 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2021...  Training loss: 1.5345...  0.3034 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2022...  Training loss: 1.5631...  0.2415 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2023...  Training loss: 1.5355...  0.2573 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2024...  Training loss: 1.5187...  0.2128 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2025...  Training loss: 1.5072...  0.2334 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2026...  Training loss: 1.4950...  0.1925 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2027...  Training loss: 1.5202...  0.1690 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2028...  Training loss: 1.5459...  0.1726 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2029...  Training loss: 1.5169...  0.2702 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2030...  Training loss: 1.5478...  0.2225 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2031...  Training loss: 1.5246...  0.2169 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2032...  Training loss: 1.5524...  0.2344 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2033...  Training loss: 1.5380...  0.2203 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2034...  Training loss: 1.5233...  0.2154 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2035...  Training loss: 1.5099...  0.2565 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2036...  Training loss: 1.5164...  0.2473 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2037...  Training loss: 1.5375...  0.2612 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2038...  Training loss: 1.5239...  0.2448 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2039...  Training loss: 1.5366...  0.2316 sec/batch\n",
      "Epoch: 12/30...  Training Step: 2040...  Training loss: 1.5181...  0.2575 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2041...  Training loss: 1.5967...  0.2431 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2042...  Training loss: 1.4969...  0.2496 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2043...  Training loss: 1.5079...  0.2342 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2044...  Training loss: 1.5551...  0.1725 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2045...  Training loss: 1.5380...  0.2106 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2046...  Training loss: 1.5737...  0.2616 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2047...  Training loss: 1.5391...  0.2043 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2048...  Training loss: 1.5225...  0.3220 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2049...  Training loss: 1.4924...  0.2419 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2050...  Training loss: 1.5031...  0.2289 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2051...  Training loss: 1.4924...  0.2098 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2052...  Training loss: 1.5219...  0.3148 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2053...  Training loss: 1.5140...  0.3048 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2054...  Training loss: 1.5159...  0.2288 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2055...  Training loss: 1.5790...  0.1791 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2056...  Training loss: 1.5680...  0.1628 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2057...  Training loss: 1.5102...  0.2497 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2058...  Training loss: 1.5384...  0.1767 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2059...  Training loss: 1.5258...  0.2380 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2060...  Training loss: 1.5263...  0.2806 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2061...  Training loss: 1.5219...  0.2675 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2062...  Training loss: 1.5001...  0.3096 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2063...  Training loss: 1.4934...  0.2134 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2064...  Training loss: 1.4928...  0.2193 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2065...  Training loss: 1.5516...  0.2359 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2066...  Training loss: 1.5226...  0.2098 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2067...  Training loss: 1.5468...  0.2529 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2068...  Training loss: 1.5448...  0.2286 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2069...  Training loss: 1.5247...  0.2986 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2070...  Training loss: 1.4838...  0.2660 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2071...  Training loss: 1.5188...  0.3054 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2072...  Training loss: 1.5111...  0.1573 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2073...  Training loss: 1.5104...  0.1680 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2074...  Training loss: 1.5023...  0.2271 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2075...  Training loss: 1.5025...  0.2403 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2076...  Training loss: 1.5197...  0.1806 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2077...  Training loss: 1.5644...  0.1735 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2078...  Training loss: 1.5397...  0.2575 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2079...  Training loss: 1.5205...  0.2514 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2080...  Training loss: 1.5419...  0.2479 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2081...  Training loss: 1.5358...  0.2281 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2082...  Training loss: 1.5230...  0.1955 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2083...  Training loss: 1.5024...  0.1594 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2084...  Training loss: 1.5648...  0.1846 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2085...  Training loss: 1.5538...  0.1894 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2086...  Training loss: 1.5177...  0.2044 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2087...  Training loss: 1.5457...  0.1793 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/30...  Training Step: 2088...  Training loss: 1.5483...  0.2532 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2089...  Training loss: 1.5276...  0.1976 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2090...  Training loss: 1.5351...  0.2258 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2091...  Training loss: 1.5862...  0.2459 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2092...  Training loss: 1.5060...  0.2803 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2093...  Training loss: 1.5060...  0.2652 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2094...  Training loss: 1.5061...  0.2289 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2095...  Training loss: 1.5218...  0.2524 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2096...  Training loss: 1.5409...  0.1886 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2097...  Training loss: 1.5292...  0.2158 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2098...  Training loss: 1.5006...  0.2182 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2099...  Training loss: 1.5477...  0.1775 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2100...  Training loss: 1.5321...  0.2608 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2101...  Training loss: 1.5441...  0.2117 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2102...  Training loss: 1.5321...  0.2479 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2103...  Training loss: 1.5010...  0.2379 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2104...  Training loss: 1.5281...  0.2121 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2105...  Training loss: 1.5414...  0.1908 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2106...  Training loss: 1.5322...  0.2752 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2107...  Training loss: 1.5482...  0.1786 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2108...  Training loss: 1.5239...  0.2220 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2109...  Training loss: 1.5199...  0.2306 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2110...  Training loss: 1.5188...  0.2367 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2111...  Training loss: 1.4907...  0.1681 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2112...  Training loss: 1.5268...  0.1987 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2113...  Training loss: 1.5319...  0.2741 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2114...  Training loss: 1.5494...  0.1926 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2115...  Training loss: 1.5494...  0.1849 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2116...  Training loss: 1.5009...  0.1632 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2117...  Training loss: 1.5196...  0.1717 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2118...  Training loss: 1.5289...  0.2059 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2119...  Training loss: 1.5415...  0.2713 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2120...  Training loss: 1.5563...  0.2775 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2121...  Training loss: 1.5492...  0.2015 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2122...  Training loss: 1.5318...  0.2283 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2123...  Training loss: 1.5305...  0.2760 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2124...  Training loss: 1.5188...  0.1889 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2125...  Training loss: 1.5246...  0.2042 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2126...  Training loss: 1.4989...  0.2927 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2127...  Training loss: 1.5363...  0.2652 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2128...  Training loss: 1.5417...  0.2741 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2129...  Training loss: 1.5546...  0.1615 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2130...  Training loss: 1.5113...  0.1812 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2131...  Training loss: 1.5139...  0.2010 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2132...  Training loss: 1.4942...  0.2632 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2133...  Training loss: 1.5346...  0.2947 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2134...  Training loss: 1.5542...  0.2002 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2135...  Training loss: 1.5168...  0.2706 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2136...  Training loss: 1.4844...  0.1663 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2137...  Training loss: 1.4901...  0.1553 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2138...  Training loss: 1.5151...  0.2463 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2139...  Training loss: 1.5177...  0.2795 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2140...  Training loss: 1.5542...  0.2506 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2141...  Training loss: 1.5624...  0.1680 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2142...  Training loss: 1.5209...  0.1652 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2143...  Training loss: 1.5212...  0.1564 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2144...  Training loss: 1.5226...  0.2806 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2145...  Training loss: 1.5194...  0.2740 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2146...  Training loss: 1.5070...  0.1907 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2147...  Training loss: 1.5159...  0.2006 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2148...  Training loss: 1.5003...  0.2119 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2149...  Training loss: 1.5386...  0.1918 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2150...  Training loss: 1.5083...  0.2198 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2151...  Training loss: 1.5004...  0.1545 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2152...  Training loss: 1.4868...  0.2630 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2153...  Training loss: 1.5069...  0.2587 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2154...  Training loss: 1.5319...  0.2560 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2155...  Training loss: 1.5269...  0.2277 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2156...  Training loss: 1.5258...  0.2594 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2157...  Training loss: 1.4892...  0.2352 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2158...  Training loss: 1.5282...  0.1951 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2159...  Training loss: 1.4979...  0.2374 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2160...  Training loss: 1.5338...  0.1892 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2161...  Training loss: 1.4874...  0.1623 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2162...  Training loss: 1.5159...  0.2100 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2163...  Training loss: 1.5174...  0.2347 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2164...  Training loss: 1.5471...  0.2249 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2165...  Training loss: 1.4988...  0.2487 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2166...  Training loss: 1.5056...  0.1934 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2167...  Training loss: 1.5280...  0.2768 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2168...  Training loss: 1.5269...  0.2177 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2169...  Training loss: 1.5219...  0.1549 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2170...  Training loss: 1.5514...  0.1708 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2171...  Training loss: 1.5626...  0.1561 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2172...  Training loss: 1.4729...  0.2300 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2173...  Training loss: 1.4968...  0.3179 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2174...  Training loss: 1.5438...  0.2393 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2175...  Training loss: 1.5142...  0.2361 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2176...  Training loss: 1.5606...  0.2917 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2177...  Training loss: 1.4993...  0.3095 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2178...  Training loss: 1.5006...  0.2029 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2179...  Training loss: 1.5088...  0.2071 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2180...  Training loss: 1.5202...  0.1570 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2181...  Training loss: 1.5024...  0.2291 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2182...  Training loss: 1.5100...  0.1655 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2183...  Training loss: 1.5057...  0.1943 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2184...  Training loss: 1.4997...  0.2613 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2185...  Training loss: 1.4997...  0.2936 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/30...  Training Step: 2186...  Training loss: 1.4677...  0.2402 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2187...  Training loss: 1.5278...  0.1871 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2188...  Training loss: 1.5211...  0.2959 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2189...  Training loss: 1.5115...  0.2295 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2190...  Training loss: 1.4970...  0.2079 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2191...  Training loss: 1.5188...  0.2626 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2192...  Training loss: 1.5297...  0.2117 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2193...  Training loss: 1.5093...  0.1543 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2194...  Training loss: 1.4940...  0.1703 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2195...  Training loss: 1.4932...  0.1973 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2196...  Training loss: 1.4777...  0.1765 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2197...  Training loss: 1.4968...  0.2117 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2198...  Training loss: 1.5218...  0.2318 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2199...  Training loss: 1.4990...  0.1893 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2200...  Training loss: 1.5218...  0.1632 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2201...  Training loss: 1.5110...  0.1628 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2202...  Training loss: 1.5316...  0.1598 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2203...  Training loss: 1.5175...  0.1628 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2204...  Training loss: 1.4925...  0.2537 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2205...  Training loss: 1.4792...  0.2598 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2206...  Training loss: 1.4945...  0.2434 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2207...  Training loss: 1.5033...  0.1603 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2208...  Training loss: 1.5087...  0.1703 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2209...  Training loss: 1.5151...  0.1559 sec/batch\n",
      "Epoch: 13/30...  Training Step: 2210...  Training loss: 1.4913...  0.1739 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2211...  Training loss: 1.5816...  0.1905 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2212...  Training loss: 1.4703...  0.1599 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2213...  Training loss: 1.4837...  0.2553 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2214...  Training loss: 1.5279...  0.2537 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2215...  Training loss: 1.5276...  0.2566 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2216...  Training loss: 1.5498...  0.1927 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2217...  Training loss: 1.5046...  0.1615 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2218...  Training loss: 1.4925...  0.2333 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2219...  Training loss: 1.4721...  0.2912 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2220...  Training loss: 1.4711...  0.2866 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2221...  Training loss: 1.4740...  0.2957 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2222...  Training loss: 1.5059...  0.1618 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2223...  Training loss: 1.4885...  0.1878 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2224...  Training loss: 1.4915...  0.2126 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2225...  Training loss: 1.5508...  0.2250 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2226...  Training loss: 1.5503...  0.1621 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2227...  Training loss: 1.4807...  0.2350 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2228...  Training loss: 1.5049...  0.1645 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2229...  Training loss: 1.4938...  0.1773 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2230...  Training loss: 1.4956...  0.1740 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2231...  Training loss: 1.5049...  0.1605 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2232...  Training loss: 1.4598...  0.2232 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2233...  Training loss: 1.4725...  0.2086 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2234...  Training loss: 1.4649...  0.2218 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2235...  Training loss: 1.5207...  0.1729 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2236...  Training loss: 1.5098...  0.2688 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2237...  Training loss: 1.5233...  0.1799 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2238...  Training loss: 1.5186...  0.2558 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2239...  Training loss: 1.5008...  0.2104 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2240...  Training loss: 1.4703...  0.2586 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2241...  Training loss: 1.4953...  0.1626 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2242...  Training loss: 1.4895...  0.1777 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2243...  Training loss: 1.4945...  0.2221 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2244...  Training loss: 1.4800...  0.1965 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2245...  Training loss: 1.4792...  0.2386 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2246...  Training loss: 1.4994...  0.2662 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2247...  Training loss: 1.5405...  0.1730 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2248...  Training loss: 1.5167...  0.2255 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2249...  Training loss: 1.4980...  0.2026 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2250...  Training loss: 1.5141...  0.2577 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2251...  Training loss: 1.5152...  0.1899 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2252...  Training loss: 1.5030...  0.2343 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2253...  Training loss: 1.4727...  0.2405 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2254...  Training loss: 1.5400...  0.2920 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2255...  Training loss: 1.5271...  0.2109 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2256...  Training loss: 1.4940...  0.1776 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2257...  Training loss: 1.5237...  0.2500 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2258...  Training loss: 1.5278...  0.2463 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2259...  Training loss: 1.4946...  0.1711 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2260...  Training loss: 1.5093...  0.1873 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2261...  Training loss: 1.5631...  0.2336 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2262...  Training loss: 1.4877...  0.2295 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2263...  Training loss: 1.4886...  0.2234 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2264...  Training loss: 1.4847...  0.2675 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2265...  Training loss: 1.5060...  0.2528 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2266...  Training loss: 1.5110...  0.1852 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2267...  Training loss: 1.5141...  0.1850 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2268...  Training loss: 1.4812...  0.2447 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2269...  Training loss: 1.5223...  0.2397 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2270...  Training loss: 1.5062...  0.2038 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2271...  Training loss: 1.5126...  0.2831 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2272...  Training loss: 1.5034...  0.2746 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2273...  Training loss: 1.4822...  0.1840 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2274...  Training loss: 1.5009...  0.2570 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2275...  Training loss: 1.5258...  0.2399 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2276...  Training loss: 1.5188...  0.2240 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2277...  Training loss: 1.5248...  0.1967 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2278...  Training loss: 1.4973...  0.3227 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2279...  Training loss: 1.4917...  0.2762 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2280...  Training loss: 1.4915...  0.2963 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2281...  Training loss: 1.4614...  0.1532 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2282...  Training loss: 1.4933...  0.2468 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2283...  Training loss: 1.5104...  0.1980 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/30...  Training Step: 2284...  Training loss: 1.5348...  0.2692 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2285...  Training loss: 1.5247...  0.1838 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2286...  Training loss: 1.4645...  0.1873 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2287...  Training loss: 1.4803...  0.2339 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2288...  Training loss: 1.5100...  0.2619 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2289...  Training loss: 1.5155...  0.1892 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2290...  Training loss: 1.5274...  0.2430 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2291...  Training loss: 1.5186...  0.2784 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2292...  Training loss: 1.5113...  0.1851 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2293...  Training loss: 1.5120...  0.2205 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2294...  Training loss: 1.4860...  0.1726 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2295...  Training loss: 1.5064...  0.1581 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2296...  Training loss: 1.4856...  0.2130 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2297...  Training loss: 1.5045...  0.1783 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2298...  Training loss: 1.5195...  0.1596 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2299...  Training loss: 1.5167...  0.1674 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2300...  Training loss: 1.4865...  0.2606 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2301...  Training loss: 1.4907...  0.2627 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2302...  Training loss: 1.4848...  0.2477 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2303...  Training loss: 1.5082...  0.1877 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2304...  Training loss: 1.5316...  0.1608 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2305...  Training loss: 1.4866...  0.3268 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2306...  Training loss: 1.4585...  0.2362 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2307...  Training loss: 1.4623...  0.2084 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2308...  Training loss: 1.4895...  0.2626 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2309...  Training loss: 1.4930...  0.1803 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2310...  Training loss: 1.5345...  0.2401 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2311...  Training loss: 1.5328...  0.2681 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2312...  Training loss: 1.4969...  0.2885 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2313...  Training loss: 1.4950...  0.1589 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2314...  Training loss: 1.4947...  0.1711 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2315...  Training loss: 1.4884...  0.1979 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2316...  Training loss: 1.4868...  0.1710 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2317...  Training loss: 1.4953...  0.2639 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2318...  Training loss: 1.4716...  0.2080 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2319...  Training loss: 1.5090...  0.1716 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2320...  Training loss: 1.4772...  0.2705 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2321...  Training loss: 1.4801...  0.2868 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2322...  Training loss: 1.4578...  0.2721 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2323...  Training loss: 1.4832...  0.2228 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2324...  Training loss: 1.5137...  0.2271 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2325...  Training loss: 1.5100...  0.1547 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2326...  Training loss: 1.5021...  0.2233 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2327...  Training loss: 1.4621...  0.1981 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2328...  Training loss: 1.5000...  0.1939 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2329...  Training loss: 1.4765...  0.1832 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2330...  Training loss: 1.4999...  0.2324 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2331...  Training loss: 1.4746...  0.1647 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2332...  Training loss: 1.4832...  0.1882 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2333...  Training loss: 1.5023...  0.2625 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2334...  Training loss: 1.5304...  0.2689 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2335...  Training loss: 1.4683...  0.2138 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2336...  Training loss: 1.4801...  0.2182 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2337...  Training loss: 1.5020...  0.2740 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2338...  Training loss: 1.5037...  0.2888 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2339...  Training loss: 1.5037...  0.2526 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2340...  Training loss: 1.5315...  0.1625 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2341...  Training loss: 1.5469...  0.1784 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2342...  Training loss: 1.4589...  0.2641 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2343...  Training loss: 1.4662...  0.2699 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2344...  Training loss: 1.5219...  0.2034 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2345...  Training loss: 1.4920...  0.1596 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2346...  Training loss: 1.5293...  0.2454 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2347...  Training loss: 1.4733...  0.2884 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2348...  Training loss: 1.4744...  0.2634 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2349...  Training loss: 1.4783...  0.2445 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2350...  Training loss: 1.4939...  0.2628 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2351...  Training loss: 1.4769...  0.1905 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2352...  Training loss: 1.4941...  0.2189 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2353...  Training loss: 1.4783...  0.2289 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2354...  Training loss: 1.4796...  0.1685 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2355...  Training loss: 1.4795...  0.2495 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2356...  Training loss: 1.4473...  0.2398 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2357...  Training loss: 1.5091...  0.2495 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2358...  Training loss: 1.4921...  0.2613 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2359...  Training loss: 1.4905...  0.2510 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2360...  Training loss: 1.4705...  0.2598 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2361...  Training loss: 1.4841...  0.2328 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2362...  Training loss: 1.5132...  0.1576 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2363...  Training loss: 1.4837...  0.1579 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2364...  Training loss: 1.4744...  0.2062 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2365...  Training loss: 1.4614...  0.1748 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2366...  Training loss: 1.4529...  0.2342 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2367...  Training loss: 1.4728...  0.2833 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2368...  Training loss: 1.4849...  0.2317 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2369...  Training loss: 1.4743...  0.1891 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2370...  Training loss: 1.4999...  0.2312 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2371...  Training loss: 1.4724...  0.1916 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2372...  Training loss: 1.5015...  0.1570 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2373...  Training loss: 1.4841...  0.2096 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2374...  Training loss: 1.4713...  0.1607 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2375...  Training loss: 1.4624...  0.2598 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2376...  Training loss: 1.4759...  0.2194 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2377...  Training loss: 1.4734...  0.2488 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2378...  Training loss: 1.4892...  0.1998 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2379...  Training loss: 1.4982...  0.1915 sec/batch\n",
      "Epoch: 14/30...  Training Step: 2380...  Training loss: 1.4716...  0.2414 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2381...  Training loss: 1.5571...  0.2001 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/30...  Training Step: 2382...  Training loss: 1.4531...  0.2511 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2383...  Training loss: 1.4649...  0.2265 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2384...  Training loss: 1.5084...  0.2237 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2385...  Training loss: 1.5011...  0.2492 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2386...  Training loss: 1.5124...  0.2409 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2387...  Training loss: 1.4827...  0.2577 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2388...  Training loss: 1.4851...  0.2236 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2389...  Training loss: 1.4556...  0.2620 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2390...  Training loss: 1.4586...  0.2688 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2391...  Training loss: 1.4510...  0.2335 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2392...  Training loss: 1.4749...  0.1828 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2393...  Training loss: 1.4664...  0.1630 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2394...  Training loss: 1.4607...  0.2641 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2395...  Training loss: 1.5212...  0.2001 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2396...  Training loss: 1.5346...  0.1605 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2397...  Training loss: 1.4654...  0.2198 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2398...  Training loss: 1.4787...  0.2106 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2399...  Training loss: 1.4713...  0.1662 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2400...  Training loss: 1.4737...  0.1743 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2401...  Training loss: 1.4892...  0.2719 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2402...  Training loss: 1.4415...  0.2645 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2403...  Training loss: 1.4539...  0.2577 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2404...  Training loss: 1.4398...  0.1704 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2405...  Training loss: 1.4931...  0.2184 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2406...  Training loss: 1.4786...  0.1849 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2407...  Training loss: 1.5111...  0.2603 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2408...  Training loss: 1.4908...  0.2273 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2409...  Training loss: 1.4786...  0.1971 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2410...  Training loss: 1.4441...  0.2564 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2411...  Training loss: 1.4831...  0.2228 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2412...  Training loss: 1.4580...  0.2396 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2413...  Training loss: 1.4758...  0.2427 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2414...  Training loss: 1.4594...  0.1827 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2415...  Training loss: 1.4663...  0.2838 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2416...  Training loss: 1.4730...  0.2318 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2417...  Training loss: 1.5177...  0.2656 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2418...  Training loss: 1.4915...  0.1599 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2419...  Training loss: 1.4735...  0.2780 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2420...  Training loss: 1.4986...  0.1697 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2421...  Training loss: 1.5026...  0.2161 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2422...  Training loss: 1.4735...  0.1752 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2423...  Training loss: 1.4615...  0.1632 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2424...  Training loss: 1.5271...  0.1649 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2425...  Training loss: 1.5108...  0.2803 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2426...  Training loss: 1.4829...  0.2413 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2427...  Training loss: 1.5021...  0.2664 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2428...  Training loss: 1.4984...  0.2650 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2429...  Training loss: 1.4930...  0.2715 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2430...  Training loss: 1.4910...  0.2693 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2431...  Training loss: 1.5323...  0.2451 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2432...  Training loss: 1.4694...  0.2140 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2433...  Training loss: 1.4672...  0.2625 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2434...  Training loss: 1.4649...  0.2921 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2435...  Training loss: 1.4853...  0.2022 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2436...  Training loss: 1.4936...  0.2766 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2437...  Training loss: 1.4872...  0.1789 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2438...  Training loss: 1.4617...  0.2470 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2439...  Training loss: 1.5070...  0.1893 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2440...  Training loss: 1.4952...  0.2490 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2441...  Training loss: 1.4853...  0.1925 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2442...  Training loss: 1.4861...  0.1696 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2443...  Training loss: 1.4598...  0.2395 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2444...  Training loss: 1.4845...  0.2155 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2445...  Training loss: 1.4940...  0.2194 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2446...  Training loss: 1.4853...  0.2534 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2447...  Training loss: 1.4991...  0.2283 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2448...  Training loss: 1.4739...  0.2295 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2449...  Training loss: 1.4697...  0.2372 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2450...  Training loss: 1.4676...  0.1674 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2451...  Training loss: 1.4491...  0.1575 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2452...  Training loss: 1.4734...  0.1768 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2453...  Training loss: 1.4834...  0.2075 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2454...  Training loss: 1.4903...  0.2387 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2455...  Training loss: 1.4952...  0.1908 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2456...  Training loss: 1.4452...  0.1937 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2457...  Training loss: 1.4636...  0.1799 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2458...  Training loss: 1.4875...  0.1673 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2459...  Training loss: 1.5095...  0.2171 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2460...  Training loss: 1.5029...  0.2358 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2461...  Training loss: 1.4945...  0.2551 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2462...  Training loss: 1.4945...  0.2598 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2463...  Training loss: 1.4889...  0.2408 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2464...  Training loss: 1.4619...  0.2500 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2465...  Training loss: 1.4871...  0.2345 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2466...  Training loss: 1.4669...  0.2287 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2467...  Training loss: 1.4854...  0.1668 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2468...  Training loss: 1.4951...  0.1612 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2469...  Training loss: 1.5010...  0.1890 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2470...  Training loss: 1.4646...  0.2316 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2471...  Training loss: 1.4641...  0.2235 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2472...  Training loss: 1.4509...  0.2601 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2473...  Training loss: 1.4885...  0.2306 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2474...  Training loss: 1.5046...  0.1865 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2475...  Training loss: 1.4681...  0.2563 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2476...  Training loss: 1.4441...  0.2782 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2477...  Training loss: 1.4510...  0.1810 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2478...  Training loss: 1.4671...  0.1834 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2479...  Training loss: 1.4739...  0.1577 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/30...  Training Step: 2480...  Training loss: 1.5155...  0.1789 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2481...  Training loss: 1.5203...  0.2294 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2482...  Training loss: 1.4935...  0.2303 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2483...  Training loss: 1.4777...  0.2335 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2484...  Training loss: 1.4736...  0.2309 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2485...  Training loss: 1.4686...  0.1892 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2486...  Training loss: 1.4681...  0.1993 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2487...  Training loss: 1.4768...  0.2674 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2488...  Training loss: 1.4548...  0.1726 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2489...  Training loss: 1.4936...  0.1699 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2490...  Training loss: 1.4564...  0.2465 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2491...  Training loss: 1.4642...  0.2516 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2492...  Training loss: 1.4362...  0.2134 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2493...  Training loss: 1.4638...  0.2344 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2494...  Training loss: 1.4854...  0.2805 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2495...  Training loss: 1.4961...  0.2075 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2496...  Training loss: 1.4797...  0.2189 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2497...  Training loss: 1.4528...  0.2068 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2498...  Training loss: 1.4775...  0.2395 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2499...  Training loss: 1.4641...  0.2627 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2500...  Training loss: 1.4817...  0.1868 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2501...  Training loss: 1.4484...  0.1838 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2502...  Training loss: 1.4696...  0.2181 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2503...  Training loss: 1.4759...  0.2383 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2504...  Training loss: 1.4972...  0.2394 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2505...  Training loss: 1.4560...  0.2525 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2506...  Training loss: 1.4548...  0.2161 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2507...  Training loss: 1.4819...  0.2180 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2508...  Training loss: 1.4722...  0.2871 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2509...  Training loss: 1.4855...  0.2670 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2510...  Training loss: 1.5053...  0.2709 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2511...  Training loss: 1.5122...  0.2563 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2512...  Training loss: 1.4356...  0.1610 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2513...  Training loss: 1.4442...  0.2629 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2514...  Training loss: 1.4965...  0.2672 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2515...  Training loss: 1.4874...  0.1856 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2516...  Training loss: 1.5026...  0.2351 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2517...  Training loss: 1.4575...  0.1836 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2518...  Training loss: 1.4556...  0.2409 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2519...  Training loss: 1.4701...  0.1818 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2520...  Training loss: 1.4808...  0.2115 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2521...  Training loss: 1.4654...  0.1599 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2522...  Training loss: 1.4686...  0.1580 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2523...  Training loss: 1.4598...  0.1591 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2524...  Training loss: 1.4502...  0.1711 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2525...  Training loss: 1.4601...  0.2219 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2526...  Training loss: 1.4262...  0.2178 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2527...  Training loss: 1.4869...  0.2269 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2528...  Training loss: 1.4695...  0.2215 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2529...  Training loss: 1.4710...  0.2808 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2530...  Training loss: 1.4464...  0.2548 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2531...  Training loss: 1.4752...  0.2751 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2532...  Training loss: 1.4944...  0.2508 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2533...  Training loss: 1.4546...  0.2189 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2534...  Training loss: 1.4478...  0.1549 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2535...  Training loss: 1.4582...  0.1934 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2536...  Training loss: 1.4341...  0.2075 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2537...  Training loss: 1.4554...  0.2065 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2538...  Training loss: 1.4692...  0.1587 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2539...  Training loss: 1.4537...  0.2174 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2540...  Training loss: 1.4834...  0.2280 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2541...  Training loss: 1.4606...  0.2404 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2542...  Training loss: 1.4928...  0.2470 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2543...  Training loss: 1.4666...  0.2093 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2544...  Training loss: 1.4556...  0.2291 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2545...  Training loss: 1.4527...  0.2512 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2546...  Training loss: 1.4607...  0.2461 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2547...  Training loss: 1.4719...  0.2155 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2548...  Training loss: 1.4670...  0.2145 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2549...  Training loss: 1.4736...  0.2077 sec/batch\n",
      "Epoch: 15/30...  Training Step: 2550...  Training loss: 1.4579...  0.2361 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2551...  Training loss: 1.5427...  0.1616 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2552...  Training loss: 1.4393...  0.1915 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2553...  Training loss: 1.4592...  0.2425 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2554...  Training loss: 1.4942...  0.2773 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2555...  Training loss: 1.4800...  0.2389 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2556...  Training loss: 1.4912...  0.2378 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2557...  Training loss: 1.4764...  0.1832 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2558...  Training loss: 1.4577...  0.1860 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2559...  Training loss: 1.4319...  0.2578 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2560...  Training loss: 1.4335...  0.2658 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2561...  Training loss: 1.4463...  0.2260 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2562...  Training loss: 1.4612...  0.1933 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2563...  Training loss: 1.4480...  0.2010 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2564...  Training loss: 1.4520...  0.2271 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2565...  Training loss: 1.5100...  0.2317 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2566...  Training loss: 1.5031...  0.1620 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2567...  Training loss: 1.4432...  0.1638 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2568...  Training loss: 1.4662...  0.2444 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2569...  Training loss: 1.4625...  0.1830 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2570...  Training loss: 1.4517...  0.2380 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2571...  Training loss: 1.4665...  0.2117 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2572...  Training loss: 1.4260...  0.2312 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2573...  Training loss: 1.4287...  0.2397 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2574...  Training loss: 1.4188...  0.1923 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2575...  Training loss: 1.4867...  0.2424 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2576...  Training loss: 1.4617...  0.2676 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2577...  Training loss: 1.4804...  0.1681 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/30...  Training Step: 2578...  Training loss: 1.4805...  0.2208 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2579...  Training loss: 1.4549...  0.1809 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2580...  Training loss: 1.4301...  0.2078 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2581...  Training loss: 1.4569...  0.2071 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2582...  Training loss: 1.4442...  0.2007 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2583...  Training loss: 1.4598...  0.2233 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2584...  Training loss: 1.4386...  0.1767 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2585...  Training loss: 1.4437...  0.2486 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2586...  Training loss: 1.4564...  0.2319 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2587...  Training loss: 1.4955...  0.2415 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2588...  Training loss: 1.4811...  0.2508 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2589...  Training loss: 1.4550...  0.2090 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2590...  Training loss: 1.4822...  0.1957 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2591...  Training loss: 1.4797...  0.2132 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2592...  Training loss: 1.4505...  0.2370 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2593...  Training loss: 1.4384...  0.1750 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2594...  Training loss: 1.5036...  0.1769 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2595...  Training loss: 1.4946...  0.2653 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2596...  Training loss: 1.4502...  0.2318 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2597...  Training loss: 1.4826...  0.1960 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2598...  Training loss: 1.4752...  0.1729 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2599...  Training loss: 1.4673...  0.2685 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2600...  Training loss: 1.4682...  0.2619 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2601...  Training loss: 1.5099...  0.1933 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2602...  Training loss: 1.4458...  0.1918 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2603...  Training loss: 1.4498...  0.1987 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2604...  Training loss: 1.4536...  0.1883 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2605...  Training loss: 1.4635...  0.1963 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2606...  Training loss: 1.4708...  0.1700 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2607...  Training loss: 1.4665...  0.1787 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2608...  Training loss: 1.4423...  0.1650 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2609...  Training loss: 1.4843...  0.2380 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2610...  Training loss: 1.4775...  0.2538 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2611...  Training loss: 1.4744...  0.1778 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2612...  Training loss: 1.4611...  0.2098 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2613...  Training loss: 1.4325...  0.1980 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2614...  Training loss: 1.4582...  0.2496 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2615...  Training loss: 1.4821...  0.2386 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2616...  Training loss: 1.4775...  0.2784 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2617...  Training loss: 1.4932...  0.2761 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2618...  Training loss: 1.4652...  0.2956 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2619...  Training loss: 1.4480...  0.1968 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2620...  Training loss: 1.4553...  0.1981 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2621...  Training loss: 1.4327...  0.2560 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2622...  Training loss: 1.4535...  0.2031 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2623...  Training loss: 1.4641...  0.2316 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2624...  Training loss: 1.4908...  0.2618 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2625...  Training loss: 1.4843...  0.2678 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2626...  Training loss: 1.4255...  0.1627 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2627...  Training loss: 1.4498...  0.2310 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2628...  Training loss: 1.4753...  0.1748 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2629...  Training loss: 1.4886...  0.2183 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2630...  Training loss: 1.4835...  0.2543 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2631...  Training loss: 1.4836...  0.2049 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2632...  Training loss: 1.4651...  0.1983 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2633...  Training loss: 1.4567...  0.2225 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2634...  Training loss: 1.4487...  0.2039 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2635...  Training loss: 1.4569...  0.2254 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2636...  Training loss: 1.4498...  0.2346 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2637...  Training loss: 1.4580...  0.2228 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2638...  Training loss: 1.4800...  0.2237 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2639...  Training loss: 1.4771...  0.2070 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2640...  Training loss: 1.4429...  0.1774 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2641...  Training loss: 1.4430...  0.2229 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2642...  Training loss: 1.4351...  0.2452 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2643...  Training loss: 1.4687...  0.2557 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2644...  Training loss: 1.4914...  0.2190 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2645...  Training loss: 1.4526...  0.2051 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2646...  Training loss: 1.4228...  0.1983 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2647...  Training loss: 1.4375...  0.1815 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2648...  Training loss: 1.4463...  0.2360 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2649...  Training loss: 1.4601...  0.2417 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2650...  Training loss: 1.4978...  0.1827 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2651...  Training loss: 1.4980...  0.1991 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2652...  Training loss: 1.4626...  0.1659 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2653...  Training loss: 1.4560...  0.1596 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2654...  Training loss: 1.4623...  0.1920 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2655...  Training loss: 1.4497...  0.2190 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2656...  Training loss: 1.4538...  0.1539 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2657...  Training loss: 1.4585...  0.2145 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2658...  Training loss: 1.4414...  0.2587 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2659...  Training loss: 1.4721...  0.1767 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2660...  Training loss: 1.4470...  0.1607 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2661...  Training loss: 1.4327...  0.1558 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2662...  Training loss: 1.4199...  0.2185 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2663...  Training loss: 1.4368...  0.1613 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2664...  Training loss: 1.4660...  0.2829 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2665...  Training loss: 1.4663...  0.2336 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2666...  Training loss: 1.4656...  0.1975 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2667...  Training loss: 1.4272...  0.2530 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2668...  Training loss: 1.4651...  0.2008 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2669...  Training loss: 1.4394...  0.1979 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2670...  Training loss: 1.4542...  0.2470 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2671...  Training loss: 1.4359...  0.1917 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2672...  Training loss: 1.4561...  0.1688 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2673...  Training loss: 1.4581...  0.2552 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2674...  Training loss: 1.4835...  0.2474 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2675...  Training loss: 1.4293...  0.2540 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/30...  Training Step: 2676...  Training loss: 1.4425...  0.2015 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2677...  Training loss: 1.4571...  0.2012 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2678...  Training loss: 1.4602...  0.1883 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2679...  Training loss: 1.4649...  0.1662 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2680...  Training loss: 1.4889...  0.2281 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2681...  Training loss: 1.4970...  0.2684 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2682...  Training loss: 1.4249...  0.2506 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2683...  Training loss: 1.4302...  0.2199 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2684...  Training loss: 1.4837...  0.2130 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2685...  Training loss: 1.4511...  0.1697 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2686...  Training loss: 1.4879...  0.1657 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2687...  Training loss: 1.4304...  0.2276 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2688...  Training loss: 1.4443...  0.1999 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2689...  Training loss: 1.4474...  0.2245 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2690...  Training loss: 1.4533...  0.2114 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2691...  Training loss: 1.4376...  0.1632 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2692...  Training loss: 1.4509...  0.2203 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2693...  Training loss: 1.4532...  0.2397 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2694...  Training loss: 1.4418...  0.1735 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2695...  Training loss: 1.4366...  0.1908 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2696...  Training loss: 1.4058...  0.2291 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2697...  Training loss: 1.4662...  0.2100 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2698...  Training loss: 1.4532...  0.2395 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2699...  Training loss: 1.4493...  0.2324 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2700...  Training loss: 1.4322...  0.2319 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2701...  Training loss: 1.4564...  0.1646 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2702...  Training loss: 1.4784...  0.1891 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2703...  Training loss: 1.4466...  0.2114 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2704...  Training loss: 1.4356...  0.1839 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2705...  Training loss: 1.4323...  0.2009 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2706...  Training loss: 1.4154...  0.2049 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2707...  Training loss: 1.4457...  0.1701 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2708...  Training loss: 1.4562...  0.2187 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2709...  Training loss: 1.4291...  0.2069 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2710...  Training loss: 1.4564...  0.2278 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2711...  Training loss: 1.4436...  0.2611 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2712...  Training loss: 1.4826...  0.2650 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2713...  Training loss: 1.4605...  0.1883 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2714...  Training loss: 1.4387...  0.1580 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2715...  Training loss: 1.4313...  0.1649 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2716...  Training loss: 1.4363...  0.2471 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2717...  Training loss: 1.4499...  0.1856 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2718...  Training loss: 1.4465...  0.1945 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2719...  Training loss: 1.4517...  0.2168 sec/batch\n",
      "Epoch: 16/30...  Training Step: 2720...  Training loss: 1.4369...  0.1959 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2721...  Training loss: 1.5146...  0.2427 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2722...  Training loss: 1.4243...  0.2553 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2723...  Training loss: 1.4333...  0.2155 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2724...  Training loss: 1.4777...  0.1597 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2725...  Training loss: 1.4586...  0.2364 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2726...  Training loss: 1.4824...  0.1898 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2727...  Training loss: 1.4459...  0.2128 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2728...  Training loss: 1.4343...  0.1897 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2729...  Training loss: 1.4133...  0.2673 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2730...  Training loss: 1.4197...  0.1710 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2731...  Training loss: 1.4033...  0.2083 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2732...  Training loss: 1.4433...  0.2694 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2733...  Training loss: 1.4236...  0.2576 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2734...  Training loss: 1.4366...  0.2277 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2735...  Training loss: 1.4944...  0.1897 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2736...  Training loss: 1.4852...  0.2845 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2737...  Training loss: 1.4287...  0.2146 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2738...  Training loss: 1.4382...  0.2422 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2739...  Training loss: 1.4461...  0.2503 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2740...  Training loss: 1.4400...  0.3066 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2741...  Training loss: 1.4529...  0.2503 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2742...  Training loss: 1.3972...  0.2158 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2743...  Training loss: 1.4129...  0.2369 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2744...  Training loss: 1.4073...  0.2306 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2745...  Training loss: 1.4531...  0.2474 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2746...  Training loss: 1.4413...  0.2432 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2747...  Training loss: 1.4641...  0.2339 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2748...  Training loss: 1.4717...  0.1876 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2749...  Training loss: 1.4390...  0.1676 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2750...  Training loss: 1.4060...  0.2467 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2751...  Training loss: 1.4413...  0.2743 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2752...  Training loss: 1.4370...  0.2356 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2753...  Training loss: 1.4374...  0.1535 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2754...  Training loss: 1.4216...  0.2344 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2755...  Training loss: 1.4224...  0.2761 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2756...  Training loss: 1.4397...  0.2238 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2757...  Training loss: 1.4727...  0.2483 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2758...  Training loss: 1.4509...  0.1698 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2759...  Training loss: 1.4418...  0.1769 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2760...  Training loss: 1.4569...  0.2471 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2761...  Training loss: 1.4588...  0.2451 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2762...  Training loss: 1.4536...  0.2377 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2763...  Training loss: 1.4197...  0.1800 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2764...  Training loss: 1.4954...  0.2744 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2765...  Training loss: 1.4781...  0.2670 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2766...  Training loss: 1.4406...  0.2096 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2767...  Training loss: 1.4595...  0.2132 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2768...  Training loss: 1.4625...  0.1778 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2769...  Training loss: 1.4446...  0.1896 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2770...  Training loss: 1.4389...  0.2543 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2771...  Training loss: 1.4863...  0.2426 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2772...  Training loss: 1.4309...  0.1910 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2773...  Training loss: 1.4295...  0.1796 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/30...  Training Step: 2774...  Training loss: 1.4278...  0.2417 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2775...  Training loss: 1.4514...  0.2593 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2776...  Training loss: 1.4654...  0.2266 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2777...  Training loss: 1.4515...  0.2270 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2778...  Training loss: 1.4179...  0.2022 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2779...  Training loss: 1.4642...  0.2007 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2780...  Training loss: 1.4579...  0.2643 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2781...  Training loss: 1.4464...  0.1935 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2782...  Training loss: 1.4319...  0.2011 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2783...  Training loss: 1.4266...  0.2335 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2784...  Training loss: 1.4428...  0.2175 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2785...  Training loss: 1.4614...  0.1846 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2786...  Training loss: 1.4507...  0.2356 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2787...  Training loss: 1.4620...  0.2009 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2788...  Training loss: 1.4414...  0.2291 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2789...  Training loss: 1.4303...  0.2660 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2790...  Training loss: 1.4375...  0.1844 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2791...  Training loss: 1.4137...  0.1559 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2792...  Training loss: 1.4326...  0.1688 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2793...  Training loss: 1.4435...  0.2495 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2794...  Training loss: 1.4674...  0.2655 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2795...  Training loss: 1.4596...  0.2261 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2796...  Training loss: 1.4148...  0.2304 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2797...  Training loss: 1.4367...  0.2093 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2798...  Training loss: 1.4547...  0.1867 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2799...  Training loss: 1.4664...  0.3131 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2800...  Training loss: 1.4668...  0.1974 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2801...  Training loss: 1.4687...  0.1572 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2802...  Training loss: 1.4658...  0.2507 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2803...  Training loss: 1.4531...  0.2423 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2804...  Training loss: 1.4277...  0.2302 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2805...  Training loss: 1.4436...  0.1850 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2806...  Training loss: 1.4309...  0.2425 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2807...  Training loss: 1.4458...  0.2463 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2808...  Training loss: 1.4735...  0.2602 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2809...  Training loss: 1.4680...  0.2678 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2810...  Training loss: 1.4215...  0.3060 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2811...  Training loss: 1.4322...  0.2379 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2812...  Training loss: 1.4151...  0.2218 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2813...  Training loss: 1.4524...  0.2071 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2814...  Training loss: 1.4685...  0.2752 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2815...  Training loss: 1.4374...  0.1572 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2816...  Training loss: 1.3987...  0.1595 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2817...  Training loss: 1.4144...  0.2841 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2818...  Training loss: 1.4289...  0.2265 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2819...  Training loss: 1.4376...  0.2614 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2820...  Training loss: 1.4852...  0.3013 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2821...  Training loss: 1.4717...  0.2649 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2822...  Training loss: 1.4482...  0.2948 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2823...  Training loss: 1.4413...  0.1912 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2824...  Training loss: 1.4382...  0.2868 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2825...  Training loss: 1.4317...  0.1688 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2826...  Training loss: 1.4369...  0.1569 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2827...  Training loss: 1.4476...  0.1873 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2828...  Training loss: 1.4228...  0.2360 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2829...  Training loss: 1.4565...  0.2874 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2830...  Training loss: 1.4277...  0.1594 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2831...  Training loss: 1.4185...  0.2471 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2832...  Training loss: 1.4112...  0.2243 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2833...  Training loss: 1.4321...  0.2772 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2834...  Training loss: 1.4471...  0.2223 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2835...  Training loss: 1.4519...  0.1566 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2836...  Training loss: 1.4403...  0.1691 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2837...  Training loss: 1.4069...  0.1857 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2838...  Training loss: 1.4394...  0.2355 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2839...  Training loss: 1.4268...  0.2136 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2840...  Training loss: 1.4509...  0.2598 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2841...  Training loss: 1.4174...  0.2786 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2842...  Training loss: 1.4378...  0.1559 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2843...  Training loss: 1.4338...  0.1968 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2844...  Training loss: 1.4623...  0.2121 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2845...  Training loss: 1.4115...  0.1749 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2846...  Training loss: 1.4174...  0.2565 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2847...  Training loss: 1.4476...  0.2433 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2848...  Training loss: 1.4393...  0.2333 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2849...  Training loss: 1.4471...  0.2304 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2850...  Training loss: 1.4697...  0.1659 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2851...  Training loss: 1.4724...  0.1762 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2852...  Training loss: 1.3925...  0.2200 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2853...  Training loss: 1.4065...  0.2564 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2854...  Training loss: 1.4764...  0.1876 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2855...  Training loss: 1.4367...  0.1657 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2856...  Training loss: 1.4724...  0.1846 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2857...  Training loss: 1.4142...  0.2830 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2858...  Training loss: 1.4303...  0.2053 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2859...  Training loss: 1.4320...  0.2116 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2860...  Training loss: 1.4399...  0.2635 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2861...  Training loss: 1.4334...  0.2151 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2862...  Training loss: 1.4273...  0.2158 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2863...  Training loss: 1.4237...  0.2658 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2864...  Training loss: 1.4228...  0.2726 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2865...  Training loss: 1.4274...  0.2045 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2866...  Training loss: 1.3981...  0.2002 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2867...  Training loss: 1.4614...  0.2335 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2868...  Training loss: 1.4359...  0.2115 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2869...  Training loss: 1.4305...  0.2750 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2870...  Training loss: 1.4247...  0.2688 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2871...  Training loss: 1.4446...  0.2065 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2872...  Training loss: 1.4627...  0.1631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/30...  Training Step: 2873...  Training loss: 1.4346...  0.2119 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2874...  Training loss: 1.4156...  0.1789 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2875...  Training loss: 1.4138...  0.2171 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2876...  Training loss: 1.3976...  0.2849 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2877...  Training loss: 1.4239...  0.1885 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2878...  Training loss: 1.4357...  0.2147 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2879...  Training loss: 1.4149...  0.1570 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2880...  Training loss: 1.4522...  0.2262 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2881...  Training loss: 1.4230...  0.2540 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2882...  Training loss: 1.4554...  0.2389 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2883...  Training loss: 1.4318...  0.1762 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2884...  Training loss: 1.4283...  0.1608 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2885...  Training loss: 1.4118...  0.1583 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2886...  Training loss: 1.4214...  0.2892 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2887...  Training loss: 1.4304...  0.1955 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2888...  Training loss: 1.4331...  0.2544 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2889...  Training loss: 1.4321...  0.2773 sec/batch\n",
      "Epoch: 17/30...  Training Step: 2890...  Training loss: 1.4228...  0.2522 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2891...  Training loss: 1.5049...  0.1627 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2892...  Training loss: 1.3973...  0.2802 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2893...  Training loss: 1.4137...  0.2482 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2894...  Training loss: 1.4512...  0.1834 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2895...  Training loss: 1.4576...  0.2103 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2896...  Training loss: 1.4559...  0.2647 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2897...  Training loss: 1.4226...  0.1880 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2898...  Training loss: 1.4277...  0.2682 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2899...  Training loss: 1.3962...  0.2362 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2900...  Training loss: 1.3936...  0.2366 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2901...  Training loss: 1.3913...  0.1598 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2902...  Training loss: 1.4221...  0.2276 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2903...  Training loss: 1.4171...  0.2308 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2904...  Training loss: 1.4225...  0.1552 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2905...  Training loss: 1.4712...  0.1684 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2906...  Training loss: 1.4703...  0.1900 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2907...  Training loss: 1.4155...  0.1893 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2908...  Training loss: 1.4275...  0.1827 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2909...  Training loss: 1.4216...  0.2794 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2910...  Training loss: 1.4160...  0.2295 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2911...  Training loss: 1.4430...  0.2961 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2912...  Training loss: 1.3893...  0.1992 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2913...  Training loss: 1.4000...  0.2570 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2914...  Training loss: 1.4011...  0.1760 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2915...  Training loss: 1.4470...  0.3052 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2916...  Training loss: 1.4273...  0.2395 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2917...  Training loss: 1.4457...  0.2613 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2918...  Training loss: 1.4348...  0.2300 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2919...  Training loss: 1.4283...  0.1961 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2920...  Training loss: 1.3913...  0.1661 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2921...  Training loss: 1.4179...  0.2975 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2922...  Training loss: 1.4078...  0.2344 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2923...  Training loss: 1.4209...  0.2333 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2924...  Training loss: 1.4074...  0.2465 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2925...  Training loss: 1.4135...  0.2948 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2926...  Training loss: 1.4254...  0.2071 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2927...  Training loss: 1.4565...  0.2729 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2928...  Training loss: 1.4390...  0.2590 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2929...  Training loss: 1.4129...  0.1633 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2930...  Training loss: 1.4467...  0.1575 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2931...  Training loss: 1.4359...  0.2806 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2932...  Training loss: 1.4333...  0.2004 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2933...  Training loss: 1.4108...  0.1968 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2934...  Training loss: 1.4769...  0.2232 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2935...  Training loss: 1.4551...  0.2187 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2936...  Training loss: 1.4299...  0.1976 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2937...  Training loss: 1.4450...  0.1581 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2938...  Training loss: 1.4516...  0.1883 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2939...  Training loss: 1.4332...  0.2182 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2940...  Training loss: 1.4463...  0.1929 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2941...  Training loss: 1.4677...  0.2855 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2942...  Training loss: 1.4171...  0.2411 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2943...  Training loss: 1.4093...  0.1594 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2944...  Training loss: 1.4062...  0.2344 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2945...  Training loss: 1.4300...  0.2081 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2946...  Training loss: 1.4413...  0.2482 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2947...  Training loss: 1.4332...  0.2134 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2948...  Training loss: 1.4064...  0.2522 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2949...  Training loss: 1.4502...  0.2474 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2950...  Training loss: 1.4456...  0.2551 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2951...  Training loss: 1.4429...  0.2408 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2952...  Training loss: 1.4332...  0.2538 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2953...  Training loss: 1.4059...  0.1748 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2954...  Training loss: 1.4261...  0.2192 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2955...  Training loss: 1.4461...  0.1697 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2956...  Training loss: 1.4374...  0.1990 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2957...  Training loss: 1.4485...  0.1988 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2958...  Training loss: 1.4301...  0.2312 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2959...  Training loss: 1.4159...  0.2617 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2960...  Training loss: 1.4272...  0.2130 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2961...  Training loss: 1.3982...  0.2265 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2962...  Training loss: 1.4169...  0.1801 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2963...  Training loss: 1.4434...  0.1987 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2964...  Training loss: 1.4505...  0.2502 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2965...  Training loss: 1.4462...  0.2512 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2966...  Training loss: 1.4006...  0.2285 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2967...  Training loss: 1.4116...  0.1991 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2968...  Training loss: 1.4348...  0.2349 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2969...  Training loss: 1.4587...  0.2322 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2970...  Training loss: 1.4537...  0.2393 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/30...  Training Step: 2971...  Training loss: 1.4451...  0.2435 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2972...  Training loss: 1.4326...  0.1682 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2973...  Training loss: 1.4416...  0.1586 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2974...  Training loss: 1.4038...  0.1959 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2975...  Training loss: 1.4357...  0.2058 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2976...  Training loss: 1.4139...  0.1718 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2977...  Training loss: 1.4355...  0.2481 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2978...  Training loss: 1.4507...  0.2183 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2979...  Training loss: 1.4484...  0.2244 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2980...  Training loss: 1.4039...  0.1541 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2981...  Training loss: 1.4120...  0.2127 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2982...  Training loss: 1.4064...  0.1799 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2983...  Training loss: 1.4373...  0.1968 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2984...  Training loss: 1.4530...  0.1610 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2985...  Training loss: 1.4228...  0.2051 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2986...  Training loss: 1.3970...  0.2103 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2987...  Training loss: 1.4007...  0.2449 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2988...  Training loss: 1.4190...  0.2064 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2989...  Training loss: 1.4286...  0.1990 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2990...  Training loss: 1.4658...  0.1607 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2991...  Training loss: 1.4686...  0.1954 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2992...  Training loss: 1.4339...  0.2145 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2993...  Training loss: 1.4265...  0.2105 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2994...  Training loss: 1.4253...  0.1697 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2995...  Training loss: 1.4209...  0.2374 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2996...  Training loss: 1.4164...  0.2003 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2997...  Training loss: 1.4243...  0.1989 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2998...  Training loss: 1.4028...  0.1690 sec/batch\n",
      "Epoch: 18/30...  Training Step: 2999...  Training loss: 1.4422...  0.1956 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3000...  Training loss: 1.4152...  0.1574 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3001...  Training loss: 1.4161...  0.1684 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3002...  Training loss: 1.3854...  0.2547 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3003...  Training loss: 1.4194...  0.1858 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3004...  Training loss: 1.4301...  0.2233 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3005...  Training loss: 1.4497...  0.2546 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3006...  Training loss: 1.4318...  0.2637 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3007...  Training loss: 1.3961...  0.2465 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3008...  Training loss: 1.4348...  0.2134 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3009...  Training loss: 1.4115...  0.2186 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3010...  Training loss: 1.4343...  0.1795 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3011...  Training loss: 1.4198...  0.1900 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3012...  Training loss: 1.4294...  0.2044 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3013...  Training loss: 1.4228...  0.2399 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3014...  Training loss: 1.4564...  0.1959 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3015...  Training loss: 1.3965...  0.1634 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3016...  Training loss: 1.4129...  0.1859 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3017...  Training loss: 1.4307...  0.2595 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3018...  Training loss: 1.4223...  0.2136 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3019...  Training loss: 1.4277...  0.3031 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3020...  Training loss: 1.4555...  0.1594 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3021...  Training loss: 1.4640...  0.2382 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3022...  Training loss: 1.3824...  0.2882 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3023...  Training loss: 1.3915...  0.1811 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3024...  Training loss: 1.4468...  0.1810 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3025...  Training loss: 1.4286...  0.2195 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3026...  Training loss: 1.4524...  0.2478 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3027...  Training loss: 1.3913...  0.2646 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3028...  Training loss: 1.4097...  0.2617 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3029...  Training loss: 1.4136...  0.1713 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3030...  Training loss: 1.4232...  0.1571 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3031...  Training loss: 1.4110...  0.2692 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3032...  Training loss: 1.4172...  0.3002 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3033...  Training loss: 1.4164...  0.1553 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3034...  Training loss: 1.4020...  0.2261 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3035...  Training loss: 1.4065...  0.2539 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3036...  Training loss: 1.3822...  0.2354 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3037...  Training loss: 1.4342...  0.2468 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3038...  Training loss: 1.4251...  0.2250 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3039...  Training loss: 1.4225...  0.1980 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3040...  Training loss: 1.4102...  0.1752 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3041...  Training loss: 1.4290...  0.1814 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3042...  Training loss: 1.4439...  0.2624 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3043...  Training loss: 1.4140...  0.2436 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3044...  Training loss: 1.3919...  0.2945 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3045...  Training loss: 1.3986...  0.3189 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3046...  Training loss: 1.3906...  0.2077 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3047...  Training loss: 1.4113...  0.1821 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3048...  Training loss: 1.4227...  0.2600 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3049...  Training loss: 1.4062...  0.1981 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3050...  Training loss: 1.4293...  0.2650 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3051...  Training loss: 1.4170...  0.2842 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3052...  Training loss: 1.4341...  0.2177 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3053...  Training loss: 1.4223...  0.1824 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3054...  Training loss: 1.4137...  0.1609 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3055...  Training loss: 1.3982...  0.2441 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3056...  Training loss: 1.4010...  0.1727 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3057...  Training loss: 1.4249...  0.2244 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3058...  Training loss: 1.4222...  0.1779 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3059...  Training loss: 1.4209...  0.1595 sec/batch\n",
      "Epoch: 18/30...  Training Step: 3060...  Training loss: 1.4001...  0.2630 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3061...  Training loss: 1.4810...  0.1973 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3062...  Training loss: 1.3827...  0.1780 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3063...  Training loss: 1.4013...  0.2478 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3064...  Training loss: 1.4411...  0.2314 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3065...  Training loss: 1.4321...  0.2755 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3066...  Training loss: 1.4516...  0.1718 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3067...  Training loss: 1.4189...  0.2768 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3068...  Training loss: 1.4083...  0.2940 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3069...  Training loss: 1.3868...  0.1941 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/30...  Training Step: 3070...  Training loss: 1.3866...  0.3034 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3071...  Training loss: 1.3827...  0.1665 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3072...  Training loss: 1.4110...  0.1762 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3073...  Training loss: 1.4042...  0.1583 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3074...  Training loss: 1.3986...  0.2039 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3075...  Training loss: 1.4596...  0.2587 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3076...  Training loss: 1.4574...  0.2113 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3077...  Training loss: 1.4011...  0.2027 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3078...  Training loss: 1.4202...  0.1682 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3079...  Training loss: 1.4150...  0.1633 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3080...  Training loss: 1.4080...  0.2749 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3081...  Training loss: 1.4201...  0.2536 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3082...  Training loss: 1.3707...  0.2035 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3083...  Training loss: 1.3835...  0.2178 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3084...  Training loss: 1.3822...  0.2183 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3085...  Training loss: 1.4287...  0.2257 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3086...  Training loss: 1.4144...  0.1538 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3087...  Training loss: 1.4352...  0.1740 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3088...  Training loss: 1.4312...  0.2382 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3089...  Training loss: 1.4038...  0.1641 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3090...  Training loss: 1.3749...  0.2062 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3091...  Training loss: 1.4162...  0.2381 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3092...  Training loss: 1.4024...  0.2503 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3093...  Training loss: 1.3976...  0.2347 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3094...  Training loss: 1.4008...  0.2285 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3095...  Training loss: 1.4013...  0.2234 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3096...  Training loss: 1.4135...  0.1892 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3097...  Training loss: 1.4437...  0.1736 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3098...  Training loss: 1.4274...  0.1606 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3099...  Training loss: 1.4000...  0.1746 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3100...  Training loss: 1.4435...  0.1591 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3101...  Training loss: 1.4333...  0.2779 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3102...  Training loss: 1.4066...  0.2807 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3103...  Training loss: 1.3919...  0.2837 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3104...  Training loss: 1.4528...  0.2804 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3105...  Training loss: 1.4427...  0.2531 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3106...  Training loss: 1.4157...  0.2605 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3107...  Training loss: 1.4352...  0.1993 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3108...  Training loss: 1.4180...  0.1690 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3109...  Training loss: 1.4152...  0.1708 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3110...  Training loss: 1.4233...  0.2265 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3111...  Training loss: 1.4655...  0.2789 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3112...  Training loss: 1.4015...  0.2364 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3113...  Training loss: 1.4077...  0.2664 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3114...  Training loss: 1.3930...  0.2646 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3115...  Training loss: 1.4206...  0.2100 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3116...  Training loss: 1.4339...  0.2522 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3117...  Training loss: 1.4214...  0.1748 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3118...  Training loss: 1.3836...  0.1726 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3119...  Training loss: 1.4305...  0.1920 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3120...  Training loss: 1.4241...  0.1816 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3121...  Training loss: 1.4287...  0.1583 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3122...  Training loss: 1.4178...  0.2057 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3123...  Training loss: 1.3900...  0.2110 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3124...  Training loss: 1.4115...  0.1749 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3125...  Training loss: 1.4338...  0.2116 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3126...  Training loss: 1.4321...  0.2679 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3127...  Training loss: 1.4384...  0.2176 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3128...  Training loss: 1.4079...  0.1762 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3129...  Training loss: 1.3947...  0.2421 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3130...  Training loss: 1.4090...  0.1656 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3131...  Training loss: 1.3834...  0.2460 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3132...  Training loss: 1.4032...  0.2185 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3133...  Training loss: 1.4339...  0.1742 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3134...  Training loss: 1.4316...  0.1917 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3135...  Training loss: 1.4314...  0.1868 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3136...  Training loss: 1.3885...  0.1680 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3137...  Training loss: 1.3891...  0.2284 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3138...  Training loss: 1.4219...  0.2412 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3139...  Training loss: 1.4372...  0.2104 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3140...  Training loss: 1.4445...  0.2812 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3141...  Training loss: 1.4285...  0.2455 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3142...  Training loss: 1.4314...  0.2486 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3143...  Training loss: 1.4173...  0.2577 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3144...  Training loss: 1.3965...  0.2262 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3145...  Training loss: 1.4180...  0.1926 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3146...  Training loss: 1.4005...  0.2179 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3147...  Training loss: 1.4177...  0.2608 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3148...  Training loss: 1.4371...  0.1966 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3149...  Training loss: 1.4405...  0.2278 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3150...  Training loss: 1.3929...  0.2680 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3151...  Training loss: 1.4086...  0.1766 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3152...  Training loss: 1.3951...  0.2116 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3153...  Training loss: 1.4173...  0.2253 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3154...  Training loss: 1.4493...  0.2628 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3155...  Training loss: 1.4019...  0.2509 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3156...  Training loss: 1.3863...  0.2855 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3157...  Training loss: 1.3859...  0.2114 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3158...  Training loss: 1.3992...  0.1728 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3159...  Training loss: 1.4074...  0.2434 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3160...  Training loss: 1.4526...  0.2619 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3161...  Training loss: 1.4579...  0.2036 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3162...  Training loss: 1.4246...  0.1977 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3163...  Training loss: 1.4098...  0.1730 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3164...  Training loss: 1.4078...  0.1633 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3165...  Training loss: 1.4046...  0.1942 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3166...  Training loss: 1.4144...  0.2583 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3167...  Training loss: 1.4098...  0.2470 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3168...  Training loss: 1.3949...  0.1978 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/30...  Training Step: 3169...  Training loss: 1.4292...  0.2097 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3170...  Training loss: 1.3956...  0.2161 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3171...  Training loss: 1.3980...  0.1626 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3172...  Training loss: 1.3850...  0.1742 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3173...  Training loss: 1.4006...  0.1556 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3174...  Training loss: 1.4251...  0.1584 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3175...  Training loss: 1.4181...  0.1535 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3176...  Training loss: 1.4101...  0.2195 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3177...  Training loss: 1.3844...  0.2688 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3178...  Training loss: 1.4171...  0.2019 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3179...  Training loss: 1.3891...  0.2076 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3180...  Training loss: 1.4163...  0.2595 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3181...  Training loss: 1.3864...  0.2373 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3182...  Training loss: 1.3994...  0.1881 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3183...  Training loss: 1.4098...  0.1592 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3184...  Training loss: 1.4436...  0.2178 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3185...  Training loss: 1.3834...  0.2701 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3186...  Training loss: 1.3996...  0.2994 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3187...  Training loss: 1.4163...  0.2127 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3188...  Training loss: 1.4188...  0.2273 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3189...  Training loss: 1.4239...  0.1828 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3190...  Training loss: 1.4406...  0.2096 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3191...  Training loss: 1.4466...  0.2595 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3192...  Training loss: 1.3743...  0.1927 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3193...  Training loss: 1.3781...  0.2334 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3194...  Training loss: 1.4326...  0.1777 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3195...  Training loss: 1.4141...  0.2289 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3196...  Training loss: 1.4353...  0.2144 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3197...  Training loss: 1.4006...  0.2640 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3198...  Training loss: 1.3947...  0.2315 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3199...  Training loss: 1.4071...  0.1720 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3200...  Training loss: 1.4114...  0.1791 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3201...  Training loss: 1.4011...  0.2321 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3202...  Training loss: 1.4003...  0.1598 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3203...  Training loss: 1.4011...  0.2203 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3204...  Training loss: 1.3976...  0.2075 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3205...  Training loss: 1.3992...  0.2263 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3206...  Training loss: 1.3693...  0.1858 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3207...  Training loss: 1.4321...  0.1778 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3208...  Training loss: 1.4083...  0.2402 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3209...  Training loss: 1.4090...  0.2746 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3210...  Training loss: 1.3860...  0.2525 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3211...  Training loss: 1.4098...  0.2408 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3212...  Training loss: 1.4284...  0.2048 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3213...  Training loss: 1.4005...  0.2637 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3214...  Training loss: 1.3883...  0.2423 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3215...  Training loss: 1.3964...  0.1562 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3216...  Training loss: 1.3708...  0.1919 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3217...  Training loss: 1.3935...  0.1896 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3218...  Training loss: 1.4104...  0.1938 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3219...  Training loss: 1.3874...  0.2633 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3220...  Training loss: 1.4199...  0.2418 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3221...  Training loss: 1.4007...  0.2575 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3222...  Training loss: 1.4346...  0.2465 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3223...  Training loss: 1.4036...  0.2098 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3224...  Training loss: 1.3966...  0.2195 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3225...  Training loss: 1.3877...  0.1635 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3226...  Training loss: 1.3868...  0.2209 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3227...  Training loss: 1.3917...  0.2872 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3228...  Training loss: 1.4048...  0.2239 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3229...  Training loss: 1.4173...  0.2603 sec/batch\n",
      "Epoch: 19/30...  Training Step: 3230...  Training loss: 1.3916...  0.2118 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3231...  Training loss: 1.4641...  0.2179 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3232...  Training loss: 1.3627...  0.2461 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3233...  Training loss: 1.3766...  0.2950 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3234...  Training loss: 1.4233...  0.2380 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3235...  Training loss: 1.4170...  0.2583 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3236...  Training loss: 1.4331...  0.2621 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3237...  Training loss: 1.4028...  0.1829 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3238...  Training loss: 1.3861...  0.2432 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3239...  Training loss: 1.3773...  0.3323 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3240...  Training loss: 1.3788...  0.1818 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3241...  Training loss: 1.3683...  0.1830 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3242...  Training loss: 1.3923...  0.2421 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3243...  Training loss: 1.3904...  0.1526 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3244...  Training loss: 1.3972...  0.2107 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3245...  Training loss: 1.4469...  0.2158 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3246...  Training loss: 1.4460...  0.2753 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3247...  Training loss: 1.3793...  0.1932 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3248...  Training loss: 1.3992...  0.2050 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3249...  Training loss: 1.3939...  0.1537 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3250...  Training loss: 1.3805...  0.2033 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3251...  Training loss: 1.4082...  0.2281 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3252...  Training loss: 1.3561...  0.1574 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3253...  Training loss: 1.3644...  0.1594 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3254...  Training loss: 1.3618...  0.1703 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3255...  Training loss: 1.4126...  0.2138 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3256...  Training loss: 1.4094...  0.1981 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3257...  Training loss: 1.4204...  0.2482 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3258...  Training loss: 1.4215...  0.2131 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3259...  Training loss: 1.3898...  0.1800 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3260...  Training loss: 1.3690...  0.2684 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3261...  Training loss: 1.3945...  0.1855 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3262...  Training loss: 1.3852...  0.1582 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3263...  Training loss: 1.3882...  0.1631 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3264...  Training loss: 1.3758...  0.1645 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3265...  Training loss: 1.3827...  0.1533 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3266...  Training loss: 1.3970...  0.2878 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/30...  Training Step: 3267...  Training loss: 1.4420...  0.2500 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3268...  Training loss: 1.4141...  0.2354 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3269...  Training loss: 1.4025...  0.2172 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3270...  Training loss: 1.4245...  0.1581 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3271...  Training loss: 1.4137...  0.3089 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3272...  Training loss: 1.3972...  0.1873 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3273...  Training loss: 1.3792...  0.2156 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3274...  Training loss: 1.4426...  0.1974 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3275...  Training loss: 1.4299...  0.2439 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3276...  Training loss: 1.3975...  0.2454 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3277...  Training loss: 1.4243...  0.3340 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3278...  Training loss: 1.4140...  0.2509 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3279...  Training loss: 1.3865...  0.2476 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3280...  Training loss: 1.4054...  0.1971 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3281...  Training loss: 1.4469...  0.2331 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3282...  Training loss: 1.3860...  0.2373 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3283...  Training loss: 1.3942...  0.1765 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3284...  Training loss: 1.3888...  0.1674 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3285...  Training loss: 1.3990...  0.2386 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3286...  Training loss: 1.4118...  0.2065 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3287...  Training loss: 1.4034...  0.2931 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3288...  Training loss: 1.3768...  0.2760 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3289...  Training loss: 1.4115...  0.2624 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3290...  Training loss: 1.4209...  0.1932 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3291...  Training loss: 1.4119...  0.1601 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3292...  Training loss: 1.4011...  0.2072 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3293...  Training loss: 1.3833...  0.2590 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3294...  Training loss: 1.4028...  0.3275 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3295...  Training loss: 1.4147...  0.3675 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3296...  Training loss: 1.4087...  0.3061 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3297...  Training loss: 1.4252...  0.3510 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3298...  Training loss: 1.4089...  0.2866 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3299...  Training loss: 1.3935...  0.2367 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3300...  Training loss: 1.4016...  0.2587 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3301...  Training loss: 1.3764...  0.1915 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3302...  Training loss: 1.3936...  0.2157 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3303...  Training loss: 1.4023...  0.1530 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3304...  Training loss: 1.4246...  0.1822 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3305...  Training loss: 1.4292...  0.2451 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3306...  Training loss: 1.3638...  0.2752 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3307...  Training loss: 1.4000...  0.2405 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3308...  Training loss: 1.4110...  0.1925 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3309...  Training loss: 1.4313...  0.1686 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3310...  Training loss: 1.4223...  0.2544 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3311...  Training loss: 1.4267...  0.1701 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3312...  Training loss: 1.4205...  0.2778 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3313...  Training loss: 1.4134...  0.2776 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3314...  Training loss: 1.3905...  0.2759 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3315...  Training loss: 1.4105...  0.1758 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3316...  Training loss: 1.3784...  0.1943 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3317...  Training loss: 1.4070...  0.3441 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3318...  Training loss: 1.4172...  0.1870 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3319...  Training loss: 1.4226...  0.2469 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3320...  Training loss: 1.3728...  0.1750 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3321...  Training loss: 1.4058...  0.2036 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3322...  Training loss: 1.3868...  0.2456 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3323...  Training loss: 1.4040...  0.2283 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3324...  Training loss: 1.4262...  0.1710 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3325...  Training loss: 1.3907...  0.2927 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3326...  Training loss: 1.3637...  0.2314 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3327...  Training loss: 1.3792...  0.2021 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3328...  Training loss: 1.3896...  0.2364 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3329...  Training loss: 1.3964...  0.2363 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3330...  Training loss: 1.4539...  0.1782 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3331...  Training loss: 1.4352...  0.2450 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3332...  Training loss: 1.4028...  0.2118 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3333...  Training loss: 1.4069...  0.2426 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3334...  Training loss: 1.4027...  0.1515 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3335...  Training loss: 1.4026...  0.1991 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3336...  Training loss: 1.3891...  0.2724 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3337...  Training loss: 1.4010...  0.2227 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3338...  Training loss: 1.3843...  0.2264 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3339...  Training loss: 1.4205...  0.2026 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3340...  Training loss: 1.3829...  0.1704 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3341...  Training loss: 1.3929...  0.1803 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3342...  Training loss: 1.3688...  0.2188 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3343...  Training loss: 1.4009...  0.2464 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3344...  Training loss: 1.4068...  0.2379 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3345...  Training loss: 1.4087...  0.1670 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3346...  Training loss: 1.4034...  0.2600 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3347...  Training loss: 1.3687...  0.1908 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3348...  Training loss: 1.3970...  0.2190 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3349...  Training loss: 1.3798...  0.2048 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3350...  Training loss: 1.4047...  0.1863 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3351...  Training loss: 1.3799...  0.1903 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3352...  Training loss: 1.4054...  0.2023 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3353...  Training loss: 1.3999...  0.2353 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3354...  Training loss: 1.4155...  0.1684 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3355...  Training loss: 1.3703...  0.2012 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3356...  Training loss: 1.3820...  0.1787 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3357...  Training loss: 1.3959...  0.2166 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3358...  Training loss: 1.4047...  0.2248 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3359...  Training loss: 1.4048...  0.2402 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3360...  Training loss: 1.4373...  0.2070 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3361...  Training loss: 1.4369...  0.1706 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3362...  Training loss: 1.3621...  0.2264 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3363...  Training loss: 1.3616...  0.2733 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3364...  Training loss: 1.4165...  0.2048 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/30...  Training Step: 3365...  Training loss: 1.3964...  0.2799 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3366...  Training loss: 1.4235...  0.2581 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3367...  Training loss: 1.3752...  0.2180 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3368...  Training loss: 1.3889...  0.1807 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3369...  Training loss: 1.3856...  0.1684 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3370...  Training loss: 1.3970...  0.1580 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3371...  Training loss: 1.3873...  0.1678 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3372...  Training loss: 1.3948...  0.1742 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3373...  Training loss: 1.3855...  0.2430 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3374...  Training loss: 1.3787...  0.2336 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3375...  Training loss: 1.3872...  0.2066 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3376...  Training loss: 1.3547...  0.2454 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3377...  Training loss: 1.4112...  0.2390 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3378...  Training loss: 1.3860...  0.2598 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3379...  Training loss: 1.3970...  0.2093 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3380...  Training loss: 1.3884...  0.1998 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3381...  Training loss: 1.4062...  0.1586 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3382...  Training loss: 1.4188...  0.2000 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3383...  Training loss: 1.3812...  0.2406 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3384...  Training loss: 1.3768...  0.2256 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3385...  Training loss: 1.3761...  0.1726 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3386...  Training loss: 1.3670...  0.2614 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3387...  Training loss: 1.3822...  0.2699 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3388...  Training loss: 1.3949...  0.1930 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3389...  Training loss: 1.3764...  0.2495 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3390...  Training loss: 1.3985...  0.1982 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3391...  Training loss: 1.3892...  0.1857 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3392...  Training loss: 1.4164...  0.3018 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3393...  Training loss: 1.3933...  0.2627 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3394...  Training loss: 1.3762...  0.2683 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3395...  Training loss: 1.3682...  0.2318 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3396...  Training loss: 1.3796...  0.2233 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3397...  Training loss: 1.3882...  0.2374 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3398...  Training loss: 1.3925...  0.1932 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3399...  Training loss: 1.3977...  0.2640 sec/batch\n",
      "Epoch: 20/30...  Training Step: 3400...  Training loss: 1.3777...  0.2183 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3401...  Training loss: 1.4560...  0.2179 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3402...  Training loss: 1.3635...  0.2750 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3403...  Training loss: 1.3747...  0.1736 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3404...  Training loss: 1.4111...  0.1823 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3405...  Training loss: 1.4093...  0.1797 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3406...  Training loss: 1.4237...  0.1891 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3407...  Training loss: 1.3964...  0.1620 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3408...  Training loss: 1.3833...  0.1741 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3409...  Training loss: 1.3488...  0.2150 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3410...  Training loss: 1.3702...  0.1822 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3411...  Training loss: 1.3507...  0.2371 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3412...  Training loss: 1.3811...  0.1742 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3413...  Training loss: 1.3696...  0.1987 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3414...  Training loss: 1.3771...  0.2456 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3415...  Training loss: 1.4371...  0.2156 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3416...  Training loss: 1.4293...  0.2309 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3417...  Training loss: 1.3752...  0.1986 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3418...  Training loss: 1.3865...  0.1994 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3419...  Training loss: 1.3827...  0.2123 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3420...  Training loss: 1.3833...  0.2168 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3421...  Training loss: 1.4003...  0.1632 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3422...  Training loss: 1.3520...  0.1872 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3423...  Training loss: 1.3585...  0.2163 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3424...  Training loss: 1.3613...  0.2527 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3425...  Training loss: 1.4009...  0.1834 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3426...  Training loss: 1.3930...  0.2241 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3427...  Training loss: 1.4091...  0.2007 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3428...  Training loss: 1.4055...  0.1745 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3429...  Training loss: 1.3827...  0.2144 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3430...  Training loss: 1.3517...  0.2075 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3431...  Training loss: 1.3783...  0.2504 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3432...  Training loss: 1.3816...  0.2645 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3433...  Training loss: 1.3782...  0.2654 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3434...  Training loss: 1.3735...  0.2612 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3435...  Training loss: 1.3710...  0.1993 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3436...  Training loss: 1.3929...  0.2344 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3437...  Training loss: 1.4202...  0.2565 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3438...  Training loss: 1.4028...  0.3230 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3439...  Training loss: 1.3806...  0.1874 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3440...  Training loss: 1.4130...  0.2404 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3441...  Training loss: 1.4053...  0.2531 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3442...  Training loss: 1.3947...  0.2572 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3443...  Training loss: 1.3738...  0.2417 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3444...  Training loss: 1.4432...  0.1683 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3445...  Training loss: 1.4156...  0.1600 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3446...  Training loss: 1.3934...  0.2239 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3447...  Training loss: 1.4205...  0.2668 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3448...  Training loss: 1.3971...  0.2484 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3449...  Training loss: 1.3883...  0.2262 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3450...  Training loss: 1.4000...  0.1572 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3451...  Training loss: 1.4405...  0.1683 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3452...  Training loss: 1.3776...  0.2362 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3453...  Training loss: 1.3775...  0.2472 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3454...  Training loss: 1.3755...  0.2105 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3455...  Training loss: 1.4017...  0.2411 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3456...  Training loss: 1.4032...  0.2588 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3457...  Training loss: 1.3993...  0.2696 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3458...  Training loss: 1.3725...  0.2302 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3459...  Training loss: 1.4066...  0.2293 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3460...  Training loss: 1.4062...  0.1620 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3461...  Training loss: 1.3932...  0.2011 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3462...  Training loss: 1.3908...  0.2105 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/30...  Training Step: 3463...  Training loss: 1.3622...  0.2384 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3464...  Training loss: 1.3913...  0.2011 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3465...  Training loss: 1.4022...  0.1697 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3466...  Training loss: 1.4048...  0.1616 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3467...  Training loss: 1.4077...  0.2626 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3468...  Training loss: 1.3891...  0.1639 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3469...  Training loss: 1.3759...  0.1949 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3470...  Training loss: 1.3820...  0.2104 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3471...  Training loss: 1.3499...  0.1633 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3472...  Training loss: 1.3851...  0.3216 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3473...  Training loss: 1.4009...  0.2378 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3474...  Training loss: 1.4162...  0.1812 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3475...  Training loss: 1.4147...  0.1755 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3476...  Training loss: 1.3627...  0.2384 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3477...  Training loss: 1.3764...  0.2114 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3478...  Training loss: 1.3995...  0.2651 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3479...  Training loss: 1.4153...  0.2234 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3480...  Training loss: 1.4175...  0.1805 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3481...  Training loss: 1.4133...  0.2119 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3482...  Training loss: 1.4077...  0.2623 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3483...  Training loss: 1.3907...  0.2642 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3484...  Training loss: 1.3737...  0.1685 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3485...  Training loss: 1.3928...  0.1760 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3486...  Training loss: 1.3693...  0.1861 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3487...  Training loss: 1.3949...  0.2557 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3488...  Training loss: 1.4142...  0.1821 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3489...  Training loss: 1.4109...  0.1825 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3490...  Training loss: 1.3720...  0.1559 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3491...  Training loss: 1.3727...  0.2122 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3492...  Training loss: 1.3799...  0.3576 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3493...  Training loss: 1.3851...  0.2398 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3494...  Training loss: 1.4145...  0.1804 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3495...  Training loss: 1.3829...  0.2334 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3496...  Training loss: 1.3563...  0.2543 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3497...  Training loss: 1.3575...  0.2196 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3498...  Training loss: 1.3734...  0.2120 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3499...  Training loss: 1.3832...  0.1796 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3500...  Training loss: 1.4306...  0.1681 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3501...  Training loss: 1.4258...  0.1627 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3502...  Training loss: 1.3857...  0.2875 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3503...  Training loss: 1.3830...  0.2352 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3504...  Training loss: 1.3924...  0.1976 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3505...  Training loss: 1.3811...  0.1969 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3506...  Training loss: 1.3936...  0.1965 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3507...  Training loss: 1.3940...  0.1673 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3508...  Training loss: 1.3777...  0.2425 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3509...  Training loss: 1.3991...  0.1536 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3510...  Training loss: 1.3892...  0.1772 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3511...  Training loss: 1.3743...  0.1692 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3512...  Training loss: 1.3644...  0.1584 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3513...  Training loss: 1.3804...  0.1905 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3514...  Training loss: 1.3948...  0.1771 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3515...  Training loss: 1.4075...  0.1572 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3516...  Training loss: 1.3966...  0.1744 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3517...  Training loss: 1.3682...  0.2590 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3518...  Training loss: 1.3948...  0.2403 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3519...  Training loss: 1.3688...  0.2307 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3520...  Training loss: 1.3900...  0.2535 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3521...  Training loss: 1.3712...  0.2826 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3522...  Training loss: 1.3939...  0.2346 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3523...  Training loss: 1.3993...  0.1663 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3524...  Training loss: 1.4145...  0.1583 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3525...  Training loss: 1.3598...  0.1668 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3526...  Training loss: 1.3606...  0.2326 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3527...  Training loss: 1.3958...  0.1995 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3528...  Training loss: 1.3972...  0.1956 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3529...  Training loss: 1.4055...  0.2986 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3530...  Training loss: 1.4240...  0.2349 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3531...  Training loss: 1.4274...  0.1794 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3532...  Training loss: 1.3477...  0.2118 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3533...  Training loss: 1.3546...  0.1677 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3534...  Training loss: 1.4069...  0.1814 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3535...  Training loss: 1.3771...  0.1795 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3536...  Training loss: 1.4113...  0.1678 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3537...  Training loss: 1.3607...  0.1875 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3538...  Training loss: 1.3743...  0.2185 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3539...  Training loss: 1.3803...  0.1608 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3540...  Training loss: 1.3954...  0.1917 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3541...  Training loss: 1.3819...  0.1861 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3542...  Training loss: 1.3849...  0.2178 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3543...  Training loss: 1.3755...  0.2026 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3544...  Training loss: 1.3688...  0.2171 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3545...  Training loss: 1.3644...  0.1869 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3546...  Training loss: 1.3439...  0.2142 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3547...  Training loss: 1.4038...  0.2714 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3548...  Training loss: 1.3850...  0.1886 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3549...  Training loss: 1.3754...  0.2153 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3550...  Training loss: 1.3613...  0.2199 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3551...  Training loss: 1.3856...  0.1735 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3552...  Training loss: 1.3986...  0.2227 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3553...  Training loss: 1.3782...  0.2830 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3554...  Training loss: 1.3686...  0.2018 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3555...  Training loss: 1.3623...  0.2384 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3556...  Training loss: 1.3402...  0.2979 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3557...  Training loss: 1.3770...  0.2176 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3558...  Training loss: 1.3780...  0.2299 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3559...  Training loss: 1.3602...  0.2118 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3560...  Training loss: 1.3890...  0.2555 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3561...  Training loss: 1.3671...  0.1935 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/30...  Training Step: 3562...  Training loss: 1.4038...  0.2426 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3563...  Training loss: 1.3823...  0.2098 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3564...  Training loss: 1.3728...  0.1540 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3565...  Training loss: 1.3680...  0.1725 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3566...  Training loss: 1.3705...  0.2694 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3567...  Training loss: 1.3837...  0.2616 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3568...  Training loss: 1.3683...  0.2247 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3569...  Training loss: 1.3795...  0.2276 sec/batch\n",
      "Epoch: 21/30...  Training Step: 3570...  Training loss: 1.3660...  0.2359 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3571...  Training loss: 1.4368...  0.1546 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3572...  Training loss: 1.3423...  0.2065 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3573...  Training loss: 1.3630...  0.2193 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3574...  Training loss: 1.3989...  0.2549 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3575...  Training loss: 1.4011...  0.1814 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3576...  Training loss: 1.4038...  0.1599 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3577...  Training loss: 1.3855...  0.1949 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3578...  Training loss: 1.3614...  0.2204 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3579...  Training loss: 1.3375...  0.1614 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3580...  Training loss: 1.3551...  0.1979 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3581...  Training loss: 1.3461...  0.2291 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3582...  Training loss: 1.3820...  0.1852 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3583...  Training loss: 1.3622...  0.1989 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3584...  Training loss: 1.3594...  0.2507 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3585...  Training loss: 1.4164...  0.2030 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3586...  Training loss: 1.4231...  0.2882 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3587...  Training loss: 1.3609...  0.2269 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3588...  Training loss: 1.3734...  0.2536 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3589...  Training loss: 1.3771...  0.2614 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3590...  Training loss: 1.3675...  0.2292 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3591...  Training loss: 1.3840...  0.1981 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3592...  Training loss: 1.3421...  0.2126 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3593...  Training loss: 1.3472...  0.2353 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3594...  Training loss: 1.3413...  0.2169 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3595...  Training loss: 1.3901...  0.1531 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3596...  Training loss: 1.3760...  0.1929 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3597...  Training loss: 1.3960...  0.1767 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3598...  Training loss: 1.4011...  0.2244 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3599...  Training loss: 1.3633...  0.1613 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3600...  Training loss: 1.3379...  0.2166 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3601...  Training loss: 1.3648...  0.1516 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3602...  Training loss: 1.3657...  0.2421 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3603...  Training loss: 1.3595...  0.2773 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3604...  Training loss: 1.3608...  0.2351 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3605...  Training loss: 1.3525...  0.2183 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3606...  Training loss: 1.3698...  0.1570 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3607...  Training loss: 1.4033...  0.1538 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3608...  Training loss: 1.3938...  0.2264 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3609...  Training loss: 1.3662...  0.2492 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3610...  Training loss: 1.3937...  0.1949 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3611...  Training loss: 1.3959...  0.2273 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3612...  Training loss: 1.3739...  0.1876 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3613...  Training loss: 1.3555...  0.2450 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3614...  Training loss: 1.4199...  0.2317 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3615...  Training loss: 1.3986...  0.2190 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3616...  Training loss: 1.3736...  0.2035 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3617...  Training loss: 1.4061...  0.2738 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3618...  Training loss: 1.3900...  0.2212 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3619...  Training loss: 1.3818...  0.2538 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3620...  Training loss: 1.3811...  0.2482 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3621...  Training loss: 1.4250...  0.2806 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3622...  Training loss: 1.3646...  0.2659 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3623...  Training loss: 1.3556...  0.2289 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3624...  Training loss: 1.3667...  0.2078 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3625...  Training loss: 1.3855...  0.1828 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3626...  Training loss: 1.3987...  0.1714 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3627...  Training loss: 1.3806...  0.1944 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3628...  Training loss: 1.3543...  0.2556 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3629...  Training loss: 1.3986...  0.2407 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3630...  Training loss: 1.3895...  0.2811 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3631...  Training loss: 1.3874...  0.2422 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3632...  Training loss: 1.3761...  0.2158 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3633...  Training loss: 1.3530...  0.1896 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3634...  Training loss: 1.3707...  0.2761 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3635...  Training loss: 1.3838...  0.1529 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3636...  Training loss: 1.3916...  0.1670 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3637...  Training loss: 1.3949...  0.2715 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3638...  Training loss: 1.3744...  0.1605 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3639...  Training loss: 1.3737...  0.1588 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3640...  Training loss: 1.3752...  0.1567 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3641...  Training loss: 1.3493...  0.1987 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3642...  Training loss: 1.3709...  0.1566 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3643...  Training loss: 1.3861...  0.2022 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3644...  Training loss: 1.3990...  0.1559 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3645...  Training loss: 1.4122...  0.2225 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3646...  Training loss: 1.3538...  0.1574 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3647...  Training loss: 1.3633...  0.1528 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3648...  Training loss: 1.3820...  0.1880 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3649...  Training loss: 1.3937...  0.2553 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3650...  Training loss: 1.3992...  0.2186 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3651...  Training loss: 1.4007...  0.1539 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3652...  Training loss: 1.3975...  0.2392 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3653...  Training loss: 1.3897...  0.2282 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3654...  Training loss: 1.3624...  0.2559 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3655...  Training loss: 1.3854...  0.2521 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3656...  Training loss: 1.3618...  0.2354 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3657...  Training loss: 1.3846...  0.1892 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3658...  Training loss: 1.3914...  0.1630 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3659...  Training loss: 1.4023...  0.2368 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3660...  Training loss: 1.3502...  0.1649 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/30...  Training Step: 3661...  Training loss: 1.3662...  0.2453 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3662...  Training loss: 1.3623...  0.2296 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3663...  Training loss: 1.3831...  0.1767 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3664...  Training loss: 1.4113...  0.1830 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3665...  Training loss: 1.3680...  0.2055 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3666...  Training loss: 1.3438...  0.2227 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3667...  Training loss: 1.3540...  0.1536 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3668...  Training loss: 1.3666...  0.1667 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3669...  Training loss: 1.3722...  0.2308 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3670...  Training loss: 1.4140...  0.2090 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3671...  Training loss: 1.4187...  0.2290 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3672...  Training loss: 1.3778...  0.2155 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3673...  Training loss: 1.3836...  0.2470 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3674...  Training loss: 1.3847...  0.2166 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3675...  Training loss: 1.3779...  0.2351 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3676...  Training loss: 1.3695...  0.1630 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3677...  Training loss: 1.3706...  0.1856 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3678...  Training loss: 1.3601...  0.1607 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3679...  Training loss: 1.3853...  0.2188 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3680...  Training loss: 1.3597...  0.2346 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3681...  Training loss: 1.3581...  0.1818 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3682...  Training loss: 1.3399...  0.1795 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3683...  Training loss: 1.3678...  0.1596 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3684...  Training loss: 1.3704...  0.1951 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3685...  Training loss: 1.3992...  0.1577 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3686...  Training loss: 1.3831...  0.1528 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3687...  Training loss: 1.3452...  0.2130 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3688...  Training loss: 1.3852...  0.1536 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3689...  Training loss: 1.3560...  0.1692 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3690...  Training loss: 1.3704...  0.1707 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3691...  Training loss: 1.3543...  0.2479 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3692...  Training loss: 1.3763...  0.1961 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3693...  Training loss: 1.3763...  0.2172 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3694...  Training loss: 1.3978...  0.1665 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3695...  Training loss: 1.3471...  0.2124 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3696...  Training loss: 1.3600...  0.2651 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3697...  Training loss: 1.3866...  0.2157 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3698...  Training loss: 1.3865...  0.2057 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3699...  Training loss: 1.3900...  0.1921 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3700...  Training loss: 1.4095...  0.1564 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3701...  Training loss: 1.4068...  0.1763 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3702...  Training loss: 1.3350...  0.2593 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3703...  Training loss: 1.3429...  0.2270 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3704...  Training loss: 1.3829...  0.1859 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3705...  Training loss: 1.3704...  0.1545 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3706...  Training loss: 1.3960...  0.1692 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3707...  Training loss: 1.3515...  0.1828 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3708...  Training loss: 1.3548...  0.2448 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3709...  Training loss: 1.3620...  0.2012 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3710...  Training loss: 1.3819...  0.1903 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3711...  Training loss: 1.3680...  0.1647 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3712...  Training loss: 1.3691...  0.2279 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3713...  Training loss: 1.3695...  0.2605 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3714...  Training loss: 1.3590...  0.2478 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3715...  Training loss: 1.3466...  0.2439 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3716...  Training loss: 1.3312...  0.2315 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3717...  Training loss: 1.3783...  0.2311 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3718...  Training loss: 1.3664...  0.2597 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3719...  Training loss: 1.3674...  0.1719 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3720...  Training loss: 1.3541...  0.2530 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3721...  Training loss: 1.3775...  0.1964 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3722...  Training loss: 1.3988...  0.2405 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3723...  Training loss: 1.3640...  0.2342 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3724...  Training loss: 1.3529...  0.2137 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3725...  Training loss: 1.3620...  0.1541 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3726...  Training loss: 1.3400...  0.2197 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3727...  Training loss: 1.3557...  0.2254 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3728...  Training loss: 1.3632...  0.2153 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3729...  Training loss: 1.3513...  0.1585 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3730...  Training loss: 1.3746...  0.2169 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3731...  Training loss: 1.3635...  0.1811 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3732...  Training loss: 1.3896...  0.1549 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3733...  Training loss: 1.3766...  0.2329 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3734...  Training loss: 1.3562...  0.2078 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3735...  Training loss: 1.3504...  0.1592 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3736...  Training loss: 1.3478...  0.1541 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3737...  Training loss: 1.3736...  0.1775 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3738...  Training loss: 1.3678...  0.2422 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3739...  Training loss: 1.3800...  0.2301 sec/batch\n",
      "Epoch: 22/30...  Training Step: 3740...  Training loss: 1.3574...  0.1862 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3741...  Training loss: 1.4368...  0.1563 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3742...  Training loss: 1.3331...  0.1799 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3743...  Training loss: 1.3585...  0.1608 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3744...  Training loss: 1.3949...  0.1998 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3745...  Training loss: 1.3750...  0.1585 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3746...  Training loss: 1.3925...  0.2038 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3747...  Training loss: 1.3674...  0.1884 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3748...  Training loss: 1.3565...  0.2426 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3749...  Training loss: 1.3337...  0.2415 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3750...  Training loss: 1.3468...  0.1912 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3751...  Training loss: 1.3320...  0.1814 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3752...  Training loss: 1.3610...  0.2087 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3753...  Training loss: 1.3575...  0.2222 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3754...  Training loss: 1.3556...  0.1652 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3755...  Training loss: 1.4117...  0.1951 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3756...  Training loss: 1.4091...  0.1964 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3757...  Training loss: 1.3571...  0.1563 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3758...  Training loss: 1.3650...  0.2492 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3759...  Training loss: 1.3686...  0.1624 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/30...  Training Step: 3760...  Training loss: 1.3632...  0.1810 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3761...  Training loss: 1.3796...  0.2274 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3762...  Training loss: 1.3207...  0.2284 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3763...  Training loss: 1.3441...  0.2365 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3764...  Training loss: 1.3338...  0.2398 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3765...  Training loss: 1.3745...  0.1968 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3766...  Training loss: 1.3691...  0.2537 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3767...  Training loss: 1.3943...  0.2346 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3768...  Training loss: 1.3781...  0.2208 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3769...  Training loss: 1.3643...  0.2259 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3770...  Training loss: 1.3334...  0.2068 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3771...  Training loss: 1.3538...  0.1774 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3772...  Training loss: 1.3573...  0.1908 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3773...  Training loss: 1.3589...  0.2476 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3774...  Training loss: 1.3418...  0.1752 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3775...  Training loss: 1.3438...  0.1643 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3776...  Training loss: 1.3622...  0.1986 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3777...  Training loss: 1.4047...  0.1864 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3778...  Training loss: 1.3842...  0.2288 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3779...  Training loss: 1.3533...  0.1973 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3780...  Training loss: 1.3935...  0.2505 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3781...  Training loss: 1.3797...  0.2075 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3782...  Training loss: 1.3630...  0.2807 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3783...  Training loss: 1.3463...  0.2461 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3784...  Training loss: 1.4092...  0.1565 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3785...  Training loss: 1.3979...  0.1622 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3786...  Training loss: 1.3640...  0.2722 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3787...  Training loss: 1.3936...  0.1846 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3788...  Training loss: 1.3861...  0.2433 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3789...  Training loss: 1.3661...  0.1600 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3790...  Training loss: 1.3666...  0.1996 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3791...  Training loss: 1.4129...  0.2470 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3792...  Training loss: 1.3517...  0.2375 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3793...  Training loss: 1.3579...  0.2238 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3794...  Training loss: 1.3522...  0.2148 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3795...  Training loss: 1.3838...  0.2513 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3796...  Training loss: 1.3709...  0.2303 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3797...  Training loss: 1.3718...  0.1572 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3798...  Training loss: 1.3402...  0.2283 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3799...  Training loss: 1.3823...  0.2274 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3800...  Training loss: 1.3871...  0.2595 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3801...  Training loss: 1.3724...  0.2791 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3802...  Training loss: 1.3610...  0.2423 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3803...  Training loss: 1.3428...  0.1824 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3804...  Training loss: 1.3604...  0.2273 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3805...  Training loss: 1.3750...  0.2576 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3806...  Training loss: 1.3758...  0.2478 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3807...  Training loss: 1.3810...  0.1861 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3808...  Training loss: 1.3659...  0.2524 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3809...  Training loss: 1.3434...  0.1903 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3810...  Training loss: 1.3699...  0.1869 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3811...  Training loss: 1.3400...  0.1588 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3812...  Training loss: 1.3560...  0.1571 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3813...  Training loss: 1.3724...  0.1580 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3814...  Training loss: 1.3887...  0.2906 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3815...  Training loss: 1.3896...  0.2852 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3816...  Training loss: 1.3487...  0.2816 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3817...  Training loss: 1.3589...  0.1716 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3818...  Training loss: 1.3785...  0.1981 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3819...  Training loss: 1.3794...  0.1795 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3820...  Training loss: 1.3935...  0.2443 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3821...  Training loss: 1.3843...  0.2112 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3822...  Training loss: 1.3898...  0.2248 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3823...  Training loss: 1.3692...  0.2319 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3824...  Training loss: 1.3469...  0.2487 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3825...  Training loss: 1.3813...  0.2858 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3826...  Training loss: 1.3578...  0.2553 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3827...  Training loss: 1.3659...  0.1538 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3828...  Training loss: 1.3885...  0.1878 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3829...  Training loss: 1.3994...  0.1552 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3830...  Training loss: 1.3448...  0.1853 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3831...  Training loss: 1.3587...  0.2663 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3832...  Training loss: 1.3511...  0.2805 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3833...  Training loss: 1.3644...  0.2571 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3834...  Training loss: 1.4061...  0.2571 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3835...  Training loss: 1.3607...  0.2354 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3836...  Training loss: 1.3332...  0.2682 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3837...  Training loss: 1.3349...  0.2699 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3838...  Training loss: 1.3553...  0.1952 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3839...  Training loss: 1.3660...  0.2276 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3840...  Training loss: 1.4108...  0.1640 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3841...  Training loss: 1.4061...  0.1620 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3842...  Training loss: 1.3706...  0.1677 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3843...  Training loss: 1.3697...  0.1660 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3844...  Training loss: 1.3759...  0.2077 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3845...  Training loss: 1.3672...  0.2737 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3846...  Training loss: 1.3680...  0.2703 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3847...  Training loss: 1.3754...  0.2065 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3848...  Training loss: 1.3457...  0.1649 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3849...  Training loss: 1.3760...  0.1686 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3850...  Training loss: 1.3520...  0.2285 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3851...  Training loss: 1.3505...  0.2260 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3852...  Training loss: 1.3402...  0.1592 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3853...  Training loss: 1.3697...  0.2846 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3854...  Training loss: 1.3678...  0.2501 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3855...  Training loss: 1.3821...  0.2799 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3856...  Training loss: 1.3699...  0.3164 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3857...  Training loss: 1.3427...  0.1767 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/30...  Training Step: 3858...  Training loss: 1.3760...  0.1644 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3859...  Training loss: 1.3538...  0.1800 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3860...  Training loss: 1.3755...  0.1873 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3861...  Training loss: 1.3444...  0.3284 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3862...  Training loss: 1.3682...  0.1729 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3863...  Training loss: 1.3679...  0.1635 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3864...  Training loss: 1.3923...  0.2176 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3865...  Training loss: 1.3419...  0.2512 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3866...  Training loss: 1.3473...  0.2582 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3867...  Training loss: 1.3677...  0.2471 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3868...  Training loss: 1.3753...  0.1626 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3869...  Training loss: 1.3761...  0.1615 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3870...  Training loss: 1.3985...  0.2028 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3871...  Training loss: 1.3979...  0.1800 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3872...  Training loss: 1.3242...  0.1733 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3873...  Training loss: 1.3330...  0.2271 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3874...  Training loss: 1.3791...  0.2179 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3875...  Training loss: 1.3699...  0.2509 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3876...  Training loss: 1.4001...  0.1713 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3877...  Training loss: 1.3495...  0.1928 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3878...  Training loss: 1.3465...  0.1722 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3879...  Training loss: 1.3590...  0.1712 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3880...  Training loss: 1.3671...  0.1993 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3881...  Training loss: 1.3546...  0.2510 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3882...  Training loss: 1.3514...  0.2555 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3883...  Training loss: 1.3589...  0.1695 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3884...  Training loss: 1.3491...  0.1561 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3885...  Training loss: 1.3527...  0.1774 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3886...  Training loss: 1.3262...  0.2162 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3887...  Training loss: 1.3854...  0.1580 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3888...  Training loss: 1.3556...  0.1803 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3889...  Training loss: 1.3579...  0.2487 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3890...  Training loss: 1.3326...  0.2464 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3891...  Training loss: 1.3700...  0.2393 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3892...  Training loss: 1.3708...  0.2288 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3893...  Training loss: 1.3485...  0.2548 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3894...  Training loss: 1.3483...  0.2117 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3895...  Training loss: 1.3435...  0.2096 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3896...  Training loss: 1.3343...  0.1925 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3897...  Training loss: 1.3500...  0.1632 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3898...  Training loss: 1.3701...  0.1953 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3899...  Training loss: 1.3329...  0.2050 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3900...  Training loss: 1.3686...  0.2009 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3901...  Training loss: 1.3525...  0.2033 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3902...  Training loss: 1.3847...  0.2449 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3903...  Training loss: 1.3577...  0.2702 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3904...  Training loss: 1.3459...  0.1637 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3905...  Training loss: 1.3403...  0.2086 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3906...  Training loss: 1.3436...  0.2461 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3907...  Training loss: 1.3564...  0.2248 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3908...  Training loss: 1.3498...  0.2128 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3909...  Training loss: 1.3616...  0.1924 sec/batch\n",
      "Epoch: 23/30...  Training Step: 3910...  Training loss: 1.3432...  0.2025 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3911...  Training loss: 1.4223...  0.2719 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3912...  Training loss: 1.3285...  0.2121 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3913...  Training loss: 1.3318...  0.1605 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3914...  Training loss: 1.3910...  0.2477 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3915...  Training loss: 1.3638...  0.2245 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3916...  Training loss: 1.3822...  0.2240 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3917...  Training loss: 1.3568...  0.2027 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3918...  Training loss: 1.3547...  0.1587 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3919...  Training loss: 1.3286...  0.1758 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3920...  Training loss: 1.3306...  0.1864 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3921...  Training loss: 1.3233...  0.1941 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3922...  Training loss: 1.3479...  0.1570 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3923...  Training loss: 1.3373...  0.1925 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3924...  Training loss: 1.3527...  0.2591 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3925...  Training loss: 1.3939...  0.2102 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3926...  Training loss: 1.3965...  0.2286 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3927...  Training loss: 1.3432...  0.1974 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3928...  Training loss: 1.3563...  0.2336 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3929...  Training loss: 1.3504...  0.2442 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3930...  Training loss: 1.3463...  0.2354 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3931...  Training loss: 1.3674...  0.1764 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3932...  Training loss: 1.3093...  0.1737 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3933...  Training loss: 1.3333...  0.1549 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3934...  Training loss: 1.3247...  0.2292 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3935...  Training loss: 1.3633...  0.2167 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3936...  Training loss: 1.3583...  0.1618 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3937...  Training loss: 1.3841...  0.1706 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3938...  Training loss: 1.3660...  0.2498 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3939...  Training loss: 1.3490...  0.1568 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3940...  Training loss: 1.3263...  0.2422 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3941...  Training loss: 1.3469...  0.1959 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3942...  Training loss: 1.3400...  0.1969 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3943...  Training loss: 1.3460...  0.1541 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3944...  Training loss: 1.3321...  0.2449 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3945...  Training loss: 1.3413...  0.2287 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3946...  Training loss: 1.3445...  0.2186 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3947...  Training loss: 1.3767...  0.1948 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3948...  Training loss: 1.3728...  0.2907 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3949...  Training loss: 1.3365...  0.2567 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3950...  Training loss: 1.3774...  0.2175 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3951...  Training loss: 1.3776...  0.2321 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3952...  Training loss: 1.3609...  0.1551 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3953...  Training loss: 1.3268...  0.2356 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3954...  Training loss: 1.3957...  0.2530 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3955...  Training loss: 1.3813...  0.2136 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/30...  Training Step: 3956...  Training loss: 1.3414...  0.2544 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3957...  Training loss: 1.3845...  0.1752 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3958...  Training loss: 1.3713...  0.1592 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3959...  Training loss: 1.3575...  0.2201 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3960...  Training loss: 1.3573...  0.2243 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3961...  Training loss: 1.4029...  0.2469 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3962...  Training loss: 1.3412...  0.1547 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3963...  Training loss: 1.3508...  0.1547 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3964...  Training loss: 1.3402...  0.2347 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3965...  Training loss: 1.3526...  0.1890 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3966...  Training loss: 1.3669...  0.1634 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3967...  Training loss: 1.3580...  0.2551 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3968...  Training loss: 1.3391...  0.2155 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3969...  Training loss: 1.3760...  0.2844 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3970...  Training loss: 1.3768...  0.2186 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3971...  Training loss: 1.3639...  0.2397 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3972...  Training loss: 1.3603...  0.1870 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3973...  Training loss: 1.3275...  0.2071 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3974...  Training loss: 1.3533...  0.2135 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3975...  Training loss: 1.3741...  0.1760 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3976...  Training loss: 1.3657...  0.1765 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3977...  Training loss: 1.3796...  0.2365 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3978...  Training loss: 1.3564...  0.1821 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3979...  Training loss: 1.3315...  0.1637 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3980...  Training loss: 1.3536...  0.1897 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3981...  Training loss: 1.3298...  0.2208 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3982...  Training loss: 1.3516...  0.1533 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3983...  Training loss: 1.3631...  0.1757 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3984...  Training loss: 1.3784...  0.1529 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3985...  Training loss: 1.3815...  0.2225 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3986...  Training loss: 1.3328...  0.2499 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3987...  Training loss: 1.3464...  0.2634 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3988...  Training loss: 1.3619...  0.1520 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3989...  Training loss: 1.3769...  0.1633 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3990...  Training loss: 1.3829...  0.2172 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3991...  Training loss: 1.3819...  0.2313 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3992...  Training loss: 1.3714...  0.1525 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3993...  Training loss: 1.3718...  0.1955 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3994...  Training loss: 1.3471...  0.1824 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3995...  Training loss: 1.3623...  0.1580 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3996...  Training loss: 1.3572...  0.2092 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3997...  Training loss: 1.3638...  0.2641 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3998...  Training loss: 1.3740...  0.1711 sec/batch\n",
      "Epoch: 24/30...  Training Step: 3999...  Training loss: 1.3836...  0.2535 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4000...  Training loss: 1.3360...  0.2565 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4001...  Training loss: 1.3435...  0.1582 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4002...  Training loss: 1.3400...  0.2129 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4003...  Training loss: 1.3565...  0.1869 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4004...  Training loss: 1.3904...  0.1828 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4005...  Training loss: 1.3451...  0.2526 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4006...  Training loss: 1.3276...  0.2406 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4007...  Training loss: 1.3243...  0.1627 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4008...  Training loss: 1.3432...  0.2132 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4009...  Training loss: 1.3378...  0.1882 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4010...  Training loss: 1.4009...  0.1962 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4011...  Training loss: 1.3989...  0.2250 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4012...  Training loss: 1.3761...  0.2105 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4013...  Training loss: 1.3545...  0.2060 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4014...  Training loss: 1.3543...  0.2524 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4015...  Training loss: 1.3695...  0.2374 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4016...  Training loss: 1.3547...  0.2077 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4017...  Training loss: 1.3577...  0.2683 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4018...  Training loss: 1.3386...  0.2029 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4019...  Training loss: 1.3771...  0.1589 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4020...  Training loss: 1.3424...  0.2050 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4021...  Training loss: 1.3446...  0.2088 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4022...  Training loss: 1.3290...  0.1674 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4023...  Training loss: 1.3512...  0.1657 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4024...  Training loss: 1.3601...  0.2026 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4025...  Training loss: 1.3687...  0.2909 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4026...  Training loss: 1.3571...  0.2669 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4027...  Training loss: 1.3301...  0.2650 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4028...  Training loss: 1.3645...  0.1894 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4029...  Training loss: 1.3323...  0.1574 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4030...  Training loss: 1.3624...  0.1998 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4031...  Training loss: 1.3464...  0.2666 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4032...  Training loss: 1.3623...  0.2447 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4033...  Training loss: 1.3707...  0.2111 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4034...  Training loss: 1.3873...  0.2469 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4035...  Training loss: 1.3382...  0.2204 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4036...  Training loss: 1.3296...  0.2103 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4037...  Training loss: 1.3571...  0.1855 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4038...  Training loss: 1.3634...  0.2667 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4039...  Training loss: 1.3691...  0.1989 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4040...  Training loss: 1.3846...  0.1716 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4041...  Training loss: 1.3913...  0.2187 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4042...  Training loss: 1.3190...  0.2059 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4043...  Training loss: 1.3221...  0.2595 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4044...  Training loss: 1.3723...  0.1534 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4045...  Training loss: 1.3483...  0.2357 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4046...  Training loss: 1.3942...  0.2147 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4047...  Training loss: 1.3256...  0.1704 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4048...  Training loss: 1.3389...  0.1654 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4049...  Training loss: 1.3475...  0.2104 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4050...  Training loss: 1.3562...  0.1740 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4051...  Training loss: 1.3371...  0.2113 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4052...  Training loss: 1.3550...  0.1971 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4053...  Training loss: 1.3435...  0.2353 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4054...  Training loss: 1.3345...  0.1723 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/30...  Training Step: 4055...  Training loss: 1.3400...  0.2375 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4056...  Training loss: 1.3056...  0.2591 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4057...  Training loss: 1.3684...  0.1947 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4058...  Training loss: 1.3388...  0.1629 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4059...  Training loss: 1.3432...  0.2197 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4060...  Training loss: 1.3238...  0.1587 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4061...  Training loss: 1.3619...  0.2103 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4062...  Training loss: 1.3711...  0.2393 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4063...  Training loss: 1.3428...  0.1593 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4064...  Training loss: 1.3341...  0.2556 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4065...  Training loss: 1.3350...  0.2818 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4066...  Training loss: 1.3240...  0.2135 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4067...  Training loss: 1.3423...  0.2569 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4068...  Training loss: 1.3540...  0.2599 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4069...  Training loss: 1.3321...  0.1788 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4070...  Training loss: 1.3539...  0.1868 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4071...  Training loss: 1.3443...  0.2538 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4072...  Training loss: 1.3633...  0.2591 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4073...  Training loss: 1.3511...  0.1625 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4074...  Training loss: 1.3417...  0.1676 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4075...  Training loss: 1.3316...  0.2598 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4076...  Training loss: 1.3405...  0.2702 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4077...  Training loss: 1.3512...  0.2367 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4078...  Training loss: 1.3383...  0.2592 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4079...  Training loss: 1.3548...  0.2014 sec/batch\n",
      "Epoch: 24/30...  Training Step: 4080...  Training loss: 1.3499...  0.1930 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4081...  Training loss: 1.4193...  0.2306 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4082...  Training loss: 1.3080...  0.1897 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4083...  Training loss: 1.3275...  0.1919 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4084...  Training loss: 1.3716...  0.2038 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4085...  Training loss: 1.3624...  0.2443 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4086...  Training loss: 1.3756...  0.1786 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4087...  Training loss: 1.3463...  0.2065 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4088...  Training loss: 1.3439...  0.1972 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4089...  Training loss: 1.3166...  0.2030 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4090...  Training loss: 1.3322...  0.1849 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4091...  Training loss: 1.3189...  0.1657 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4092...  Training loss: 1.3486...  0.2646 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4093...  Training loss: 1.3355...  0.2230 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4094...  Training loss: 1.3318...  0.2128 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4095...  Training loss: 1.3833...  0.2216 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4096...  Training loss: 1.3882...  0.2438 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4097...  Training loss: 1.3256...  0.2501 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4098...  Training loss: 1.3536...  0.2568 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4099...  Training loss: 1.3563...  0.2085 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4100...  Training loss: 1.3396...  0.1880 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4101...  Training loss: 1.3535...  0.2334 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4102...  Training loss: 1.3057...  0.2458 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4103...  Training loss: 1.3195...  0.1538 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4104...  Training loss: 1.3070...  0.1566 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4105...  Training loss: 1.3586...  0.2459 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4106...  Training loss: 1.3552...  0.2429 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4107...  Training loss: 1.3642...  0.2697 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4108...  Training loss: 1.3502...  0.2639 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4109...  Training loss: 1.3577...  0.1918 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4110...  Training loss: 1.3143...  0.1583 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4111...  Training loss: 1.3446...  0.1913 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4112...  Training loss: 1.3248...  0.1786 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4113...  Training loss: 1.3315...  0.1669 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4114...  Training loss: 1.3341...  0.1618 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4115...  Training loss: 1.3277...  0.1692 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4116...  Training loss: 1.3431...  0.1768 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4117...  Training loss: 1.3710...  0.1572 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4118...  Training loss: 1.3618...  0.1721 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4119...  Training loss: 1.3374...  0.2194 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4120...  Training loss: 1.3706...  0.2503 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4121...  Training loss: 1.3762...  0.1959 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4122...  Training loss: 1.3679...  0.1656 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4123...  Training loss: 1.3187...  0.1950 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4124...  Training loss: 1.3849...  0.1594 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4125...  Training loss: 1.3742...  0.1605 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4126...  Training loss: 1.3427...  0.2425 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4127...  Training loss: 1.3721...  0.2748 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4128...  Training loss: 1.3661...  0.2366 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4129...  Training loss: 1.3439...  0.1920 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4130...  Training loss: 1.3486...  0.2069 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4131...  Training loss: 1.4012...  0.1639 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4132...  Training loss: 1.3327...  0.2204 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4133...  Training loss: 1.3257...  0.2311 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4134...  Training loss: 1.3243...  0.2676 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4135...  Training loss: 1.3545...  0.2382 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4136...  Training loss: 1.3515...  0.2162 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4137...  Training loss: 1.3569...  0.2850 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4138...  Training loss: 1.3212...  0.1856 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4139...  Training loss: 1.3647...  0.2602 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4140...  Training loss: 1.3756...  0.2462 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4141...  Training loss: 1.3523...  0.1797 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4142...  Training loss: 1.3507...  0.1804 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4143...  Training loss: 1.3159...  0.1885 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4144...  Training loss: 1.3429...  0.2614 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4145...  Training loss: 1.3552...  0.1942 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4146...  Training loss: 1.3546...  0.2332 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4147...  Training loss: 1.3710...  0.1974 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4148...  Training loss: 1.3453...  0.2289 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4149...  Training loss: 1.3330...  0.2424 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4150...  Training loss: 1.3456...  0.2059 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4151...  Training loss: 1.3232...  0.1785 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4152...  Training loss: 1.3376...  0.2215 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/30...  Training Step: 4153...  Training loss: 1.3568...  0.2596 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4154...  Training loss: 1.3701...  0.1824 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4155...  Training loss: 1.3707...  0.1734 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4156...  Training loss: 1.3172...  0.1787 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4157...  Training loss: 1.3396...  0.1619 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4158...  Training loss: 1.3531...  0.2152 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4159...  Training loss: 1.3649...  0.2225 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4160...  Training loss: 1.3719...  0.1854 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4161...  Training loss: 1.3712...  0.2684 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4162...  Training loss: 1.3647...  0.1883 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4163...  Training loss: 1.3667...  0.1886 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4164...  Training loss: 1.3357...  0.1839 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4165...  Training loss: 1.3519...  0.1734 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4166...  Training loss: 1.3380...  0.1908 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4167...  Training loss: 1.3437...  0.2458 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4168...  Training loss: 1.3608...  0.2737 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4169...  Training loss: 1.3747...  0.2551 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4170...  Training loss: 1.3343...  0.1739 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4171...  Training loss: 1.3325...  0.2331 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4172...  Training loss: 1.3310...  0.2046 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4173...  Training loss: 1.3545...  0.1595 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4174...  Training loss: 1.3800...  0.1764 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4175...  Training loss: 1.3417...  0.2458 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4176...  Training loss: 1.3156...  0.1784 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4177...  Training loss: 1.3178...  0.2401 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4178...  Training loss: 1.3299...  0.1883 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4179...  Training loss: 1.3388...  0.2402 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4180...  Training loss: 1.3933...  0.2724 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4181...  Training loss: 1.3928...  0.2168 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4182...  Training loss: 1.3546...  0.1607 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4183...  Training loss: 1.3482...  0.1818 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4184...  Training loss: 1.3456...  0.2539 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4185...  Training loss: 1.3509...  0.2662 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4186...  Training loss: 1.3428...  0.2402 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4187...  Training loss: 1.3539...  0.1764 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4188...  Training loss: 1.3269...  0.1631 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4189...  Training loss: 1.3661...  0.2077 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4190...  Training loss: 1.3355...  0.2574 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4191...  Training loss: 1.3370...  0.2078 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4192...  Training loss: 1.3124...  0.2503 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4193...  Training loss: 1.3480...  0.1807 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4194...  Training loss: 1.3535...  0.1818 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4195...  Training loss: 1.3652...  0.2528 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4196...  Training loss: 1.3385...  0.1960 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4197...  Training loss: 1.3157...  0.1667 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4198...  Training loss: 1.3556...  0.2282 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4199...  Training loss: 1.3330...  0.1999 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4200...  Training loss: 1.3527...  0.2176 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4201...  Training loss: 1.3367...  0.1722 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4202...  Training loss: 1.3489...  0.1591 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4203...  Training loss: 1.3565...  0.2105 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4204...  Training loss: 1.3736...  0.2479 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4205...  Training loss: 1.3138...  0.2261 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4206...  Training loss: 1.3293...  0.2210 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4207...  Training loss: 1.3456...  0.2360 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4208...  Training loss: 1.3408...  0.2304 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4209...  Training loss: 1.3613...  0.1723 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4210...  Training loss: 1.3709...  0.2229 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4211...  Training loss: 1.3802...  0.2526 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4212...  Training loss: 1.3158...  0.1904 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4213...  Training loss: 1.3083...  0.2447 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4214...  Training loss: 1.3619...  0.1978 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4215...  Training loss: 1.3445...  0.2918 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4216...  Training loss: 1.3728...  0.2477 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4217...  Training loss: 1.3264...  0.2545 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4218...  Training loss: 1.3272...  0.1865 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4219...  Training loss: 1.3254...  0.1639 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4220...  Training loss: 1.3452...  0.1932 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4221...  Training loss: 1.3317...  0.2440 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4222...  Training loss: 1.3454...  0.1833 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4223...  Training loss: 1.3371...  0.1597 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4224...  Training loss: 1.3371...  0.2301 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4225...  Training loss: 1.3264...  0.1600 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4226...  Training loss: 1.3088...  0.1620 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4227...  Training loss: 1.3562...  0.1675 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4228...  Training loss: 1.3436...  0.1569 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4229...  Training loss: 1.3408...  0.2428 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4230...  Training loss: 1.3223...  0.1746 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4231...  Training loss: 1.3422...  0.1613 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4232...  Training loss: 1.3636...  0.1570 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4233...  Training loss: 1.3319...  0.2201 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4234...  Training loss: 1.3301...  0.2462 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4235...  Training loss: 1.3209...  0.1839 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4236...  Training loss: 1.3122...  0.1621 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4237...  Training loss: 1.3356...  0.1575 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4238...  Training loss: 1.3437...  0.2404 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4239...  Training loss: 1.3318...  0.2206 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4240...  Training loss: 1.3497...  0.2415 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4241...  Training loss: 1.3311...  0.1984 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4242...  Training loss: 1.3640...  0.1643 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4243...  Training loss: 1.3489...  0.1581 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4244...  Training loss: 1.3296...  0.1540 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4245...  Training loss: 1.3215...  0.1591 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4246...  Training loss: 1.3263...  0.2547 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4247...  Training loss: 1.3458...  0.2110 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4248...  Training loss: 1.3439...  0.1713 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4249...  Training loss: 1.3408...  0.2502 sec/batch\n",
      "Epoch: 25/30...  Training Step: 4250...  Training loss: 1.3321...  0.2016 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/30...  Training Step: 4251...  Training loss: 1.4125...  0.2538 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4252...  Training loss: 1.3011...  0.1936 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4253...  Training loss: 1.3175...  0.1951 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4254...  Training loss: 1.3585...  0.1770 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4255...  Training loss: 1.3467...  0.1659 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4256...  Training loss: 1.3656...  0.2000 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4257...  Training loss: 1.3386...  0.3044 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4258...  Training loss: 1.3307...  0.2771 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4259...  Training loss: 1.3024...  0.2381 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4260...  Training loss: 1.3168...  0.2627 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4261...  Training loss: 1.3001...  0.2469 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4262...  Training loss: 1.3343...  0.1794 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4263...  Training loss: 1.3185...  0.1614 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4264...  Training loss: 1.3317...  0.2499 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4265...  Training loss: 1.3787...  0.2440 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4266...  Training loss: 1.3822...  0.2506 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4267...  Training loss: 1.3182...  0.1580 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4268...  Training loss: 1.3348...  0.2345 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4269...  Training loss: 1.3386...  0.2693 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4270...  Training loss: 1.3343...  0.2127 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4271...  Training loss: 1.3498...  0.2016 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4272...  Training loss: 1.3008...  0.1741 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4273...  Training loss: 1.3049...  0.2361 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4274...  Training loss: 1.2981...  0.2164 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4275...  Training loss: 1.3460...  0.2122 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4276...  Training loss: 1.3381...  0.1655 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4277...  Training loss: 1.3554...  0.2055 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4278...  Training loss: 1.3637...  0.2676 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4279...  Training loss: 1.3366...  0.2463 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4280...  Training loss: 1.3071...  0.2448 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4281...  Training loss: 1.3336...  0.2538 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4282...  Training loss: 1.3236...  0.1985 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4283...  Training loss: 1.3273...  0.2133 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4284...  Training loss: 1.3167...  0.2425 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4285...  Training loss: 1.3244...  0.2580 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4286...  Training loss: 1.3312...  0.1854 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4287...  Training loss: 1.3721...  0.2485 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4288...  Training loss: 1.3573...  0.2352 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4289...  Training loss: 1.3288...  0.2434 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4290...  Training loss: 1.3588...  0.2144 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4291...  Training loss: 1.3465...  0.1752 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4292...  Training loss: 1.3277...  0.1609 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4293...  Training loss: 1.3118...  0.1861 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4294...  Training loss: 1.3716...  0.1970 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4295...  Training loss: 1.3672...  0.2761 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4296...  Training loss: 1.3284...  0.2730 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4297...  Training loss: 1.3657...  0.2601 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4298...  Training loss: 1.3460...  0.2696 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4299...  Training loss: 1.3415...  0.2617 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4300...  Training loss: 1.3438...  0.1931 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4301...  Training loss: 1.3747...  0.1589 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4302...  Training loss: 1.3219...  0.2493 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4303...  Training loss: 1.3280...  0.2637 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4304...  Training loss: 1.3271...  0.2554 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4305...  Training loss: 1.3409...  0.2430 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4306...  Training loss: 1.3443...  0.2530 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4307...  Training loss: 1.3413...  0.2693 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4308...  Training loss: 1.3196...  0.2665 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4309...  Training loss: 1.3491...  0.2043 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4310...  Training loss: 1.3643...  0.1922 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4311...  Training loss: 1.3390...  0.2678 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4312...  Training loss: 1.3328...  0.2201 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4313...  Training loss: 1.3116...  0.2652 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4314...  Training loss: 1.3306...  0.2804 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4315...  Training loss: 1.3563...  0.2119 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4316...  Training loss: 1.3531...  0.2698 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4317...  Training loss: 1.3658...  0.2538 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4318...  Training loss: 1.3269...  0.2618 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4319...  Training loss: 1.3272...  0.2107 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4320...  Training loss: 1.3370...  0.1580 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4321...  Training loss: 1.2990...  0.2852 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4322...  Training loss: 1.3389...  0.2521 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4323...  Training loss: 1.3331...  0.1705 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4324...  Training loss: 1.3555...  0.1926 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4325...  Training loss: 1.3680...  0.2471 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4326...  Training loss: 1.3126...  0.1976 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4327...  Training loss: 1.3342...  0.2844 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4328...  Training loss: 1.3488...  0.2145 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4329...  Training loss: 1.3578...  0.1610 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4330...  Training loss: 1.3648...  0.1798 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4331...  Training loss: 1.3674...  0.2507 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4332...  Training loss: 1.3599...  0.2112 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4333...  Training loss: 1.3465...  0.1965 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4334...  Training loss: 1.3123...  0.2788 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4335...  Training loss: 1.3464...  0.2643 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4336...  Training loss: 1.3273...  0.2737 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4337...  Training loss: 1.3473...  0.2729 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4338...  Training loss: 1.3511...  0.1810 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4339...  Training loss: 1.3732...  0.1648 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4340...  Training loss: 1.3200...  0.2317 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4341...  Training loss: 1.3339...  0.2019 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4342...  Training loss: 1.3233...  0.2404 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4343...  Training loss: 1.3498...  0.1932 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4344...  Training loss: 1.3683...  0.2529 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4345...  Training loss: 1.3295...  0.2566 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4346...  Training loss: 1.3145...  0.2658 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4347...  Training loss: 1.3185...  0.2394 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4348...  Training loss: 1.3178...  0.1545 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/30...  Training Step: 4349...  Training loss: 1.3332...  0.2428 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4350...  Training loss: 1.3782...  0.2391 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4351...  Training loss: 1.3710...  0.2670 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4352...  Training loss: 1.3439...  0.2496 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4353...  Training loss: 1.3500...  0.2039 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4354...  Training loss: 1.3437...  0.1596 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4355...  Training loss: 1.3317...  0.1674 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4356...  Training loss: 1.3344...  0.2259 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4357...  Training loss: 1.3345...  0.2095 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4358...  Training loss: 1.3225...  0.1561 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4359...  Training loss: 1.3469...  0.1675 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4360...  Training loss: 1.3210...  0.2204 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4361...  Training loss: 1.3267...  0.2380 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4362...  Training loss: 1.3019...  0.2172 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4363...  Training loss: 1.3390...  0.1807 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4364...  Training loss: 1.3508...  0.2600 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4365...  Training loss: 1.3565...  0.2223 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4366...  Training loss: 1.3367...  0.1639 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4367...  Training loss: 1.3261...  0.1559 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4368...  Training loss: 1.3455...  0.2502 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4369...  Training loss: 1.3198...  0.1526 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4370...  Training loss: 1.3439...  0.2488 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4371...  Training loss: 1.3289...  0.2247 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4372...  Training loss: 1.3343...  0.1617 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4373...  Training loss: 1.3470...  0.1941 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4374...  Training loss: 1.3710...  0.2448 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4375...  Training loss: 1.3107...  0.1818 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4376...  Training loss: 1.3203...  0.2111 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4377...  Training loss: 1.3435...  0.1881 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4378...  Training loss: 1.3357...  0.1653 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4379...  Training loss: 1.3430...  0.1792 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4380...  Training loss: 1.3828...  0.1687 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4381...  Training loss: 1.3701...  0.1905 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4382...  Training loss: 1.2989...  0.2556 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4383...  Training loss: 1.3064...  0.2241 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4384...  Training loss: 1.3575...  0.2716 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4385...  Training loss: 1.3344...  0.2253 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4386...  Training loss: 1.3573...  0.1769 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4387...  Training loss: 1.3221...  0.2396 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4388...  Training loss: 1.3299...  0.2702 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4389...  Training loss: 1.3278...  0.2108 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4390...  Training loss: 1.3408...  0.1705 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4391...  Training loss: 1.3249...  0.1820 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4392...  Training loss: 1.3299...  0.2152 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4393...  Training loss: 1.3189...  0.1709 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4394...  Training loss: 1.3194...  0.2090 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4395...  Training loss: 1.3232...  0.2568 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4396...  Training loss: 1.2898...  0.2336 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4397...  Training loss: 1.3504...  0.2084 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4398...  Training loss: 1.3268...  0.1583 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4399...  Training loss: 1.3336...  0.2583 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4400...  Training loss: 1.3057...  0.2417 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4401...  Training loss: 1.3246...  0.1905 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4402...  Training loss: 1.3586...  0.1609 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4403...  Training loss: 1.3276...  0.1613 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4404...  Training loss: 1.3274...  0.2137 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4405...  Training loss: 1.3191...  0.1615 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4406...  Training loss: 1.3086...  0.2865 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4407...  Training loss: 1.3252...  0.1850 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4408...  Training loss: 1.3308...  0.2331 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4409...  Training loss: 1.3182...  0.2556 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4410...  Training loss: 1.3431...  0.2251 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4411...  Training loss: 1.3222...  0.2471 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4412...  Training loss: 1.3468...  0.2416 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4413...  Training loss: 1.3409...  0.2231 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4414...  Training loss: 1.3213...  0.1791 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4415...  Training loss: 1.3111...  0.1591 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4416...  Training loss: 1.3133...  0.1923 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4417...  Training loss: 1.3354...  0.2450 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4418...  Training loss: 1.3316...  0.2252 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4419...  Training loss: 1.3384...  0.1578 sec/batch\n",
      "Epoch: 26/30...  Training Step: 4420...  Training loss: 1.3261...  0.1563 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4421...  Training loss: 1.3899...  0.1874 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4422...  Training loss: 1.2893...  0.2139 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4423...  Training loss: 1.3131...  0.1849 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4424...  Training loss: 1.3501...  0.1822 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4425...  Training loss: 1.3403...  0.1674 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4426...  Training loss: 1.3588...  0.1867 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4427...  Training loss: 1.3274...  0.2080 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4428...  Training loss: 1.3191...  0.1782 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4429...  Training loss: 1.2938...  0.1636 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4430...  Training loss: 1.3170...  0.2532 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4431...  Training loss: 1.2983...  0.2569 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4432...  Training loss: 1.3269...  0.1979 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4433...  Training loss: 1.3058...  0.2390 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4434...  Training loss: 1.3272...  0.1639 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4435...  Training loss: 1.3652...  0.2453 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4436...  Training loss: 1.3725...  0.1679 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4437...  Training loss: 1.3111...  0.2006 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4438...  Training loss: 1.3147...  0.1938 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4439...  Training loss: 1.3288...  0.2088 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4440...  Training loss: 1.3257...  0.2301 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4441...  Training loss: 1.3322...  0.2644 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4442...  Training loss: 1.2882...  0.1799 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4443...  Training loss: 1.2995...  0.1738 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4444...  Training loss: 1.2861...  0.2296 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4445...  Training loss: 1.3361...  0.2570 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4446...  Training loss: 1.3293...  0.1965 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/30...  Training Step: 4447...  Training loss: 1.3458...  0.1607 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4448...  Training loss: 1.3509...  0.1860 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4449...  Training loss: 1.3160...  0.1617 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4450...  Training loss: 1.2989...  0.2638 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4451...  Training loss: 1.3284...  0.2783 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4452...  Training loss: 1.3141...  0.2304 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4453...  Training loss: 1.3133...  0.2284 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4454...  Training loss: 1.3040...  0.1948 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4455...  Training loss: 1.3154...  0.1814 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4456...  Training loss: 1.3200...  0.1675 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4457...  Training loss: 1.3482...  0.2099 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4458...  Training loss: 1.3402...  0.1747 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4459...  Training loss: 1.3165...  0.1589 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4460...  Training loss: 1.3489...  0.2436 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4461...  Training loss: 1.3428...  0.2433 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4462...  Training loss: 1.3326...  0.2165 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4463...  Training loss: 1.3069...  0.2185 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4464...  Training loss: 1.3688...  0.1691 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4465...  Training loss: 1.3509...  0.2314 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4466...  Training loss: 1.3192...  0.2236 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4467...  Training loss: 1.3514...  0.1540 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4468...  Training loss: 1.3534...  0.1612 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4469...  Training loss: 1.3318...  0.2043 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4470...  Training loss: 1.3259...  0.1611 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4471...  Training loss: 1.3737...  0.1878 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4472...  Training loss: 1.3197...  0.2494 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4473...  Training loss: 1.3130...  0.2498 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4474...  Training loss: 1.3124...  0.1844 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4475...  Training loss: 1.3281...  0.1795 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4476...  Training loss: 1.3432...  0.1589 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4477...  Training loss: 1.3362...  0.1724 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4478...  Training loss: 1.3200...  0.2486 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4479...  Training loss: 1.3436...  0.1836 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4480...  Training loss: 1.3453...  0.2514 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4481...  Training loss: 1.3285...  0.2082 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4482...  Training loss: 1.3173...  0.1612 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4483...  Training loss: 1.3127...  0.2103 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4484...  Training loss: 1.3315...  0.1680 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4485...  Training loss: 1.3462...  0.2104 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4486...  Training loss: 1.3295...  0.2462 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4487...  Training loss: 1.3482...  0.2561 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4488...  Training loss: 1.3308...  0.2407 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4489...  Training loss: 1.3100...  0.2338 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4490...  Training loss: 1.3266...  0.2363 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4491...  Training loss: 1.3024...  0.2782 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4492...  Training loss: 1.3161...  0.2319 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4493...  Training loss: 1.3303...  0.2173 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4494...  Training loss: 1.3520...  0.2619 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4495...  Training loss: 1.3588...  0.2267 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4496...  Training loss: 1.3021...  0.1543 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4497...  Training loss: 1.3364...  0.2532 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4498...  Training loss: 1.3443...  0.2082 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4499...  Training loss: 1.3427...  0.1585 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4500...  Training loss: 1.3627...  0.1723 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4501...  Training loss: 1.3560...  0.1902 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4502...  Training loss: 1.3538...  0.1640 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4503...  Training loss: 1.3399...  0.1692 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4504...  Training loss: 1.3143...  0.2216 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4505...  Training loss: 1.3300...  0.1919 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4506...  Training loss: 1.3187...  0.2077 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4507...  Training loss: 1.3290...  0.1925 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4508...  Training loss: 1.3554...  0.2093 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4509...  Training loss: 1.3698...  0.2257 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4510...  Training loss: 1.3166...  0.2378 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4511...  Training loss: 1.3261...  0.2292 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4512...  Training loss: 1.3238...  0.2312 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4513...  Training loss: 1.3343...  0.2428 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4514...  Training loss: 1.3604...  0.2229 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4515...  Training loss: 1.3304...  0.2852 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4516...  Training loss: 1.3043...  0.2812 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4517...  Training loss: 1.3096...  0.2596 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4518...  Training loss: 1.3128...  0.1640 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4519...  Training loss: 1.3258...  0.1596 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4520...  Training loss: 1.3707...  0.1528 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4521...  Training loss: 1.3634...  0.2210 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4522...  Training loss: 1.3333...  0.1592 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4523...  Training loss: 1.3357...  0.2664 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4524...  Training loss: 1.3308...  0.2516 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4525...  Training loss: 1.3280...  0.2721 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4526...  Training loss: 1.3256...  0.2806 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4527...  Training loss: 1.3267...  0.2147 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4528...  Training loss: 1.3112...  0.1845 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4529...  Training loss: 1.3378...  0.1572 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4530...  Training loss: 1.3139...  0.1919 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4531...  Training loss: 1.3104...  0.1938 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4532...  Training loss: 1.3045...  0.2585 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4533...  Training loss: 1.3173...  0.2633 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4534...  Training loss: 1.3424...  0.1765 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4535...  Training loss: 1.3416...  0.1645 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4536...  Training loss: 1.3362...  0.1604 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4537...  Training loss: 1.3013...  0.2612 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4538...  Training loss: 1.3327...  0.2681 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4539...  Training loss: 1.3183...  0.1904 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4540...  Training loss: 1.3347...  0.2194 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4541...  Training loss: 1.3218...  0.2569 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4542...  Training loss: 1.3362...  0.2579 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4543...  Training loss: 1.3386...  0.2283 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4544...  Training loss: 1.3630...  0.2344 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/30...  Training Step: 4545...  Training loss: 1.2945...  0.2746 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4546...  Training loss: 1.3125...  0.2531 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4547...  Training loss: 1.3338...  0.2335 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4548...  Training loss: 1.3366...  0.1558 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4549...  Training loss: 1.3384...  0.1658 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4550...  Training loss: 1.3710...  0.1559 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4551...  Training loss: 1.3685...  0.2352 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4552...  Training loss: 1.2968...  0.1770 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4553...  Training loss: 1.2989...  0.1646 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4554...  Training loss: 1.3372...  0.1618 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4555...  Training loss: 1.3218...  0.2547 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4556...  Training loss: 1.3566...  0.2134 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4557...  Training loss: 1.3122...  0.1666 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4558...  Training loss: 1.3122...  0.1621 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4559...  Training loss: 1.3131...  0.1600 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4560...  Training loss: 1.3355...  0.1659 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4561...  Training loss: 1.3282...  0.2436 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4562...  Training loss: 1.3266...  0.1694 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4563...  Training loss: 1.3181...  0.1696 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4564...  Training loss: 1.3136...  0.2585 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4565...  Training loss: 1.3102...  0.1815 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4566...  Training loss: 1.2846...  0.2454 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4567...  Training loss: 1.3395...  0.2364 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4568...  Training loss: 1.3200...  0.1595 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4569...  Training loss: 1.3312...  0.1968 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4570...  Training loss: 1.2989...  0.2212 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4571...  Training loss: 1.3241...  0.1654 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4572...  Training loss: 1.3564...  0.2290 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4573...  Training loss: 1.3152...  0.2180 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4574...  Training loss: 1.3095...  0.2389 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4575...  Training loss: 1.3041...  0.1828 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4576...  Training loss: 1.3004...  0.1948 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4577...  Training loss: 1.3183...  0.2045 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4578...  Training loss: 1.3245...  0.1700 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4579...  Training loss: 1.3140...  0.2225 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4580...  Training loss: 1.3326...  0.1577 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4581...  Training loss: 1.3109...  0.1647 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4582...  Training loss: 1.3386...  0.1531 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4583...  Training loss: 1.3142...  0.2621 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4584...  Training loss: 1.3289...  0.2627 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4585...  Training loss: 1.3089...  0.2724 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4586...  Training loss: 1.3133...  0.2738 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4587...  Training loss: 1.3224...  0.2754 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4588...  Training loss: 1.3324...  0.2570 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4589...  Training loss: 1.3297...  0.2714 sec/batch\n",
      "Epoch: 27/30...  Training Step: 4590...  Training loss: 1.3193...  0.1857 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4591...  Training loss: 1.3891...  0.1777 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4592...  Training loss: 1.2879...  0.2486 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4593...  Training loss: 1.3156...  0.2647 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4594...  Training loss: 1.3394...  0.2205 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4595...  Training loss: 1.3336...  0.2077 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4596...  Training loss: 1.3396...  0.2783 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4597...  Training loss: 1.3258...  0.2353 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4598...  Training loss: 1.3144...  0.2393 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4599...  Training loss: 1.2868...  0.2314 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4600...  Training loss: 1.2888...  0.1613 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4601...  Training loss: 1.2882...  0.2465 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4602...  Training loss: 1.3192...  0.1628 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4603...  Training loss: 1.3169...  0.1641 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4604...  Training loss: 1.3107...  0.1849 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4605...  Training loss: 1.3653...  0.2371 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4606...  Training loss: 1.3688...  0.1634 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4607...  Training loss: 1.3010...  0.1667 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4608...  Training loss: 1.3232...  0.1959 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4609...  Training loss: 1.3174...  0.2532 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4610...  Training loss: 1.3123...  0.2791 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4611...  Training loss: 1.3327...  0.2326 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4612...  Training loss: 1.2824...  0.1682 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4613...  Training loss: 1.2948...  0.2597 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4614...  Training loss: 1.2874...  0.2031 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4615...  Training loss: 1.3300...  0.2655 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4616...  Training loss: 1.3309...  0.2182 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4617...  Training loss: 1.3413...  0.1547 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4618...  Training loss: 1.3372...  0.1862 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4619...  Training loss: 1.3043...  0.2039 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4620...  Training loss: 1.2875...  0.2384 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4621...  Training loss: 1.3189...  0.2115 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4622...  Training loss: 1.3144...  0.1579 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4623...  Training loss: 1.3115...  0.1717 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4624...  Training loss: 1.3102...  0.1788 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4625...  Training loss: 1.2951...  0.2131 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4626...  Training loss: 1.3118...  0.2687 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4627...  Training loss: 1.3540...  0.1645 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4628...  Training loss: 1.3391...  0.2209 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4629...  Training loss: 1.3110...  0.2221 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4630...  Training loss: 1.3362...  0.2470 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4631...  Training loss: 1.3395...  0.2212 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4632...  Training loss: 1.3193...  0.2202 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4633...  Training loss: 1.2997...  0.2543 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4634...  Training loss: 1.3594...  0.1705 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4635...  Training loss: 1.3537...  0.1603 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4636...  Training loss: 1.3169...  0.2444 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4637...  Training loss: 1.3426...  0.2576 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4638...  Training loss: 1.3439...  0.1886 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4639...  Training loss: 1.3139...  0.1934 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4640...  Training loss: 1.3150...  0.1747 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4641...  Training loss: 1.3650...  0.1754 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4642...  Training loss: 1.3120...  0.2300 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4643...  Training loss: 1.3162...  0.1788 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/30...  Training Step: 4644...  Training loss: 1.3125...  0.2314 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4645...  Training loss: 1.3301...  0.2094 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4646...  Training loss: 1.3290...  0.2427 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4647...  Training loss: 1.3344...  0.2139 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4648...  Training loss: 1.3040...  0.2521 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4649...  Training loss: 1.3366...  0.2079 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4650...  Training loss: 1.3397...  0.2436 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4651...  Training loss: 1.3266...  0.2646 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4652...  Training loss: 1.3269...  0.2663 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4653...  Training loss: 1.2958...  0.2541 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4654...  Training loss: 1.3139...  0.2478 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4655...  Training loss: 1.3326...  0.2061 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4656...  Training loss: 1.3342...  0.1953 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4657...  Training loss: 1.3434...  0.2503 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4658...  Training loss: 1.3224...  0.2003 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4659...  Training loss: 1.3076...  0.1699 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4660...  Training loss: 1.3196...  0.2418 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4661...  Training loss: 1.2930...  0.2454 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4662...  Training loss: 1.3165...  0.2590 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4663...  Training loss: 1.3184...  0.2241 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4664...  Training loss: 1.3392...  0.2267 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4665...  Training loss: 1.3486...  0.2142 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4666...  Training loss: 1.2898...  0.2082 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4667...  Training loss: 1.3272...  0.1701 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4668...  Training loss: 1.3267...  0.2424 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4669...  Training loss: 1.3482...  0.2171 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4670...  Training loss: 1.3513...  0.2195 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4671...  Training loss: 1.3503...  0.2356 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4672...  Training loss: 1.3472...  0.1530 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4673...  Training loss: 1.3387...  0.1584 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4674...  Training loss: 1.3054...  0.2677 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4675...  Training loss: 1.3215...  0.2495 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4676...  Training loss: 1.3146...  0.2131 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4677...  Training loss: 1.3243...  0.1885 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4678...  Training loss: 1.3448...  0.2521 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4679...  Training loss: 1.3396...  0.2081 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4680...  Training loss: 1.3082...  0.2475 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4681...  Training loss: 1.3161...  0.2472 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4682...  Training loss: 1.3118...  0.2368 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4683...  Training loss: 1.3303...  0.2153 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4684...  Training loss: 1.3509...  0.1944 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4685...  Training loss: 1.3166...  0.1622 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4686...  Training loss: 1.2888...  0.2488 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4687...  Training loss: 1.3010...  0.1775 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4688...  Training loss: 1.3018...  0.2330 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4689...  Training loss: 1.3221...  0.1930 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4690...  Training loss: 1.3697...  0.1618 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4691...  Training loss: 1.3566...  0.1698 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4692...  Training loss: 1.3276...  0.1608 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4693...  Training loss: 1.3254...  0.2159 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4694...  Training loss: 1.3218...  0.2706 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4695...  Training loss: 1.3242...  0.1957 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4696...  Training loss: 1.3274...  0.2221 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4697...  Training loss: 1.3187...  0.1847 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4698...  Training loss: 1.3037...  0.1581 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4699...  Training loss: 1.3274...  0.2548 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4700...  Training loss: 1.3143...  0.2062 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4701...  Training loss: 1.3091...  0.1742 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4702...  Training loss: 1.3009...  0.2197 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4703...  Training loss: 1.3224...  0.2118 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4704...  Training loss: 1.3339...  0.2631 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4705...  Training loss: 1.3310...  0.2266 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4706...  Training loss: 1.3273...  0.1661 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4707...  Training loss: 1.2989...  0.2300 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4708...  Training loss: 1.3225...  0.2501 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4709...  Training loss: 1.3088...  0.2070 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4710...  Training loss: 1.3281...  0.2580 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4711...  Training loss: 1.3047...  0.2797 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4712...  Training loss: 1.3289...  0.2423 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4713...  Training loss: 1.3260...  0.2647 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4714...  Training loss: 1.3540...  0.2107 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4715...  Training loss: 1.2871...  0.1950 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4716...  Training loss: 1.3085...  0.2362 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4717...  Training loss: 1.3311...  0.1560 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4718...  Training loss: 1.3340...  0.1698 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4719...  Training loss: 1.3385...  0.1533 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4720...  Training loss: 1.3562...  0.2588 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4721...  Training loss: 1.3504...  0.2683 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4722...  Training loss: 1.2878...  0.2164 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4723...  Training loss: 1.3010...  0.1636 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4724...  Training loss: 1.3374...  0.2205 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4725...  Training loss: 1.3180...  0.2155 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4726...  Training loss: 1.3515...  0.1577 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4727...  Training loss: 1.2975...  0.1967 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4728...  Training loss: 1.3001...  0.2398 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4729...  Training loss: 1.3077...  0.2554 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4730...  Training loss: 1.3386...  0.1667 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4731...  Training loss: 1.3031...  0.1865 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4732...  Training loss: 1.3231...  0.2479 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4733...  Training loss: 1.3154...  0.1714 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4734...  Training loss: 1.3104...  0.1749 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4735...  Training loss: 1.3030...  0.1664 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4736...  Training loss: 1.2779...  0.1818 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4737...  Training loss: 1.3330...  0.2020 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4738...  Training loss: 1.3223...  0.2005 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4739...  Training loss: 1.3114...  0.2361 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4740...  Training loss: 1.2905...  0.2770 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4741...  Training loss: 1.3186...  0.2396 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/30...  Training Step: 4742...  Training loss: 1.3267...  0.2017 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4743...  Training loss: 1.3100...  0.2466 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4744...  Training loss: 1.3044...  0.2331 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4745...  Training loss: 1.3043...  0.2438 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4746...  Training loss: 1.2871...  0.1749 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4747...  Training loss: 1.3139...  0.1559 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4748...  Training loss: 1.3186...  0.2092 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4749...  Training loss: 1.2951...  0.2113 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4750...  Training loss: 1.3240...  0.1651 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4751...  Training loss: 1.3095...  0.2522 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4752...  Training loss: 1.3381...  0.2540 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4753...  Training loss: 1.3209...  0.2382 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4754...  Training loss: 1.3100...  0.2627 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4755...  Training loss: 1.2993...  0.2176 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4756...  Training loss: 1.3005...  0.1597 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4757...  Training loss: 1.3233...  0.1673 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4758...  Training loss: 1.3087...  0.1834 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4759...  Training loss: 1.3114...  0.2532 sec/batch\n",
      "Epoch: 28/30...  Training Step: 4760...  Training loss: 1.3101...  0.2487 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4761...  Training loss: 1.3798...  0.1543 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4762...  Training loss: 1.2638...  0.2138 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4763...  Training loss: 1.2808...  0.2537 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4764...  Training loss: 1.3355...  0.1918 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4765...  Training loss: 1.3171...  0.1583 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4766...  Training loss: 1.3415...  0.1881 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4767...  Training loss: 1.3190...  0.1918 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4768...  Training loss: 1.3035...  0.1856 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4769...  Training loss: 1.2775...  0.2512 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4770...  Training loss: 1.2846...  0.1651 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4771...  Training loss: 1.2925...  0.1611 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4772...  Training loss: 1.3079...  0.1772 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4773...  Training loss: 1.2973...  0.2426 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4774...  Training loss: 1.3121...  0.1910 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4775...  Training loss: 1.3498...  0.1765 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4776...  Training loss: 1.3504...  0.2751 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4777...  Training loss: 1.2940...  0.2678 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4778...  Training loss: 1.3043...  0.2195 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4779...  Training loss: 1.3088...  0.2455 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4780...  Training loss: 1.3025...  0.2230 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4781...  Training loss: 1.3225...  0.2143 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4782...  Training loss: 1.2738...  0.2208 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4783...  Training loss: 1.2860...  0.2549 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4784...  Training loss: 1.2762...  0.2369 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4785...  Training loss: 1.3222...  0.1595 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4786...  Training loss: 1.3044...  0.1789 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4787...  Training loss: 1.3263...  0.1916 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4788...  Training loss: 1.3369...  0.1594 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4789...  Training loss: 1.3116...  0.1856 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4790...  Training loss: 1.2928...  0.2763 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4791...  Training loss: 1.3155...  0.2660 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4792...  Training loss: 1.2957...  0.2359 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4793...  Training loss: 1.3080...  0.2041 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4794...  Training loss: 1.2856...  0.2439 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4795...  Training loss: 1.2925...  0.2601 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4796...  Training loss: 1.3041...  0.2445 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4797...  Training loss: 1.3448...  0.1714 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4798...  Training loss: 1.3192...  0.1601 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4799...  Training loss: 1.3042...  0.1566 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4800...  Training loss: 1.3385...  0.1709 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4801...  Training loss: 1.3365...  0.2543 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4802...  Training loss: 1.3200...  0.2232 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4803...  Training loss: 1.2844...  0.2085 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4804...  Training loss: 1.3479...  0.1578 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4805...  Training loss: 1.3450...  0.1625 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4806...  Training loss: 1.3027...  0.1712 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4807...  Training loss: 1.3396...  0.2124 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4808...  Training loss: 1.3349...  0.2276 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4809...  Training loss: 1.3166...  0.1911 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4810...  Training loss: 1.3223...  0.2271 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4811...  Training loss: 1.3742...  0.2519 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4812...  Training loss: 1.2968...  0.1791 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4813...  Training loss: 1.2953...  0.2738 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4814...  Training loss: 1.2972...  0.1937 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4815...  Training loss: 1.3236...  0.1936 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4816...  Training loss: 1.3217...  0.2515 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4817...  Training loss: 1.3174...  0.1981 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4818...  Training loss: 1.2972...  0.2013 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4819...  Training loss: 1.3335...  0.1576 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4820...  Training loss: 1.3269...  0.2520 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4821...  Training loss: 1.3187...  0.2570 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4822...  Training loss: 1.3117...  0.2122 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4823...  Training loss: 1.2881...  0.1926 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4824...  Training loss: 1.3146...  0.2390 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4825...  Training loss: 1.3219...  0.1754 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4826...  Training loss: 1.3250...  0.2566 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4827...  Training loss: 1.3375...  0.2469 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4828...  Training loss: 1.3135...  0.2274 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4829...  Training loss: 1.2935...  0.2352 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4830...  Training loss: 1.3151...  0.2114 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4831...  Training loss: 1.2873...  0.2360 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4832...  Training loss: 1.3110...  0.1739 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4833...  Training loss: 1.3177...  0.1631 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4834...  Training loss: 1.3369...  0.1897 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4835...  Training loss: 1.3357...  0.1576 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4836...  Training loss: 1.2919...  0.1535 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4837...  Training loss: 1.3064...  0.1932 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4838...  Training loss: 1.3282...  0.2043 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4839...  Training loss: 1.3431...  0.1737 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/30...  Training Step: 4840...  Training loss: 1.3435...  0.2312 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4841...  Training loss: 1.3352...  0.2059 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4842...  Training loss: 1.3401...  0.1616 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4843...  Training loss: 1.3269...  0.1861 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4844...  Training loss: 1.2920...  0.2225 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4845...  Training loss: 1.3194...  0.1922 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4846...  Training loss: 1.2970...  0.1748 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4847...  Training loss: 1.3240...  0.1836 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4848...  Training loss: 1.3391...  0.2141 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4849...  Training loss: 1.3461...  0.2312 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4850...  Training loss: 1.2944...  0.2223 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4851...  Training loss: 1.3131...  0.1872 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4852...  Training loss: 1.3106...  0.2735 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4853...  Training loss: 1.3185...  0.2410 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4854...  Training loss: 1.3415...  0.2565 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4855...  Training loss: 1.3042...  0.2408 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4856...  Training loss: 1.2843...  0.1713 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4857...  Training loss: 1.2877...  0.2274 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4858...  Training loss: 1.3025...  0.1656 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4859...  Training loss: 1.3141...  0.2605 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4860...  Training loss: 1.3698...  0.2019 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4861...  Training loss: 1.3535...  0.1670 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4862...  Training loss: 1.3179...  0.2215 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4863...  Training loss: 1.3160...  0.2813 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4864...  Training loss: 1.3130...  0.1699 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4865...  Training loss: 1.3216...  0.2067 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4866...  Training loss: 1.3160...  0.2230 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4867...  Training loss: 1.3228...  0.2086 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4868...  Training loss: 1.3037...  0.2627 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4869...  Training loss: 1.3232...  0.2575 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4870...  Training loss: 1.3109...  0.2173 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4871...  Training loss: 1.2970...  0.2656 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4872...  Training loss: 1.2924...  0.2582 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4873...  Training loss: 1.3206...  0.2591 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4874...  Training loss: 1.3304...  0.2056 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4875...  Training loss: 1.3357...  0.2169 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4876...  Training loss: 1.3056...  0.1674 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4877...  Training loss: 1.2882...  0.1814 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4878...  Training loss: 1.3174...  0.2134 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4879...  Training loss: 1.2929...  0.1747 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4880...  Training loss: 1.3196...  0.2087 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4881...  Training loss: 1.3067...  0.3168 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4882...  Training loss: 1.3162...  0.1814 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4883...  Training loss: 1.3120...  0.2459 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4884...  Training loss: 1.3326...  0.2376 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4885...  Training loss: 1.2831...  0.2456 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4886...  Training loss: 1.2950...  0.1661 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4887...  Training loss: 1.3234...  0.1853 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4888...  Training loss: 1.3261...  0.2480 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4889...  Training loss: 1.3324...  0.1795 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4890...  Training loss: 1.3570...  0.2079 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4891...  Training loss: 1.3494...  0.1678 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4892...  Training loss: 1.2902...  0.1705 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4893...  Training loss: 1.2870...  0.2320 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4894...  Training loss: 1.3319...  0.2485 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4895...  Training loss: 1.3090...  0.1581 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4896...  Training loss: 1.3412...  0.3472 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4897...  Training loss: 1.2843...  0.2674 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4898...  Training loss: 1.2986...  0.3639 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4899...  Training loss: 1.3051...  0.1925 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4900...  Training loss: 1.3235...  0.2353 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4901...  Training loss: 1.3018...  0.3078 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4902...  Training loss: 1.3051...  0.2039 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4903...  Training loss: 1.3110...  0.2555 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4904...  Training loss: 1.3057...  0.2854 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4905...  Training loss: 1.3001...  0.3042 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4906...  Training loss: 1.2732...  0.2824 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4907...  Training loss: 1.3283...  0.2846 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4908...  Training loss: 1.3096...  0.3231 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4909...  Training loss: 1.3089...  0.3401 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4910...  Training loss: 1.2864...  0.1787 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4911...  Training loss: 1.3057...  0.1771 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4912...  Training loss: 1.3316...  0.2821 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4913...  Training loss: 1.2990...  0.2030 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4914...  Training loss: 1.2975...  0.2265 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4915...  Training loss: 1.2963...  0.2549 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4916...  Training loss: 1.2868...  0.2285 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4917...  Training loss: 1.3029...  0.2622 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4918...  Training loss: 1.3126...  0.2047 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4919...  Training loss: 1.2939...  0.2299 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4920...  Training loss: 1.3162...  0.2835 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4921...  Training loss: 1.2996...  0.2217 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4922...  Training loss: 1.3305...  0.2259 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4923...  Training loss: 1.3281...  0.1563 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4924...  Training loss: 1.3073...  0.1661 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4925...  Training loss: 1.2917...  0.2218 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4926...  Training loss: 1.3035...  0.2614 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4927...  Training loss: 1.3095...  0.2597 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4928...  Training loss: 1.3122...  0.1819 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4929...  Training loss: 1.3159...  0.2522 sec/batch\n",
      "Epoch: 29/30...  Training Step: 4930...  Training loss: 1.3124...  0.2396 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4931...  Training loss: 1.3796...  0.2446 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4932...  Training loss: 1.2815...  0.1849 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4933...  Training loss: 1.2936...  0.1609 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4934...  Training loss: 1.3336...  0.2676 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4935...  Training loss: 1.3251...  0.2034 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4936...  Training loss: 1.3298...  0.2044 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4937...  Training loss: 1.3082...  0.2249 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30...  Training Step: 4938...  Training loss: 1.2997...  0.2413 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4939...  Training loss: 1.2677...  0.2055 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4940...  Training loss: 1.2793...  0.2113 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4941...  Training loss: 1.2820...  0.2243 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4942...  Training loss: 1.3111...  0.1713 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4943...  Training loss: 1.2906...  0.1816 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4944...  Training loss: 1.3068...  0.1598 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4945...  Training loss: 1.3430...  0.1753 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4946...  Training loss: 1.3465...  0.2409 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4947...  Training loss: 1.2911...  0.2359 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4948...  Training loss: 1.3001...  0.2897 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4949...  Training loss: 1.3075...  0.2151 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4950...  Training loss: 1.2908...  0.2988 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4951...  Training loss: 1.3274...  0.2309 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4952...  Training loss: 1.2656...  0.2228 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4953...  Training loss: 1.2843...  0.1668 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4954...  Training loss: 1.2796...  0.2463 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4955...  Training loss: 1.3180...  0.2783 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4956...  Training loss: 1.3082...  0.1937 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4957...  Training loss: 1.3281...  0.1872 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4958...  Training loss: 1.3152...  0.2510 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4959...  Training loss: 1.3002...  0.1960 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4960...  Training loss: 1.2801...  0.1918 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4961...  Training loss: 1.3071...  0.2652 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4962...  Training loss: 1.2890...  0.1559 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4963...  Training loss: 1.2892...  0.2817 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4964...  Training loss: 1.2938...  0.2104 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4965...  Training loss: 1.2873...  0.2592 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4966...  Training loss: 1.2969...  0.2160 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4967...  Training loss: 1.3372...  0.1719 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4968...  Training loss: 1.3160...  0.2423 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4969...  Training loss: 1.3014...  0.2723 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4970...  Training loss: 1.3221...  0.1960 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4971...  Training loss: 1.3365...  0.1749 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4972...  Training loss: 1.3219...  0.1553 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4973...  Training loss: 1.2852...  0.2917 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4974...  Training loss: 1.3354...  0.2422 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4975...  Training loss: 1.3314...  0.1549 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4976...  Training loss: 1.3070...  0.2277 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4977...  Training loss: 1.3285...  0.2355 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4978...  Training loss: 1.3325...  0.2088 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4979...  Training loss: 1.3062...  0.2353 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4980...  Training loss: 1.3111...  0.1785 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4981...  Training loss: 1.3466...  0.2190 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4982...  Training loss: 1.3006...  0.2074 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4983...  Training loss: 1.2919...  0.2965 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4984...  Training loss: 1.2814...  0.2948 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4985...  Training loss: 1.3173...  0.1556 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4986...  Training loss: 1.3179...  0.1988 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4987...  Training loss: 1.3109...  0.2901 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4988...  Training loss: 1.2801...  0.2950 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4989...  Training loss: 1.3158...  0.2811 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4990...  Training loss: 1.3288...  0.1546 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4991...  Training loss: 1.3190...  0.1766 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4992...  Training loss: 1.3136...  0.3104 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4993...  Training loss: 1.2923...  0.2506 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4994...  Training loss: 1.3119...  0.2527 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4995...  Training loss: 1.3185...  0.2772 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4996...  Training loss: 1.3182...  0.2866 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4997...  Training loss: 1.3325...  0.2703 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4998...  Training loss: 1.3035...  0.2424 sec/batch\n",
      "Epoch: 30/30...  Training Step: 4999...  Training loss: 1.2812...  0.1541 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5000...  Training loss: 1.3128...  0.1762 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5001...  Training loss: 1.2779...  0.1590 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5002...  Training loss: 1.3011...  0.1724 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5003...  Training loss: 1.3045...  0.2084 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5004...  Training loss: 1.3428...  0.1653 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5005...  Training loss: 1.3327...  0.1779 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5006...  Training loss: 1.2802...  0.1547 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5007...  Training loss: 1.3005...  0.1692 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5008...  Training loss: 1.3183...  0.1580 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5009...  Training loss: 1.3290...  0.1834 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5010...  Training loss: 1.3327...  0.1743 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5011...  Training loss: 1.3257...  0.2544 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5012...  Training loss: 1.3174...  0.2450 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5013...  Training loss: 1.3280...  0.2278 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5014...  Training loss: 1.2959...  0.1878 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5015...  Training loss: 1.3058...  0.2283 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5016...  Training loss: 1.2939...  0.2548 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5017...  Training loss: 1.3088...  0.2650 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5018...  Training loss: 1.3315...  0.1887 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5019...  Training loss: 1.3379...  0.1657 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5020...  Training loss: 1.2888...  0.1776 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5021...  Training loss: 1.3005...  0.1858 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5022...  Training loss: 1.2843...  0.1582 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5023...  Training loss: 1.3122...  0.1836 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5024...  Training loss: 1.3478...  0.1783 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5025...  Training loss: 1.2977...  0.1899 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5026...  Training loss: 1.2776...  0.2412 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5027...  Training loss: 1.2898...  0.1600 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5028...  Training loss: 1.2938...  0.2738 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5029...  Training loss: 1.3058...  0.2047 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5030...  Training loss: 1.3523...  0.2711 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5031...  Training loss: 1.3511...  0.1847 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5032...  Training loss: 1.3201...  0.3010 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5033...  Training loss: 1.3111...  0.2714 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5034...  Training loss: 1.3097...  0.2354 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5035...  Training loss: 1.3132...  0.2370 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5036...  Training loss: 1.3218...  0.1590 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/30...  Training Step: 5037...  Training loss: 1.3121...  0.2405 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5038...  Training loss: 1.3060...  0.2519 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5039...  Training loss: 1.3294...  0.2512 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5040...  Training loss: 1.3008...  0.2437 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5041...  Training loss: 1.2929...  0.2245 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5042...  Training loss: 1.2842...  0.2695 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5043...  Training loss: 1.3165...  0.2187 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5044...  Training loss: 1.3324...  0.2346 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5045...  Training loss: 1.3131...  0.1924 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5046...  Training loss: 1.3146...  0.2625 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5047...  Training loss: 1.2947...  0.2663 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5048...  Training loss: 1.3218...  0.2643 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5049...  Training loss: 1.2875...  0.2491 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5050...  Training loss: 1.3109...  0.2324 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5051...  Training loss: 1.3042...  0.2618 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5052...  Training loss: 1.3167...  0.2260 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5053...  Training loss: 1.3145...  0.2149 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5054...  Training loss: 1.3329...  0.1540 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5055...  Training loss: 1.2681...  0.1951 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5056...  Training loss: 1.2903...  0.2848 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5057...  Training loss: 1.3160...  0.2022 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5058...  Training loss: 1.3129...  0.2133 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5059...  Training loss: 1.3236...  0.1849 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5060...  Training loss: 1.3517...  0.2410 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5061...  Training loss: 1.3436...  0.2487 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5062...  Training loss: 1.2689...  0.2111 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5063...  Training loss: 1.2789...  0.2629 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5064...  Training loss: 1.3214...  0.2571 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5065...  Training loss: 1.3110...  0.2549 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5066...  Training loss: 1.3397...  0.2601 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5067...  Training loss: 1.2837...  0.2576 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5068...  Training loss: 1.2887...  0.2507 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5069...  Training loss: 1.2908...  0.2425 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5070...  Training loss: 1.3002...  0.1881 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5071...  Training loss: 1.2948...  0.2174 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5072...  Training loss: 1.3116...  0.1746 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5073...  Training loss: 1.2941...  0.1696 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5074...  Training loss: 1.2918...  0.1586 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5075...  Training loss: 1.2941...  0.2020 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5076...  Training loss: 1.2603...  0.2174 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5077...  Training loss: 1.3134...  0.2414 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5078...  Training loss: 1.3012...  0.2643 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5079...  Training loss: 1.3081...  0.2510 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5080...  Training loss: 1.2873...  0.2627 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5081...  Training loss: 1.2998...  0.2661 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5082...  Training loss: 1.3248...  0.2262 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5083...  Training loss: 1.2957...  0.1650 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5084...  Training loss: 1.2915...  0.2591 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5085...  Training loss: 1.2764...  0.2375 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5086...  Training loss: 1.2777...  0.2021 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5087...  Training loss: 1.3006...  0.1617 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5088...  Training loss: 1.3041...  0.2123 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5089...  Training loss: 1.2849...  0.2568 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5090...  Training loss: 1.3125...  0.2095 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5091...  Training loss: 1.2855...  0.1791 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5092...  Training loss: 1.3200...  0.2070 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5093...  Training loss: 1.3081...  0.1762 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5094...  Training loss: 1.3009...  0.2041 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5095...  Training loss: 1.2752...  0.1992 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5096...  Training loss: 1.2882...  0.1642 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5097...  Training loss: 1.3067...  0.1727 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5098...  Training loss: 1.3082...  0.2252 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5099...  Training loss: 1.3039...  0.2706 sec/batch\n",
      "Epoch: 30/30...  Training Step: 5100...  Training loss: 1.2973...  0.2524 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "# Сохраняться каждый N итераций\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Можно раскомментировать строчку ниже и продолжить обучение с checkpoint'а\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Обучаем сеть\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i5100_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i5100_l512.ckpt\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"Гостиная Анны Павловны начала понемногу наполняться.\"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i5100_l512.ckpt\n",
      "Кто такой идиот к него. В половине маленькая женщина подала кость и покраснела, и он не знал, что ему совестно представилось так приеме.\n",
      "\n",
      "– Ну, так вы! – отвечал Степан Аркадьич, взяв ему в своей просить перевернуть ее. Она вспомнила, что ничто, по невыносим собер стоять при ней и стали выпорять свою мугу и откуда протил из положения, как все бывало, и все выражало его. В просиле его высшего возвращенного всей того, что он сказал, о том, что ты поступила на ней не могла понять, что она поняла, что, когда ее поняли, несолнан и пред ним, все это было трех условий после своей выборы. Он был так же, что она стала студать, но теперь, как бы переставая смеялся с собой от своих слова, не отделая на то представление, что он старого ни на стене.\n",
      "\n",
      "Он замечал, что он не мог просить этого и в отчаянии доказала ему остоящего всегда на свои волнения. Но он покраснел встречать эту первую руку, они прямо знала, что она подребала в собесавшийся на партии новый взгляд и приехал к себе, как будто напряженный ног, несметвя не понимал еще в том же состоянии думать. Он вскочил, он согласился на него; но она не спорила и не могла.\n",
      "\n",
      "– Да чет великодал вот. Я не могу быть несчастлив… Но он не может боялсс, он сказать то, что она не может быть так не для него, – сказал он по-занятия. – Все приехал про всякого. Он все подоровал об этом в совет.\n",
      "\n",
      "– Нет, я ваше последовала вам не могу, – сказала Долли. – Ну когда, – сказал Левин.\n",
      "\n",
      "– Да, я отвечал на свою прежде возвратить его важности.\n",
      "\n",
      "– Как ты думаешь обо мне? – сказала она подле него. – Но я никогда не видал на свой проченки, которые мне сказал этого. Надо, пожалуйста, не буду видеть этого.\n",
      "\n",
      "– Нет, это нет, я скоро, как не могу прямо вспомнить с ней. То я поставила в свойму девушку в своей самах, и вы не можете без тебя испытывают. Но я все была в нем сказать в ней. Она просили его приехоть и поспешно обещала себе покосить, но все это было в положении с тем, что все это согласие прежде, и потому, что все это надвонит. Привычке неприятно было так страшно.\n",
      "\n",
      "–\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Кто такой идиот\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
