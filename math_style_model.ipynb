{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MathStyleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math_vocab import MathVocab\n",
    "from vocabulary import BOS_CODE\n",
    "from style_dataset import StyleDataset\n",
    "from math_dataset import MathDataset\n",
    "from utils import words_to_sent\n",
    "\n",
    "voc_path        = \"data/math_style/math_vocab.data\"\n",
    "tr_dataset_path = \"data/math_style/math_dataset_%d_train.data\"\n",
    "va_dataset_path = \"data/math_style/math_dataset_%d_valid.data\"\n",
    "model_path      = 'models/math/style-autoencoder-003/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MathVocab:\n",
      "  size: 17\n",
      "  _tokens_to_words: ['<UNK>', '<BOS>', '<EOS>', '<PAD>', '+', '1', '2', '=', ' ', '3', '4', '5', '6', '7', '8', '9', '0']\n"
     ]
    }
   ],
   "source": [
    "voc = MathVocab()\n",
    "voc.restore(voc_path)\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StyleDataset:\n",
      "  path_0: data/math_style/math_dataset_0_train.data\n",
      "  path_1: data/math_style/math_dataset_1_train.data\n",
      "  shape: [20000, 34]\n",
      "\n",
      "StyleDataset:\n",
      "  path_0: data/math_style/math_dataset_0_valid.data\n",
      "  path_1: data/math_style/math_dataset_1_valid.data\n",
      "  shape: [20000, 34]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_dataset = StyleDataset(MathDataset)\n",
    "tr_dataset.restore(tr_dataset_path)\n",
    "print(tr_dataset)\n",
    "\n",
    "va_dataset = StyleDataset(MathDataset)\n",
    "va_dataset.restore(va_dataset_path)\n",
    "print(va_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import make_dir\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from visualization import show_train_stats_ex\n",
    "\n",
    "class StyleAutoencoder:\n",
    "    def __init__(self):\n",
    "        self._scope    = 'style_autoencoder'\n",
    "        # state\n",
    "        self.tr_step   = 0\n",
    "        self.tr_epoch  = 0\n",
    "        self.tr_losses = []\n",
    "        self.va_losses = []\n",
    "    \n",
    "    \n",
    "    def save(self, path):\n",
    "        make_dir(path)\n",
    "        pickle.dump([self.tr_epoch, self.tr_step, self.tr_losses, self.va_losses], \n",
    "                    open(os.path.join(path, \"state.p\"), \"wb\"))\n",
    "        self._saver.save(self._sess, path)\n",
    "        \n",
    "        \n",
    "    def restore(self, path):\n",
    "        try:\n",
    "            [self.tr_epoch, self.tr_step, self.tr_losses, self.va_losses] = pickle.load(\n",
    "                open(os.path.join(path, \"state.p\"), \"rb\"))\n",
    "        except: \n",
    "            print(\"State not found at\", path)\n",
    "        self._saver.restore(self._sess, path)\n",
    "        \n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.embedding.eval(self._sess)\n",
    "    \n",
    "        \n",
    "    def _make_loss(self, logits, labels):\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits,\n",
    "            labels = labels)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "\n",
    "    def _make_optimizer(self, loss, lr, grad_clip):\n",
    "        tr_vars   = tf.trainable_variables()\n",
    "        grads, _  = tf.clip_by_global_norm(tf.gradients(loss, tr_vars), grad_clip)\n",
    "        train_op  = tf.train.AdamOptimizer(lr)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tr_vars))\n",
    "        return optimizer\n",
    "    \n",
    "    # =====================================================================\n",
    "    \n",
    "    def _make_rnn_cell(self, num_units, num_layers, keep_prob):\n",
    "        def make_layer():\n",
    "            l = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "            l = tf.contrib.rnn.DropoutWrapper(l, output_keep_prob=keep_prob)\n",
    "            return l        \n",
    "        return tf.contrib.rnn.MultiRNNCell([make_layer() for _ in range(num_layers)])\n",
    "    \n",
    "    \n",
    "    def _make_decoder(self, inp, h, num_units, num_layers, voc_size, seq_len, keep_prob):\n",
    "        cell = self._make_rnn_cell(num_units, num_layers, keep_prob)\n",
    "        logits_seq = []\n",
    "\n",
    "        def loop_function(outputs):\n",
    "            logits = tf.layers.dense(outputs, voc_size, name='projection')\n",
    "            token = tf.argmax(logits, axis=1)\n",
    "            inp = tf.nn.embedding_lookup(self.embedding, token)\n",
    "            return inp, logits\n",
    "            \n",
    "        with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "            zeros = tf.zeros_like(h)\n",
    "            state = (tf.nn.rnn_cell.LSTMStateTuple(zeros, zeros),\n",
    "                     tf.nn.rnn_cell.LSTMStateTuple(zeros, h))\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                output, state = cell.apply(inp, state)\n",
    "                inp, logits = loop_function(output)\n",
    "                logits_seq.append(tf.expand_dims(logits, 1))\n",
    "\n",
    "        return tf.concat(logits_seq, axis=1)\n",
    "    \n",
    "    \n",
    "    def build(self, voc_size, bos_token, emb_size, seq_len, num_units, num_layers, grad_clip=5):\n",
    "        tf.reset_default_graph()\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default(), tf.variable_scope(self._scope):\n",
    "            # placeholders\n",
    "            self.enc_inputs_pl = tf.placeholder(tf.int32, [None, None], name='enc_inputs')\n",
    "            self.targets_pl    = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "            self.styles_pl     = tf.placeholder(tf.int32, shape=[None], name='styles')\n",
    "            self.batch_size_pl = tf.placeholder(tf.int32, shape=[], name='batch_size')\n",
    "            self.keep_prob_pl  = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            self.lr_pl         = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "            # embedding\n",
    "            self.embedding     = tf.Variable(tf.random_normal(shape=[voc_size, emb_size], dtype=tf.float32))\n",
    "            en_inputs          = tf.nn.embedding_lookup(self.embedding, self.enc_inputs_pl)\n",
    "            bos_tokens         = tf.tile([bos_token], tf.expand_dims(self.batch_size_pl,0))  \n",
    "            de_inputs          = tf.nn.embedding_lookup(self.embedding, bos_tokens)\n",
    "            \n",
    "            # encoder\n",
    "            en_cell            = self._make_rnn_cell(num_units, num_layers, self.keep_prob_pl)\n",
    "            en_initial_state   = en_cell.zero_state(self.batch_size_pl, tf.float32)\n",
    "            seq_lens           = tf.tile([seq_len], tf.expand_dims(self.batch_size_pl,0))\n",
    "            _, en_state        = tf.nn.dynamic_rnn(en_cell, en_inputs, seq_lens, en_initial_state, scope='encoder')\n",
    "            \n",
    "            # latent vector = [content, style]\n",
    "            style_num  = 2\n",
    "            style_vec  = tf.one_hot(self.styles_pl, style_num)\n",
    "            content_style = tf.concat([en_state[~0].h, style_vec], axis = 1)\n",
    "\n",
    "            # decoder\n",
    "            de_logits = self._make_decoder(\n",
    "                inp        = de_inputs, \n",
    "                h          = content_style, \n",
    "                num_units  = num_units+style_num, \n",
    "                num_layers = num_layers,\n",
    "                voc_size   = voc_size,\n",
    "                seq_len    = seq_len, \n",
    "                keep_prob  = self.keep_prob_pl)\n",
    "\n",
    "            # prediction\n",
    "            self.outputs       = tf.argmax(tf.nn.softmax(de_logits), axis=2)\n",
    "\n",
    "            # training\n",
    "            self.loss_op       = self._make_loss(de_logits, self.targets_pl)\n",
    "            self.train_op      = self._make_optimizer(self.loss_op, self.lr_pl, grad_clip)\n",
    "            \n",
    "            # utils\n",
    "            self.init_op       = tf.global_variables_initializer()\n",
    "            self._saver        = tf.train.Saver()\n",
    "\n",
    "        # session\n",
    "        self._sess = tf.Session(graph=self._graph)\n",
    "        self._sess.run(self.init_op)\n",
    "        \n",
    "    # =====================================================================\n",
    "    \n",
    "\n",
    "    def train(self, tr_dataset, va_dataset, \n",
    "              step_num,  batch_size, learning_rate, keep_prob=.5, log_every=20, mean_win=30):\n",
    "        try:\n",
    "            data_size = tr_dataset.get_data_size()\n",
    "            for self.tr_step in range(self.tr_step, step_num-1):\n",
    "                ep = self.tr_step*batch_size/data_size\n",
    "                # Train\n",
    "                sents, styles = tr_dataset.get_next_batch(batch_size)\n",
    "                tr_loss, _ = self._sess.run(\n",
    "                    [self.loss_op, self.train_op], \n",
    "                    feed_dict = {\n",
    "                        self.enc_inputs_pl: sents,\n",
    "                        self.targets_pl   : sents,\n",
    "                        self.styles_pl    : styles,\n",
    "                        self.batch_size_pl: batch_size,\n",
    "                        self.keep_prob_pl : keep_prob,\n",
    "                        self.lr_pl        : learning_rate,\n",
    "                })\n",
    "                self.tr_losses.append(tr_loss)\n",
    "                # Eval\n",
    "                if self.tr_step % log_every == log_every-1:\n",
    "                    sents, styles = va_dataset.get_next_batch(batch_size)\n",
    "                    va_loss = self._sess.run(\n",
    "                        self.loss_op, \n",
    "                        feed_dict={\n",
    "                            self.enc_inputs_pl: sents,\n",
    "                            self.targets_pl   : sents,\n",
    "                            self.styles_pl    : styles,\n",
    "                            self.batch_size_pl: batch_size,\n",
    "                            self.keep_prob_pl : keep_prob,\n",
    "                            self.lr_pl        : learning_rate,\n",
    "                        })\n",
    "                    self.va_losses.extend([va_loss]*log_every)\n",
    "                    show_train_stats_ex(ep, self.tr_step, self.tr_losses, self.va_losses, mean_win)\n",
    "                        \n",
    "        except KeyboardInterrupt:\n",
    "            show_train_stats_ex(ep, self.tr_step, self.tr_losses, self.va_losses, mean_win)\n",
    "            \n",
    "\n",
    "    def run(self, sents, styles, batch_size, seq_length):\n",
    "        return self._sess.run(\n",
    "            self.outputs, \n",
    "            feed_dict = {\n",
    "                self.enc_inputs_pl: sents,\n",
    "                self.targets_pl   : sents,\n",
    "                self.styles_pl    : styles,\n",
    "                self.batch_size_pl: batch_size,\n",
    "                self.keep_prob_pl : 1,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 92.2 ms, total: 7.11 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = StyleAutoencoder()\n",
    "seq_len = tr_dataset.get_seq_len()\n",
    "assert seq_len == va_dataset.get_seq_len()\n",
    "\n",
    "model.build(\n",
    "    voc_size   = voc.size, \n",
    "    bos_token  = BOS_CODE,\n",
    "    seq_len    = seq_len,\n",
    "    emb_size   = 100, \n",
    "    num_units  = 256, \n",
    "    num_layers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,1032]\n\t [[Node: style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/concat_10, style_autoencoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read)]]\n\t [[Node: style_autoencoder/Mean/_63 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_13486_style_autoencoder/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-2e394f5386c3>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', 'model = StyleAutoencoder()\\nseq_len = tr_dataset.get_seq_len()\\nassert seq_len == va_dataset.get_seq_len()\\n\\nmodel.build(\\n    voc_size   = voc.size, \\n    bos_token  = BOS_CODE,\\n    seq_len    = seq_len,\\n    emb_size   = 100, \\n    num_units  = 256, \\n    num_layers = 2)')\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2131, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-62>\", line 2, in time\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magics/execution.py\", line 1238, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 11, in <module>\n  File \"<ipython-input-13-57365299c6bb>\", line 122, in build\n    keep_prob  = self.keep_prob_pl)\n  File \"<ipython-input-13-57365299c6bb>\", line 78, in _make_decoder\n    output, state = cell.apply(inp, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1066, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 891, in __call__\n    output, new_state = self._cell(inputs, state, scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 441, in call\n    value=self._linear([inputs, h]), num_or_size_splits=4, axis=1)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,1032]\n\t [[Node: style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/concat_10, style_autoencoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read)]]\n\t [[Node: style_autoencoder/Mean/_63 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_13486_style_autoencoder/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,1032]\n\t [[Node: style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/concat_10, style_autoencoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read)]]\n\t [[Node: style_autoencoder/Mean/_63 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_13486_style_autoencoder/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-57365299c6bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tr_dataset, va_dataset, step_num, batch_size, learning_rate, keep_prob, log_every, mean_win)\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob_pl\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_pl\u001b[0m        \u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 })\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtr_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,1032]\n\t [[Node: style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/concat_10, style_autoencoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read)]]\n\t [[Node: style_autoencoder/Mean/_63 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_13486_style_autoencoder/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-2e394f5386c3>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', 'model = StyleAutoencoder()\\nseq_len = tr_dataset.get_seq_len()\\nassert seq_len == va_dataset.get_seq_len()\\n\\nmodel.build(\\n    voc_size   = voc.size, \\n    bos_token  = BOS_CODE,\\n    seq_len    = seq_len,\\n    emb_size   = 100, \\n    num_units  = 256, \\n    num_layers = 2)')\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2131, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-62>\", line 2, in time\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/magics/execution.py\", line 1238, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 11, in <module>\n  File \"<ipython-input-13-57365299c6bb>\", line 122, in build\n    keep_prob  = self.keep_prob_pl)\n  File \"<ipython-input-13-57365299c6bb>\", line 78, in _make_decoder\n    output, state = cell.apply(inp, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1066, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 891, in __call__\n    output, new_state = self._cell(inputs, state, scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 441, in call\n    value=self._linear([inputs, h]), num_or_size_splits=4, axis=1)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,1032]\n\t [[Node: style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_10 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](style_autoencoder/decoder/decoder/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/concat_10, style_autoencoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read)]]\n\t [[Node: style_autoencoder/Mean/_63 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_13486_style_autoencoder/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "model.train(\n",
    "    step_num      = 50000,\n",
    "    batch_size    = 64, \n",
    "    learning_rate = 1e-3, \n",
    "    mean_win      = 30,\n",
    "    log_every     = 10,\n",
    "    tr_dataset    = tr_dataset,\n",
    "    va_dataset    = va_dataset,\n",
    ")\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MathStyleDataset:\n",
      "  path_0: data/math_style/math_dataset_0_train.data\n",
      "  path_1: data/math_style/math_dataset_1_train.data\n",
      "  shape: [20000, 34]\n",
      "\n",
      "--------------------------------------------------\n",
      "[4+14+1 = 3+7+1+1+2+5]____________\n",
      "[4+14+1=3+7+1+1+2+1+5]____________\n",
      "--------------------------------------------------\n",
      "[5+1+1+7 = 4+10]__________________\n",
      "[5+1+1+7=4+1+10]__________________\n",
      "--------------------------------------------------\n",
      "[17 = 6+9+2]______________________\n",
      "[17=5+9+5+1]______________________\n",
      "--------------------------------------------------\n",
      "[2+3+2+1+1+6 = 3+1+5+6]___________\n",
      "[2+3+2+1+1+7=3+1+4+4+6]___________\n",
      "--------------------------------------------------\n",
      "[10+6 = 13+1+2]___________________\n",
      "[10+6=13+1+2]_____________________\n",
      "--------------------------------------------------\n",
      "[7+5+1=5+8]_______________________\n",
      "[7+5+1 = 5+8]_____________________\n",
      "--------------------------------------------------\n",
      "[4+4+2=3+1+4+2]___________________\n",
      "[4+4+2 = 3+2+2]___________________\n",
      "--------------------------------------------------\n",
      "[15=1+7+7]________________________\n",
      "[15 = 2+7+7]______________________\n",
      "--------------------------------------------------\n",
      "[8+2=1+7+1+1]_____________________\n",
      "[8+2 = 1+7+1]_____________________\n",
      "--------------------------------------------------\n",
      "[4+1+6+3=3+1+9+1]_________________\n",
      "[4+1+6+3 = 3+3+7]_________________\n",
      "\n",
      "================================================================================\n",
      "MathStyleDataset:\n",
      "  path_0: data/math_style/math_dataset_0_valid.data\n",
      "  path_1: data/math_style/math_dataset_1_valid.data\n",
      "  shape: [20000, 34]\n",
      "\n",
      "--------------------------------------------------\n",
      "[1+7+1+4 = 6+7]___________________\n",
      "[1+7+1+4=6+4+7]___________________\n",
      "--------------------------------------------------\n",
      "[9+4 = 1+3+3+3+3]_________________\n",
      "[9+4=1+3+3+3+2+3]_________________\n",
      "--------------------------------------------------\n",
      "[2+1+1+1+3+2 = 4+6]_______________\n",
      "[2+1+1+1+3+2=3+4+6]_______________\n",
      "--------------------------------------------------\n",
      "[6+3+1+2+1 = 6+7]_________________\n",
      "[6+3+1+3+1=5+4+7]_________________\n",
      "--------------------------------------------------\n",
      "[2+2+7+1+6 = 9+1+7+1]_____________\n",
      "[2+2+8+1+6=9+1+5+5+1]_____________\n",
      "--------------------------------------------------\n",
      "[3+2+1+6=4+8]_____________________\n",
      "[3+2+1+5 = 4+8]___________________\n",
      "--------------------------------------------------\n",
      "[3+1+2+5+2=2+1+9+1]_______________\n",
      "[3+1+2+4+2 = 2+3+8+2]_____________\n",
      "--------------------------------------------------\n",
      "[1+1+1+5+5=5+8]___________________\n",
      "[1+1+1+4+5 = 5+8]_________________\n",
      "--------------------------------------------------\n",
      "[1+8+2+1=12]______________________\n",
      "[1+8+2+1 = 12]____________________\n",
      "--------------------------------------------------\n",
      "[10+9=1+6+12]_____________________\n",
      "[10+8 = 2+4+]_____________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for dataset in [tr_dataset, va_dataset]:\n",
    "    print('\\n'+'='*80+'\\n'+str(dataset))\n",
    "    inp, styles = dataset.get_next_batch(batch_size)\n",
    "    styles = 1-styles\n",
    "    out = model.run(inp, styles, batch_size, seq_len)\n",
    "\n",
    "    def prepare(s):\n",
    "        words = voc.to_words(s)\n",
    "        s = \"\".join(words)\n",
    "        s = s.replace('<BOS>', '[').replace('<EOS>', ']').replace('<PAD>', '_')\n",
    "        return s\n",
    "\n",
    "    for i in range(len(inp)):\n",
    "        print('-'*50)\n",
    "        print(prepare(inp[i]))\n",
    "        print(prepare(out[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 100)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "embeddings = model.get_embeddings()\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 5, 6, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc488119e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import product\n",
    "digits = voc.to_tokens(list(\"0123456789\"))\n",
    "print(digits)\n",
    "emb = embeddings[digits,:]\n",
    "\n",
    "dist = np.zeros([10,10])\n",
    "for i,j in product(range(10), range(10)):\n",
    "    a,b = emb[i], emb[j]\n",
    "    dist[i,j] = np.sum((a-b)**2)\n",
    "    \n",
    "plt.imshow(dist, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc488105b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "la = np.linalg\n",
    "words = voc.to_words(range(voc.size))\n",
    "\n",
    "start = 0\n",
    "\n",
    "X =  embeddings[start:]\n",
    "\n",
    "U,S,V = la.svd(X)\n",
    "n = len(X)\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "for i in range(n):\n",
    "    x,y,w = U[i,0], U[i,1], words[start+i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(w, xy=(x, y), xytext=(7, 2), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n",
    "    #print('%+.2f, %+.2f, %s' % (x, y, w))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/math/style-autoencoder-002/\n"
     ]
    }
   ],
   "source": [
    "model.restore(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
