{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_dataset import WordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "class WordDataset:\n",
    "    def __init__(self):\n",
    "        self.text    = None\n",
    "        self.encoded = []\n",
    "    \n",
    "    \n",
    "    def load(self, path):\n",
    "        STOP_WORDS   = [u\"[\", u\"]\"]\n",
    "        PUNCTS       = [u\".\", u\",\", u\"?\", u\"!\"]\n",
    "        MIN_SENT_LEN = 2\n",
    "        MAX_SENT_LEN = 30\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            text=f.read()\n",
    "            text = text.replace(u'\\xa0', u' ').replace(u'\\ufeff','')\n",
    "            text = text.replace('\\n', '.\\n').replace('..\\n', '.\\n')\n",
    "\n",
    "        sents = []\n",
    "        for line in text.split('\\n'):\n",
    "            ls = nltk.tokenize.sent_tokenize(line)\n",
    "            for s in ls:\n",
    "                sents.extend(s.split(\";\"))\n",
    "\n",
    "        word_text = []\n",
    "        vocab     = set()\n",
    "        for sent in sents:\n",
    "            words = [w.lower() for w in nltk.tokenize.word_tokenize(sent) if w not in STOP_WORDS]\n",
    "            if MIN_SENT_LEN <= len(words) <= MAX_SENT_LEN:\n",
    "                if len(words) == 2 and words[1] in PUNCTS:\n",
    "                    continue\n",
    "                word_text.extend(words)\n",
    "                vocab.update(words)\n",
    "\n",
    "        self.char_text     = text\n",
    "        self.word_text     = word_text\n",
    "        self.word_to_token = {w: i for i, w in enumerate(vocab)}\n",
    "        self.token_to_word = dict(enumerate(vocab))\n",
    "        self.num_tokens    = len(vocab)\n",
    "        self.encoded       = self.encode(word_text)\n",
    "\n",
    "\n",
    "    def encode(self, words):\n",
    "        return np.array([self.word_to_token[w] for w in words], dtype=np.int32)\n",
    "\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.token_to_word[t] for t in tokens])\n",
    "        return text\n",
    "\n",
    "    \n",
    "    def decode_ext(self, tokens):\n",
    "        text = \" \".join([self.token_to_word[t] for t in tokens])\n",
    "        text = text.replace(' .', '.').replace(' ,', ',')\n",
    "        snts = text.split('.')\n",
    "        text = ''\n",
    "        for s in snts:\n",
    "            s = s.strip()\n",
    "            if len(s)<1: \n",
    "                continue\n",
    "            s = s[0].upper() + s[1:]+'. '\n",
    "            text += s\n",
    "        return text.strip()\n",
    "\n",
    "    \n",
    "    def get_batches(self, n_seqs, n_steps):\n",
    "        arr = self.encoded\n",
    "        words_per_batch = n_seqs * n_steps\n",
    "        n_batches = len(arr)//words_per_batch\n",
    "\n",
    "        arr = arr[:n_batches * words_per_batch]\n",
    "        arr = arr.reshape((n_seqs, -1))\n",
    "\n",
    "        for n in range(0, arr.shape[1], n_steps):\n",
    "            x = arr[:, n:n+n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26078\n",
      "CPU times: user 9.72 s, sys: 20 ms, total: 9.74 s\n",
      "Wall time: 9.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = WordDataset()\n",
    "dataset.load('data/anna.txt')\n",
    "print(dataset.num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЧАСТЬ ПЕРВАЯ.\n",
      ".\n",
      ".\n",
      ".\n",
      "I.\n",
      ".\n",
      "Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.\n",
      ".\n",
      "Все смешалось в доме Облонских. Жена узнала, что муж был в связи с бывшею в их доме француженкою-гувернанткой, и объявила мужу, что не может жить с ним в одном доме.\n",
      "--------------------------------------------------\n",
      "['часть', 'первая', '.', 'все', 'счастливые', 'семьи', 'похожи', 'друг', 'на', 'друга', ',', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'по-своему', '.', 'все', 'смешалось', 'в', 'доме', 'облонских', '.', 'жена', 'узнала', ',', 'что', 'муж', 'был', 'в', 'связи', 'с', 'бывшею', 'в', 'их', 'доме', 'француженкою-гувернанткой', ',', 'и', 'объявила', 'мужу', ',', 'что', 'не', 'может', 'жить', 'с', 'ним', 'в', 'одном', 'доме', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.char_text[:284])\n",
    "print('-'*50)\n",
    "print(dataset.word_text[:52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18547  3469  1285 14202  8378 22978 11997 20217 13743 22347  2245 24258\n",
      "  9394  3765 24520 13166  1285 14202  7840 15874  9021 19907  1285 18258\n",
      " 20764  2245 10130  2270 11481 15874  8121    47 23763 15874  5954  9021\n",
      " 24066  2245  8543 18025  8384  2245 10130   876  5141  4337    47 24266\n",
      " 15874 13254  9021  1285]\n",
      "[Часть первая. Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему. Все смешалось в доме облонских. Жена узнала, что муж был в связи с бывшею в их доме француженкою-гувернанткой, и объявила мужу, что не может жить с ним в одном доме.]\n",
      "[18547  3469  1285]\n"
     ]
    }
   ],
   "source": [
    "words = dataset.word_text[:52]\n",
    "print(dataset.encode(words))\n",
    "print(\"[%s]\" % dataset.decode_ext(dataset.encode(words)))\n",
    "print(dataset.encode(['часть', 'первая', '.']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18547  3469  1285 14202  8378 22978 11997 20217 13743 22347]\n",
      " [13619 18501  2245 10601  8255 23306 24105  2245 18943  9098]\n",
      " [18881  5456  8543 26061    47 16612 17294  1285 10601   775]]\n",
      "[[ 3469  1285 14202  8378 22978 11997 20217 13743 22347 18547]\n",
      " [18501  2245 10601  8255 23306 24105  2245 18943  9098 13619]\n",
      " [ 5456  8543 26061    47 16612 17294  1285 10601   775 18881]]\n",
      "\n",
      "[часть первая . все счастливые семьи похожи друг на друга]\n",
      "[константин дмитрич , – сказала дарья александровна , улыбаясь своею]\n",
      "[стоял стремов и разговаривал с нею : . – теноров]\n",
      "\n",
      "[первая . все счастливые семьи похожи друг на друга часть]\n",
      "[дмитрич , – сказала дарья александровна , улыбаясь своею константин]\n",
      "[стремов и разговаривал с нею : . – теноров стоял]\n"
     ]
    }
   ],
   "source": [
    "batches = dataset.get_batches(3, 10)\n",
    "x,y = next(batches)\n",
    "print(x)\n",
    "print(y)\n",
    "print()\n",
    "for t in x:\n",
    "    print('[%s]' % dataset.decode(t))\n",
    "print()\n",
    "for t in y:\n",
    "    print('[%s]' % dataset.decode(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
